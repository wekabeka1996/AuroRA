{
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Aurora — Mathematical Audit Notebook\n",
      "\n",
      "Аудит критичних та складних математичних компонентів Aurora: TCA-тотожності, знакові конвенції, хвости ризику (EVT), статистична валідація (bootstrap CI, DM-тест), walk-forward стабільність. Артефакти пишуться до `/content/drive/MyDrive/reports/`.\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Setup (Colab) — інструкції\n",
      "- Натисни ▶ у наступній комірці, щоб встановити залежності (якщо потрібно).\n",
      "- Дочекайся зеленої галочки. Далі переходь до Drive/Paths.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"setup"},"execution_count":null,"outputs":[],"source":[
      "IN_COLAB = 'google.colab' in __import__('sys').modules\n",
      "if IN_COLAB:\n",
      "    %pip -q install pandas numpy matplotlib seaborn tomli || true\n",
      "import numpy as np, pandas as pd, json, math, os, gzip, io, statistics, time\n",
      "import matplotlib.pyplot as plt; import seaborn as sns; sns.set(style='whitegrid')\n",
      "from pathlib import Path\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Paths (Google Drive) — інструкції\n",
      "- В Colab поклади дані у `/content/drive/MyDrive/datasets/`, конфіги — у `/content/drive/MyDrive/configs/`.\n",
      "- Натисни ▶ — підключимо Drive та зафіксуємо ROOT.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"paths"},"execution_count":null,"outputs":[],"source":[
      "ROOT = Path('/content/drive/MyDrive') if IN_COLAB else Path.cwd()\n",
      "if IN_COLAB:\n",
      "    try:\n",
      "        from google.colab import drive as _drive\n",
      "        _drive.mount('/content/drive', force_remount=False)\n",
      "    except Exception as e:\n",
      "        print('Drive mount warning:', e)\n",
      "REPORTS = ROOT/'reports'; REPORTS.mkdir(parents=True, exist_ok=True)\n",
      "DATASETS = ROOT/'datasets'; DATASETS.mkdir(parents=True, exist_ok=True)\n",
      "CONFIGS = ROOT/'configs'\n",
      "print('Using ROOT =', ROOT)\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Load Config + Logs — інструкції\n",
      "- Конфіги очікуємо у `/content/drive/MyDrive/configs/` (`default.toml`, `schema.json`).\n",
      "- Логи шукаємо у `/content/drive/MyDrive/datasets/` (`*.jsonl.gz` або `*.jsonl`). Якщо немає — зʼявиться віджет завантаження.\n",
      "- Натисни ▶.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"load"},"execution_count":null,"outputs":[],"source":[
      "def _toml_load_bytes(b):\n",
      "    try:\n",
      "        import tomllib; return tomllib.loads(b.decode('utf-8') if isinstance(b,(bytes,bytearray)) else b)\n",
      "    except Exception:\n",
      "        import tomli; return tomli.loads(b.decode('utf-8') if isinstance(b,(bytes,bytearray)) else b)\n",
      "\n",
      "def load_ssot(configs_dir: Path):\n",
      "    cfg = configs_dir/'default.toml'\n",
      "    if cfg.exists(): return _toml_load_bytes(cfg.read_bytes())\n",
      "    return {}\n",
      "\n",
      "def read_jsonl_any(p: Path):\n",
      "    buf='';\n",
      "    try:\n",
      "        if str(p).endswith('.gz'):\n",
      "            import gzip\n",
      "            with gzip.open(p,'rt',encoding='utf-8') as f: buf=f.read()\n",
      "        else:\n",
      "            buf=p.read_text(encoding='utf-8')\n",
      "    except Exception:\n",
      "        return []\n",
      "    out=[]\n",
      "    for line in buf.splitlines():\n",
      "        try: out.append(json.loads(line))\n",
      "        except Exception: pass\n",
      "    return out\n",
      "\n",
      "CFG = load_ssot(CONFIGS)\n",
      "found = sorted(DATASETS.rglob('*.jsonl.gz')) + sorted(DATASETS.rglob('*.jsonl'))\n",
      "LOG_FILES = found[:200]\n",
      "if IN_COLAB and not LOG_FILES:\n",
      "    try:\n",
      "        from google.colab import files\n",
      "        up = files.upload(); LOG_FILES=[Path(k) for k in up.keys()]\n",
      "    except Exception: pass\n",
      "rows=[]\n",
      "for p in LOG_FILES: rows.extend(read_jsonl_any(p))\n",
      "df = pd.json_normalize(rows) if rows else pd.DataFrame()\n",
      "print('Configs loaded:', bool(CFG), '| Logs files:', len(LOG_FILES), '| Rows:', len(df))\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## TCA Identity & Sign Conventions — інструкції\n",
      "- Натисни ▶ — перевіримо: IS = raw + fees + slippages + adverse + latency + impact + rebate.\n",
      "- Перевіримо знаки компонентів (≤0 або ≥0, як належить). Отримаємо агреговану статистику порушень.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"tca_checks"},"execution_count":null,"outputs":[],"source":[
      "checks = {}\n",
      "if df.empty:\n",
      "    print('No data to audit.')\n",
      "else:\n",
      "    cols = ['is_bps','raw_edge_bps','fees_bps','slippage_in_bps','slippage_out_bps','adverse_bps','latency_bps','impact_bps','rebate_bps']\n",
      "    present = [c for c in cols if c in df.columns]\n",
      "    if 'is_bps' not in df.columns and 'IS_bps' in df.columns: df['is_bps']=pd.to_numeric(df['IS_bps'],errors='coerce')\n",
      "    for c in cols:\n",
      "        if c not in df.columns: df[c]=np.nan\n",
      "    X = df.copy()\n",
      "    for c in cols: X[c]=pd.to_numeric(X[c],errors='coerce')\n",
      "    comp_sum = X['raw_edge_bps']+X['fees_bps']+X['slippage_in_bps']+X['slippage_out_bps']+X['adverse_bps']+X['latency_bps']+X['impact_bps']+X['rebate_bps']\n",
      "    diff = (X['is_bps'] - comp_sum).abs()\n",
      "    tol = 1e-6\n",
      "    id_ok = (diff<=tol).sum(); id_bad = (diff>tol).sum()\n",
      "    # Sign gates\n",
      "    gates = {\n",
      "      'fees_bps<=0': (X['fees_bps']<=1e-12).mean(),\n",
      "      'slip_in<=0': (X['slippage_in_bps']<=1e-12).mean(),\n",
      "      'slip_out<=0': (X['slippage_out_bps']<=1e-12).mean(),\n",
      "      'adverse<=0': (X['adverse_bps']<=1e-12).mean(),\n",
      "      'latency<=0': (X['latency_bps']<=1e-12).mean(),\n",
      "      'impact<=0': (X['impact_bps']<=1e-12).mean(),\n",
      "      'rebate>=0': (X['rebate_bps']>=-1e-12).mean(),\n",
      "    }\n",
      "    checks = {'identity_ok': int(id_ok), 'identity_bad': int(id_bad), 'gates_share': gates}\n",
      "    print(json.dumps(checks, indent=2))\n",
      "    (REPORTS/'audit_tca_checks.json').write_text(json.dumps(checks, indent=2), encoding='utf-8')\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Bootstrap CI (IS, Sharpe) — інструкції\n",
      "- Натисни ▶ — отримаєш довірчі інтервали для середнього IS та Sharpe (простий bootstrap).\n",
      "- Якщо даних мало, інтервали будуть широкі.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"bootstrap"},"execution_count":null,"outputs":[],"source":[
      "def bootstrap_ci(series: np.ndarray, func, B=1000, alpha=0.05, random_state=123):\n",
      "    rng = np.random.default_rng(random_state)\n",
      "    series = series[~np.isnan(series)]\n",
      "    if len(series)==0: return (np.nan,np.nan,np.nan)\n",
      "    stats = []\n",
      "    n=len(series)\n",
      "    for _ in range(B):\n",
      "        idx = rng.integers(0,n,size=n)\n",
      "        stats.append(func(series[idx]))\n",
      "    stats = np.array(stats)\n",
      "    lo,hi = np.quantile(stats,[alpha/2,1-alpha/2])\n",
      "    return float(np.mean(stats)), float(lo), float(hi)\n",
      "\n",
      "def sharpe(series: np.ndarray):\n",
      "    x = series[~np.isnan(series)]\n",
      "    if len(x)<2: return np.nan\n",
      "    return float(np.mean(x)/(np.std(x,ddof=1)+1e-12))\n",
      "\n",
      "if df.empty or ('is_bps' not in df.columns and 'IS_bps' not in df.columns):\n",
      "    print('No IS data for bootstrap.')\n",
      "else:\n",
      "    s = pd.to_numeric(df['is_bps'] if 'is_bps' in df.columns else df['IS_bps'], errors='coerce').values\n",
      "    m_mu, m_lo, m_hi = bootstrap_ci(s, lambda z: np.mean(z))\n",
      "    sh, sh_lo, sh_hi = bootstrap_ci(s, sharpe)\n",
      "    res = {'IS_mean': {'est':m_mu,'lo':m_lo,'hi':m_hi}, 'Sharpe': {'est':sh,'lo':sh_lo,'hi':sh_hi}}\n",
      "    print(json.dumps(res, indent=2))\n",
      "    (REPORTS/'audit_bootstrap.json').write_text(json.dumps(res, indent=2), encoding='utf-8')\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## EVT Tail Audit (POT) — інструкції\n",
      "- Натисни ▶ — оцінимо хвіст за методом Peaks-Over-Threshold (спрощено).\n",
      "- Отримаємо VaR/CVaR емпірично, mean-excess діаграму, прості оцінки.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"evt"},"execution_count":null,"outputs":[],"source":[
      "def empirical_var_cvar(x, alpha=0.95):\n",
      "    x = np.sort(x)\n",
      "    n = len(x)\n",
      "    if n==0: return (np.nan,np.nan)\n",
      "    k = max(1,int((1-alpha)*n))\n",
      "    tail = x[:k]  # worst side assuming IS positive is good; if working with losses use sign accordingly\n",
      "    var = x[k-1] if k>0 else x[0]\n",
      "    cvar = float(np.mean(tail)) if k>0 else var\n",
      "    return (var,cvar)\n",
      "\n",
      "if df.empty or ('is_bps' not in df.columns and 'IS_bps' not in df.columns):\n",
      "    print('No IS data for EVT.')\n",
      "else:\n",
      "    s = pd.to_numeric(df['is_bps'] if 'is_bps' in df.columns else df['IS_bps'], errors='coerce').dropna().values\n",
      "    # Для downside tail беремо мінімальні значення IS (гірша сторона)\n",
      "    s_sorted = np.sort(s)\n",
      "    alpha=0.95\n",
      "    k = max(1,int((1-alpha)*len(s_sorted)))\n",
      "    tail = s_sorted[:k]\n",
      "    if len(tail)==0: print('Tail too small.'); tail=np.array([])\n",
      "    # Mean-excess plot\n",
      "    if len(s_sorted)>10:\n",
      "        qs = np.linspace(0.8,0.99,20)\n",
      "        us = [np.quantile(s_sorted,q) for q in qs]\n",
      "        me = []\n",
      "        for u in us:\n",
      "            exc = s_sorted[s_sorted<=u]  # worst side\n",
      "            me.append(float(np.mean(u-exc)) if len(exc)>0 else np.nan)\n",
      "        plt.figure(figsize=(5,3)); plt.plot(us, me, marker='o'); plt.title('Mean Excess (downside)'); plt.xlabel('u'); plt.ylabel('E[u - X | X≤u]'); plt.tight_layout(); plt.savefig(REPORTS/'audit_mean_excess.png', dpi=150); plt.close()\n",
      "    var95, cvar95 = empirical_var_cvar(s_sorted, alpha=0.95)\n",
      "    out = {'VaR95_emp': var95, 'CVaR95_emp': cvar95}\n",
      "    print(json.dumps(out, indent=2))\n",
      "    (REPORTS/'audit_evt.json').write_text(json.dumps(out, indent=2), encoding='utf-8')\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## VaR Backtests (Kupiec/Christoffersen) — інструкції\n",
      "- Натисни ▶ — перевіримо частоту пробоїв та незалежність.\n",
      "- Потрібна послідовність прибутків/метрики (використаємо IS).\n"
    ]},
    {"cell_type":"code","metadata":{"id":"var_tests"},"execution_count":null,"outputs":[],"source":[
      "def kupiec_pof(x, alpha=0.95):\n",
      "    # exceedance if x <= VaR_alpha (downside)\n",
      "    n=len(x); k = max(1,int((1-alpha)*n)); var = np.sort(x)[k-1]\n",
      "    I = (x<=var).astype(int); N=n; xk=I.sum(); p=1-alpha\n",
      "    # LR_pof = -2 ln( (1-p)^(N-k) p^k / ((1-xk/N)^(N-k) (xk/N)^k) )\n",
      "    if xk==0 or xk==N: return {'LR_pof':np.nan,'exceed_rate':xk/N}\n",
      "    import math\n",
      "    num = ((1-p)**(N-xk))*(p**xk)\n",
      "    den = ((1-xk/N)**(N-xk))*((xk/N)**xk)\n",
      "    lr = -2*math.log(num/den)\n",
      "    return {'LR_pof': float(lr), 'exceed_rate': float(xk/N)}\n",
      "\n",
      "def christoffersen_ind(x, alpha=0.95):\n",
      "    n=len(x); k = max(1,int((1-alpha)*n)); var = np.sort(x)[k-1]\n",
      "    I = (x<=var).astype(int)\n",
      "    # transition counts\n",
      "    n00=n01=n10=n11=0\n",
      "    for i in range(1,n):\n",
      "        a,b = I[i-1], I[i]\n",
      "        if a==0 and b==0: n00+=1\n",
      "        elif a==0 and b==1: n01+=1\n",
      "        elif a==1 and b==0: n10+=1\n",
      "        else: n11+=1\n",
      "    import math\n",
      "    pi0 = n01/max(1,(n00+n01)); pi1 = n11/max(1,(n10+n11)); pi = (n01+n11)/max(1,(n00+n01+n10+n11))\n",
      "    def L(a,b,p):\n",
      "        return ((1-p)**a)*(p**b) if a>=0 and b>=0 else 1.0\n",
      "    L0 = L(n00+n10, n01+n11, pi)\n",
      "    L1 = L(n00, n01, pi0)*L(n10, n11, pi1)\n",
      "    if L0<=0 or L1<=0: return {'LR_ind':np.nan,'n00':n00,'n01':n01,'n10':n10,'n11':n11}\n",
      "    LR_ind = -2*math.log(L0/L1)\n",
      "    return {'LR_ind': float(LR_ind), 'n00':n00,'n01':n01,'n10':n10,'n11':n11}\n",
      "\n",
      "if df.empty or ('is_bps' not in df.columns and 'IS_bps' not in df.columns):\n",
      "    print('No IS data for VaR tests.')\n",
      "else:\n",
      "    s = pd.to_numeric(df['is_bps'] if 'is_bps' in df.columns else df['IS_bps'], errors='coerce').dropna().values\n",
      "    out = {'kupiec': kupiec_pof(s,0.95), 'christoffersen': christoffersen_ind(s,0.95)}\n",
      "    print(json.dumps(out, indent=2))\n",
      "    (REPORTS/'audit_var_tests.json').write_text(json.dumps(out, indent=2), encoding='utf-8')\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Walk-Forward Stability — інструкції\n",
      "- Натисни ▶ — розібʼємо дані на k-вікон по часу (за `ts_ms`/індексом), порахуємо EΠ та CVaR по кожному вікну.\n",
      "- Це показує стабільність/крихкість результатів у часі.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"walk"},"execution_count":null,"outputs":[],"source":[
      "def cvar_downside(x, alpha=0.95):\n",
      "    x=np.sort(x); n=len(x); k=max(1,int((1-alpha)*n)); return float(np.mean(x[:k])) if k>0 else (x[0] if n>0 else np.nan)\n",
      "\n",
      "def walk_forward(df, k=5):\n",
      "    if df.empty: return []\n",
      "    # time-key\n",
      "    if 'ts_ms' in df.columns: t = pd.to_numeric(df['ts_ms'], errors='coerce').fillna(method='ffill').fillna(0)\n",
      "    elif 'ts' in df.columns: t = pd.to_numeric(df['ts'], errors='coerce').fillna(method='ffill').fillna(0)\n",
      "    else: t = pd.Series(range(len(df)))\n",
      "    order = np.argsort(t.values)\n",
      "    idx_chunks = np.array_split(order, k)\n",
      "    series = pd.to_numeric(df['is_bps'] if 'is_bps' in df.columns else df.get('IS_bps', np.nan), errors='coerce').values\n",
      "    out=[]\n",
      "    for i,chunk in enumerate(idx_chunks):\n",
      "        s = series[chunk]\n",
      "        s = s[~np.isnan(s)]\n",
      "        if len(s)==0: out.append({'win':i,'mean_IS':np.nan,'CVaR95':np.nan}); continue\n",
      "        out.append({'win':i,'mean_IS': float(np.mean(s)), 'CVaR95': float(cvar_downside(s,0.95))})\n",
      "    return out\n",
      "\n",
      "wf = walk_forward(df, k=5)\n",
      "print(json.dumps(wf, indent=2))\n",
      "(REPORTS/'audit_walk_forward.json').write_text(json.dumps(wf, indent=2), encoding='utf-8')\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Audit Summary — інструкції\n",
      "- Натисни ▶ — зберемо коротке резюме /content/drive/MyDrive/reports/audit_summary.md.\n"
    ]},
    {"cell_type":"code","metadata":{"id":"summary"},"execution_count":null,"outputs":[],"source":[
      "lines=[\n",
      "  '# Aurora Audit Summary',\n",
      "  '',\n",
      "  'Artifacts:',\n",
      "  '- audit_tca_checks.json — TCA тотожність та знакові конвенції',\n",
      "  '- audit_bootstrap.json — Bootstrap CI (IS, Sharpe)',\n",
      "  '- audit_evt.json — EVT (емпіричні VaR/CVaR, mean-excess.png)',\n",
      "  '- audit_var_tests.json — Kupiec/Christoffersen тести',\n",
      "  '- audit_walk_forward.json — walk-forward стабільність',\n",
      "]\n",
      "(REPORTS/'audit_summary.md').write_text('\n'.join(lines), encoding='utf-8')\n",
      "print('Wrote', REPORTS/'audit_summary.md')\n"
    ]}
  ],
  "metadata": {"kernelspec": {"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11"}},
  "nbformat": 4,
  "nbformat_minor": 5
}

