{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aurora — SSOT + XAI/TCA/Risk Analysis with Optuna\n",
        "\n",
        "This Colab notebook loads SSOT config, ingests `*.jsonl.gz` logs, computes metrics, visualizes distributions, manages local profiles, runs multi-objective Optuna (maximize EΠ_after_TCA, minimize CVaR95), renders a Pareto frontier, and performs a simple OOS re-evaluation of top configs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ІНСТРУКЦІЇ (Setup):\\n",
        "- Натисни ▶ у наступній комірці, щоб встановити бібліотеки.\\n",
        "- Дочекайся зеленої галочки, потім переходь далі.\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_colab"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни кнопку ▶ зліва від цієї комірки.\n",
        "# Вона встановить потрібні бібліотеки у Colab.\n",
        "# Просто натисни і зачекай, поки з'явиться зелена галочка ліворуч.\n",
        "# Якщо не у Colab, комірка теж безпечна.\n",
        "# Після завершення переходь до наступної комірки.\n",
        "# Setup - install extras only if running in Colab\n",
        "import sys, subprocess, os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    %pip -q install pandas numpy seaborn matplotlib plotly optuna tomli ipywidgets\n",
        "\n",
        "import json, gzip, math, io\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Plotly is optional; use matplotlib fallback if unavailable\n",
        "try:\n",
        "    import plotly.express as px\n",
        "except Exception:\n",
        "    px = None\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "except Exception as e:\n",
        "    raise RuntimeError('Optuna is required. Please install optuna.') from e\n",
        "\n",
        "# Widgets\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "except Exception:\n",
        "    widgets = None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env_root"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб знайти кореневу папку проекту.\n",
        "# Якщо з'явиться запит на шлях - встав повний шлях до папки, де є папка 'configs'.\n",
        "# У Colab зазвичай шлях вже правильний. Після виконання побачиш текст 'Using ROOT = ...'.\n",
        "# Environment - detect repository root (expects 'configs/' to exist)\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT / 'configs').exists():\n",
        "    # If running outside the repo root in Colab, let user set a custom path\n",
        "    print('Repository root not found in current directory.')\n",
        "    ROOT = Path(input('Enter absolute path to repo root (containing configs/): ').strip())\n",
        "assert (ROOT / 'configs').exists(), 'configs/ directory not found at ROOT'\n",
        "REPORTS = ROOT / 'reports'\n",
        "REPORTS.mkdir(parents=True, exist_ok=True)\n",
        "print('Using ROOT=', ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paths (Google Drive)\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- У Colab дані і конфіги мають бути у `/content/drive/MyDrive`.\n",
        "- Натисни ▶ у наступній комірці — ми зафіксуємо ROOT на MyDrive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "force_paths"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Force ROOT to MyDrive paths in Colab\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive as _drive\n",
        "        _drive.mount('/content/drive', force_remount=False)\n",
        "    except Exception as _e:\n",
        "        print('Drive mount warning:', _e)\n",
        "    from pathlib import Path as _P\n",
        "    ROOT = _P('/content/drive/MyDrive')\n",
        "    REPORTS = ROOT / 'reports'\n",
        "    REPORTS.mkdir(parents=True, exist_ok=True)\n",
        "    print('Using ROOT=', ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssot_load"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб завантажити конфігурацію (SSOT).\n",
        "# Після виконання побачиш перелік секцій конфігу у вигляді таблиці.\n",
        "# SSOT loading - configs/schema.json + configs/default.toml\n",
        "# Prefer project loader; fallback to raw TOML if import fails.\n",
        "def _toml_load_bytes(b):\n",
        "    try:\n",
        "        import tomllib\n",
        "        return tomllib.loads(b.decode('utf-8') if isinstance(b, (bytes, bytearray)) else b)\n",
        "    except Exception:\n",
        "        import tomli\n",
        "        return tomli.loads(b.decode('utf-8') if isinstance(b, (bytes, bytearray)) else b)\n",
        "\n",
        "def deep_get(d, path, default=None):\n",
        "    cur = d\n",
        "    for p in path.split('.'):\n",
        "        if not isinstance(cur, dict) or p not in cur:\n",
        "            return default\n",
        "        cur = cur[p]\n",
        "    return cur\n",
        "\n",
        "def deep_merge(a: dict, b: dict) -> dict:\n",
        "    out = json.loads(json.dumps(a))\n",
        "    for k, v in (b or {}).items():\n",
        "        if isinstance(v, dict) and isinstance(out.get(k), dict):\n",
        "            out[k] = deep_merge(out[k], v)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def normalize_profile_flat_keys(profile: dict) -> dict:\n",
        "    mapping = {\n",
        "        'execution_sla_max_latency_ms': ('execution','sla','max_latency_ms'),\n",
        "        'replay_chunk_minutes': ('replay','chunk_minutes'),\n",
        "        'sizing_max_position_usd': ('sizing','limits','max_notional_usd'),\n",
        "        'sizing_leverage': ('sizing','limits','leverage_max'),\n",
        "        'universe_top_n': ('universe','ranking','top_n'),\n",
        "        'universe_spread_bps_limit': ('execution','router','spread_limit_bps'),\n",
        "        'reward_ttl_minutes': ('reward','ttl_minutes'),\n",
        "        'xai_sample_every': ('xai','decision_log','sample_every'),\n",
        "        'xai_level': ('xai','decision_log','level'),\n",
        "        'xai_sig': ('xai','decision_log','enabled'),\n",
        "        'leadlag_enable': ('leadlag','enable'),\n",
        "        'leadlag_lag_grid': ('leadlag','lag_grid'),\n",
        "    }\n",
        "    out = {}\n",
        "    for k, v in profile.items():\n",
        "        if k in mapping:\n",
        "            parts = mapping[k]\n",
        "            cur = out\n",
        "            for p in parts[:-1]:\n",
        "                cur = cur.setdefault(p, {})\n",
        "            cur[parts[-1]] = v\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def load_ssot(config_path=None, schema_path=None):\n",
        "    config_path = config_path or (ROOT / 'configs' / 'default.toml')\n",
        "    schema_path = schema_path or (ROOT / 'configs' / 'schema.json')\n",
        "    try:\n",
        "        sys.path.insert(0, str(ROOT))\n",
        "        from core.config.loader import load_config\n",
        "        cfg = load_config(config_path=config_path, schema_path=schema_path, enable_watcher=False)\n",
        "        cfgd = cfg.as_dict()\n",
        "    except Exception:\n",
        "        cfgd = _toml_load_bytes(Path(config_path).read_bytes())\n",
        "        # Schema defaults best-effort: we won't re-validate here.\n",
        "    return cfgd\n",
        "\n",
        "CFG = load_ssot()\n",
        "sections = {\n",
        "    'sizing': CFG.get('sizing', {}),\n",
        "    'risk': CFG.get('risk', {}),\n",
        "    'execution': CFG.get('execution', {}),\n",
        "    'reward': CFG.get('reward', {}),\n",
        "    'tca': CFG.get('tca', {}),\n",
        "    'features': CFG.get('features', {}),\n",
        "    'universe': CFG.get('universe', {}),\n",
        "    'sim_local': CFG.get('order_sink', {}).get('sim_local', {}),\n",
        "}\n",
        "print('Loaded SSOT sections:', ', '.join(sections.keys()))\n",
        "pd.DataFrame({k:[str(type(v).__name__), len(v) if hasattr(v, 'items') else ''] for k,v in sections.items()}, index=['type','size'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Logs (*.jsonl.gz)\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Поклади файли з логами до `/content/drive/MyDrive` (напр. `datasets/`).\n",
        "- Натисни ▶ у наступній комірці — спочатку пошукаємо `*.jsonl.gz` у MyDrive.\n",
        "- Якщо файлів не знайдено — з’явиться вікно, де можна обрати `*.jsonl.gz` вручну.\n",
        "- Після виконання шукай текст: `Loaded rows: N`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "load_logs"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб завантажити логи.\n",
        "# У Colab з'явиться вікно вибору файлів - обери свої *.jsonl.gz.\n",
        "# Коли все пройде, побачиш напис 'Loaded rows: N'. Якщо N=0 - дані не знайдено, це не помилка.\n",
        "# In Colab, use the upload widget; otherwise specify local paths.\n",
        "LOG_FILES = []  # you can set to list of absolute paths\n",
        "if IN_COLAB and not LOG_FILES:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        LOG_FILES = [Path(k) for k in uploaded.keys()]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def read_jsonl_any(path: Path) -> list:\n",
        "    buf = ''\n",
        "    try:\n",
        "        if str(path).endswith('.gz'):\n",
        "            with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
        "                buf = f.read()\n",
        "        else:\n",
        "            buf = Path(path).read_text(encoding='utf-8')\n",
        "    except Exception:\n",
        "        return []\n",
        "    out = []\n",
        "    for line in buf.splitlines():\n",
        "        try:\n",
        "            out.append(json.loads(line))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "def load_logs_to_df(paths):\n",
        "    records = []\n",
        "    for p in paths:\n",
        "        records.extend(read_jsonl_any(Path(p)))\n",
        "    if not records:\n",
        "        return pd.DataFrame()\n",
        "    return pd.json_normalize(records)\n",
        "\n",
        "df_logs = load_logs_to_df(LOG_FILES)\n",
        "print('Loaded rows:', len(df_logs))\n",
        "display(df_logs.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse and Compute Metrics\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Натисни ▶ у наступній комірці, щоб порахувати метрики.\n",
        "- Результат буде у вигляді JSON з блоками: order_level, session_level, tca, risk.\n",
        "- Якщо в логах бракує деяких полів — частина значень може бути NaN (це нормально).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "compute_metrics"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб порахувати метрики.\n",
        "# Зачекай, і внизу побачиш JSON з результатами. Якщо деякі значення NaN - це нормально при малих даних.\n",
        "# Helpers for lifecycle latencies (nearest-rank percentiles)\n",
        "def _percentiles(values, qs=[50,95,99]):\n",
        "    if not len(values):\n",
        "        return {q:0.0 for q in qs}\n",
        "    arr = sorted(values)\n",
        "    n = len(arr)\n",
        "    out = {}\n",
        "    for q in qs:\n",
        "        k = max(1, min(n, int((q/100.0)*n + 0.9999)))\n",
        "        out[q] = float(arr[k-1])\n",
        "    return out\n",
        "\n",
        "def correlate_latencies(df: pd.DataFrame):\n",
        "    # Expect columns: 'cid'/'oid', 'ts_ns' or 'ts_ms', 'event_code' or similar\n",
        "    if df.empty:\n",
        "        return {'submit_ack':{'p50':0,'p95':0,'p99':0}, 'ack_done':{'p50':0,'p95':0,'p99':0}}\n",
        "    by = {}\n",
        "    def norm_ts_ns(row):\n",
        "        if pd.notna(row.get('ts_ns')):\n",
        "            try:\n",
        "                return int(row['ts_ns'])\n",
        "            except Exception:\n",
        "                pass\n",
        "        if pd.notna(row.get('ts_ms')):\n",
        "            return int(float(row['ts_ms'])*1e6)\n",
        "        if pd.notna(row.get('ts')):\n",
        "            t = float(row['ts'])\n",
        "            if t >= 1e12:\n",
        "                return int(t*1e6)\n",
        "            return int(t*1e9)\n",
        "        return None\n",
        "    for _, r in df.iterrows():\n",
        "        cid = r.get('cid')\n",
        "        oid = r.get('oid')\n",
        "        key = str(cid or oid)\n",
        "        if not key:\n",
        "            continue\n",
        "        st = by.setdefault(key, {'submit':None,'ack':None,'done':None})\n",
        "        ts = norm_ts_ns(r)\n",
        "        et = str(r.get('event_code') or r.get('type') or r.get('event') or '').upper().replace('.', '_')\n",
        "        if et.endswith('ORDER_SUBMIT') or et=='ORDER_SUBMIT':\n",
        "            st['submit'] = ts\n",
        "        elif et.endswith('ORDER_ACK') or et=='ORDER_ACK':\n",
        "            st['ack'] = ts\n",
        "        elif et.endswith('ORDER_CANCEL') or et=='ORDER_CANCEL' or et.endswith('ORDER_REJECT') or et.endswith('ORDER_EXPIRE') or et.endswith('ORDER_FILL'):\n",
        "            st['done'] = ts\n",
        "    submit_ack = []\n",
        "    ack_done = []\n",
        "    for st in by.values():\n",
        "        if st['submit'] and st['ack']:\n",
        "            submit_ack.append((st['ack']-st['submit'])/1e6)\n",
        "        if st['ack'] and st['done']:\n",
        "            ack_done.append((st['done']-st['ack'])/1e6)\n",
        "    p1 = _percentiles(submit_ack)\n",
        "    p2 = _percentiles(ack_done)\n",
        "    return {'submit_ack':{'p50':p1[50],'p95':p1[95],'p99':p1[99]}, 'ack_done':{'p50':p2[50],'p95':p2[95],'p99':p2[99]}}\n",
        "\n",
        "def compute_metrics(df: pd.DataFrame):\n",
        "    out = {}\n",
        "    if df.empty:\n",
        "        return out\n",
        "    # Order-level ratios\n",
        "    qty = df.get('order_qty') or df.get('qty')\n",
        "    filled = df.get('filled_qty') or df.get('fill_qty')\n",
        "    fill_ratio = None\n",
        "    try:\n",
        "        q = pd.to_numeric(qty, errors='coerce')\n",
        "        f = pd.to_numeric(filled, errors='coerce')\n",
        "        fill_ratio = (f / q).clip(0,1)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Maker/Taker flags (best-effort)\n",
        "    side = (df.get('liquidity') or df.get('taker_maker') or df.get('context.liquidity'))\n",
        "    maker_mask = side.astype(str).str.lower().str.contains('maker') if side is not None else pd.Series([False]*len(df))\n",
        "    maker_fill_ratio = None\n",
        "    if fill_ratio is not None:\n",
        "        maker_fill_ratio = fill_ratio[maker_mask].mean() if maker_mask.any() else np.nan\n",
        "    # Cancel ratio\n",
        "    code = (df.get('event_code') or df.get('status') or df.get('final'))\n",
        "    is_cancel = code.astype(str).str.upper().str.contains('CANCEL') if code is not None else pd.Series([False]*len(df))\n",
        "    cancel_ratio = float(is_cancel.mean()) if len(is_cancel)>0 else np.nan\n",
        "    # Latencies from lifecycle events (if a separate events log exists, re-run with that DataFrame)\n",
        "    lat = correlate_latencies(df)\n",
        "    # Slippage/IS\n",
        "    slip = pd.to_numeric(df.get('slippage_bps'), errors='coerce') if 'slippage_bps' in df.columns else None\n",
        "    is_bps = pd.to_numeric(df.get('is_bps'), errors='coerce') if 'is_bps' in df.columns else None\n",
        "    # Session-level\n",
        "    trades = int((code.astype(str).str.upper().str.contains('FILL').sum()) if code is not None else len(df))\n",
        "    # Trades per minute heuristic\n",
        "    tms = pd.to_numeric(df.get('ts_ms'), errors='coerce') if 'ts_ms' in df.columns else None\n",
        "    tsec = pd.to_numeric(df.get('ts'), errors='coerce') if 'ts' in df.columns else None\n",
        "    if tms is not None and tms.notna().any():\n",
        "        dur_min = (tms.max()-tms.min())/60000.0\n",
        "    elif tsec is not None and tsec.notna().any():\n",
        "        dur_min = (tsec.max()-tsec.min())/60.0\n",
        "    else:\n",
        "        dur_min = np.nan\n",
        "    trades_per_min = (trades/max(1e-9, dur_min)) if not math.isnan(dur_min) else np.nan\n",
        "    # Sharpe_daily proxy from IS series (bps)\n",
        "    sharpe_daily = np.nan\n",
        "    ref = is_bps if is_bps is not None and is_bps.notna().any() else ( -slip if slip is not None else None )\n",
        "    if ref is not None:\n",
        "        s = ref.dropna()\n",
        "        if len(s) >= 2:\n",
        "            mu, sd = s.mean(), s.std(ddof=1)\n",
        "            sharpe_daily = (mu / (sd+1e-9)) * np.sqrt(1440.0)  # 1-min bars proxy\n",
        "    # HitRate\n",
        "    hit_rate = float((ref>0).mean()) if ref is not None and len(ref.dropna())>0 else np.nan\n",
        "    # kappa (bps/ms): mean(IS)/mean(lat_ms)\n",
        "    kappa = np.nan\n",
        "    lat_ack = lat['submit_ack']['p50'] if lat else np.nan\n",
        "    if ref is not None and not math.isnan(lat_ack) and lat_ack>0:\n",
        "        kappa = float(ref.dropna().mean())/float(lat_ack)\n",
        "    # TCA components (if present)\n",
        "    comp = {}\n",
        "    for k in ['is_bps','spread_bps','latency_bps','adverse_bps','impact_bps','fees_bps']:\n",
        "        if k in df.columns:\n",
        "            comp[k] = float(pd.to_numeric(df[k], errors='coerce').dropna().mean())\n",
        "    # Risk metrics: CVaR95 on reference series (downside)\n",
        "    cvar95 = np.nan\n",
        "    if ref is not None and len(ref.dropna())>=20:\n",
        "        x = ref.dropna().values.astype(float)\n",
        "        q = np.quantile(x, 0.05)\n",
        "        tail = x[x<=q]\n",
        "        cvar95 = -float(tail.mean())  # positive number for downside magnitude\n",
        "    # deny-rate, SLA breach\n",
        "    deny = df.get('reason_code') if 'reason_code' in df.columns else df.get('deny_reason')\n",
        "    deny_rate = float(pd.notna(deny).mean()) if deny is not None else 0.0\n",
        "    sla_max = deep_get(CFG, 'execution.sla.max_latency_ms', 25)\n",
        "    sla_breach = float((pd.Series([lat_ack]) > float(sla_max)).mean()) if not math.isnan(lat_ack) else np.nan\n",
        "    # Pack\n",
        "    out = {\n",
        "        'order_level': {\n",
        "            'fill_ratio_mean': float(fill_ratio.mean()) if fill_ratio is not None else np.nan,\n",
        "            'maker_fill_ratio': float(maker_fill_ratio) if maker_fill_ratio is not None else np.nan,\n",
        "            'cancel_ratio': float(cancel_ratio),\n",
        "            'latency_ms': lat,\n",
        "            'slip_bps_mean': float(slip.mean()) if slip is not None else np.nan,\n",
        "        },\n",
        "        'session_level': {\n",
        "            'trades_per_min': float(trades_per_min),\n",
        "            'sharpe_daily': float(sharpe_daily) if not math.isnan(sharpe_daily) else np.nan,\n",
        "            'hit_rate': float(hit_rate) if not math.isnan(hit_rate) else np.nan,\n",
        "            'kappa_bps_per_ms': float(kappa) if not math.isnan(kappa) else np.nan,\n",
        "        },\n",
        "        'tca': {\n",
        "            'IS_bps': float(comp.get('is_bps', np.nan)),\n",
        "            'Spread_bps': float(comp.get('spread_bps', np.nan)),\n",
        "            'Latency_bps': float(comp.get('latency_bps', np.nan)),\n",
        "            'Adverse_bps': float(comp.get('adverse_bps', np.nan)),\n",
        "            'Impact_bps': float(comp.get('impact_bps', np.nan)),\n",
        "            'Fees_bps': float(comp.get('fees_bps', np.nan)),\n",
        "        },\n",
        "        'risk': {\n",
        "            'CVaR95': float(cvar95) if not math.isnan(cvar95) else np.nan,\n",
        "            'deny_rate': float(deny_rate),\n",
        "            'SLA_breach': float(sla_breach) if not math.isnan(sla_breach) else np.nan,\n",
        "        },\n",
        "    }\n",
        "    return out, ref\n",
        "\n",
        "metrics, ref_series = compute_metrics(df_logs)\n",
        "print(json.dumps(metrics, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Натисни ▶ у наступній комірці — побачиш графіки: IS/слiпідж, fill_ratio, deny-коди, CVaR хвіст.\n",
        "- Якщо даних мало — графіки можуть бути порожні або короткі.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "viz"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб намалювати графіки.\n",
        "# Після виконання прокрути вниз - побачиш гістограми та стовпчики.\n",
        "# IS distribution, fill_ratio (if inferable), deny codes, CVaR tail\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16,4))\n",
        "if 'is_bps' in df_logs.columns:\n",
        "    sns.histplot(pd.to_numeric(df_logs['is_bps'], errors='coerce').dropna(), bins=50, ax=axes[0])\n",
        "    axes[0].set_title('IS_bps distribution')\n",
        "else:\n",
        "    sns.histplot(pd.to_numeric(df_logs.get('slippage_bps', pd.Series(dtype=float)), errors='coerce').dropna(), bins=50, ax=axes[0])\n",
        "    axes[0].set_title('slip_bps distribution')\n",
        "# Fill ratio proxy\n",
        "qty = pd.to_numeric(df_logs.get('order_qty', np.nan), errors='coerce')\n",
        "fqty = pd.to_numeric(df_logs.get('filled_qty', np.nan), errors='coerce')\n",
        "fr = (fqty/qty).clip(0,1) if isinstance(qty, pd.Series) and isinstance(fqty, pd.Series) else pd.Series(dtype=float)\n",
        "sns.histplot(fr.dropna(), bins=50, ax=axes[1])\n",
        "axes[1].set_title('fill_ratio distribution')\n",
        "# Deny codes\n",
        "deny = df_logs.get('reason_code') if 'reason_code' in df_logs.columns else df_logs.get('deny_reason')\n",
        "if deny is not None:\n",
        "    top = deny.astype(str).value_counts().head(10)\n",
        "    sns.barplot(x=top.values, y=top.index, ax=axes[2])\n",
        "    axes[2].set_title('Top deny codes')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# CVaR tail plot\n",
        "if ref_series is not None and isinstance(ref_series, pd.Series) and len(ref_series.dropna())>10:\n",
        "    x = ref_series.dropna().sort_values()\n",
        "    n = len(x)\n",
        "    cut = int(max(1, math.floor(0.05*n)))\n",
        "    tail = x.iloc[:cut]\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.histplot(tail, bins=30)\n",
        "    plt.title('CVaR95 tail (worst 5%)')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiles (local_low/mid/high) and Effective Config\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Натисни ▶ у наступній комірці — згенеруємо ефективний конфіг для обраного профілю.\n",
        "- Файл буде збережено в `/content/drive/MyDrive/reports/effect_<profile>.toml`.\n",
        "- Далі відкрий секцію 'Interactive Controls' для зміни параметрів.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "profiles"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб підготувати профілі і зібрати ефективний конфіг.\n",
        "# Після цього переходь до наступної секції 'Interactive Controls'. Також буде збережено effect_<profile>.toml у reports.\n",
        "profiles = CFG.get('profile', {})\n",
        "print('Available profiles:', list(profiles.keys()))\n",
        "SELECTED_PROFILE = 'local_mid'  # change as needed\n",
        "prof_raw = profiles.get(SELECTED_PROFILE, {})\n",
        "prof = normalize_profile_flat_keys(prof_raw)\n",
        "effective_cfg = deep_merge(CFG, prof)\n",
        "print('Effective config overlayed with profile =', SELECTED_PROFILE)\n",
        "# Helper: minimal TOML dumper\n",
        "def dump_toml(d: dict, prefix: str = '') -> str:\n",
        "    lines = []\n",
        "    simple = {k:v for k,v in d.items() if not isinstance(v, dict)}\n",
        "    nested = {k:v for k,v in d.items() if isinstance(v, dict)}\n",
        "    def _fmt(v):\n",
        "        if isinstance(v, str): return f'\"{v}\"'\n",
        "        if isinstance(v, bool): return str(v).lower()\n",
        "        return json.dumps(v) if isinstance(v, (list, tuple)) else str(v)\n",
        "    for k, v in simple.items():\n",
        "        lines.append(f\"{k} = {_fmt(v)}\")\n",
        "    for k, v in nested.items():\n",
        "        table = f\"{prefix}{k}\" if not prefix else f\"{prefix}.{k}\"\n",
        "        lines.append('')\n",
        "        lines.append(f\"[{table}]\")\n",
        "        lines.extend(dump_toml(v, table).splitlines())\n",
        "    return '\n'.join(lines)\n",
        "\n",
        "# Preview a few key sections\n",
        "for sec in ['sizing','risk','execution','tca','features','universe','order_sink']:\n",
        "    if sec in effective_cfg:\n",
        "        print(f'[{sec}] keys:', list(effective_cfg[sec].keys())[:10])\n",
        "\n",
        "# Optionally write a profile-effective TOML to reports (for review)\n",
        "eff_text = dump_toml(effective_cfg)\n",
        "(REPORTS / f'effect_{SELECTED_PROFILE}.toml').write_text(eff_text, encoding='utf-8')\n",
        "print('Wrote', REPORTS / f'effect_{SELECTED_PROFILE}.toml')\n",
        "# Preview key parameters requested: sizing max_position_usd, universe top_n/spread_bps_limit, reward TTL\n",
        "print('sizing.limits.max_notional_usd =', deep_get(effective_cfg, 'sizing.limits.max_notional_usd'))\n",
        "print('universe.ranking.top_n =', deep_get(effective_cfg, 'universe.ranking.top_n', 'N/A'))\n",
        "print('execution.router.spread_limit_bps =', deep_get(effective_cfg, 'execution.router.spread_limit_bps', 'N/A'))\n",
        "print('reward.ttl_minutes =', deep_get(effective_cfg, 'reward.ttl_minutes', 'N/A'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Controls\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Обери профіль (local_low/mid/high) у випадаючому меню.\n",
        "- Посунь слайдери: top_n, TTL (min), max_notional, spread_bps.\n",
        "- Заповни числа: latency_ms, leverage.\n",
        "- Натисни 'Apply to Config' — конфіг оновиться та запишеться до reports/.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "controls"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: 1) Обери профіль у випадаючому списку. 2) Посунь слайдери і зміни числа як хочеш.\n",
        "# 3) Натисни кнопку 'Apply to Config'. 4) Читай повідомлення нижче - там будуть обрані значення.\n",
        "# Dropdown for profile; sliders and numeric inputs for core knobs.\n",
        "if widgets is None:\n",
        "    print('ipywidgets not available. Install ipywidgets to use controls.')\n",
        "else:\n",
        "    prof_opts = list(CFG.get('profile', {}).keys()) or ['local_low','local_mid','local_high']\n",
        "    prof_dd = widgets.Dropdown(options=prof_opts, value=SELECTED_PROFILE if 'SELECTED_PROFILE' in globals() else (prof_opts[0] if prof_opts else None), description='Profile')\n",
        "    # Defaults from current effective_cfg or sensible fallbacks\n",
        "    def _dv(path, dv):\n",
        "        return deep_get(effective_cfg if 'effective_cfg' in globals() else CFG, path, dv)\n",
        "    top_n = int(_dv('universe.ranking.top_n', 10) or 10)\n",
        "    ttl = int(_dv('reward.ttl_minutes', 120) or 120)\n",
        "    max_notional = float(_dv('sizing.limits.max_notional_usd', 5000.0) or 5000.0)\n",
        "    spread_bps = float(_dv('execution.router.spread_limit_bps', 30.0) or 30.0)\n",
        "    latency_ms = int(_dv('execution.sla.max_latency_ms', 25) or 25)\n",
        "    leverage = float(_dv('sizing.limits.leverage_max', 2.0) or 2.0)\n",
        "    s_topn = widgets.IntSlider(value=top_n, min=1, max=100, step=1, description='top_n')\n",
        "    s_ttl = widgets.IntSlider(value=ttl, min=5, max=720, step=5, description='TTL (min)')\n",
        "    s_maxn = widgets.FloatSlider(value=max_notional, min=100.0, max=50000.0, step=100.0, readout_format='.0f', description='max_notional')\n",
        "    s_spread = widgets.FloatSlider(value=spread_bps, min=1.0, max=200.0, step=1.0, description='spread_bps')\n",
        "    i_latency = widgets.BoundedIntText(value=latency_ms, min=1, max=1000, step=1, description='latency_ms')\n",
        "    f_lev = widgets.BoundedFloatText(value=leverage, min=1.0, max=25.0, step=0.1, description='leverage')\n",
        "    apply_btn = widgets.Button(description='Apply to Config', button_style='success')\n",
        "    out = widgets.Output()\n",
        "\n",
        "    def on_apply(_):\n",
        "        global SELECTED_PROFILE, effective_cfg\n",
        "        SELECTED_PROFILE = prof_dd.value\n",
        "        prof_raw = CFG.get('profile', {}).get(SELECTED_PROFILE, {})\n",
        "        prof_norm = normalize_profile_flat_keys(prof_raw)\n",
        "        # Overlay profile first\n",
        "        cfg1 = deep_merge(CFG, prof_norm)\n",
        "        # Then overlay widget values\n",
        "        overlay = {\n",
        "            'universe': {'ranking': {'top_n': int(s_topn.value)}},\n",
        "            'reward': {'ttl_minutes': int(s_ttl.value)},\n",
        "            'sizing': {'limits': {'max_notional_usd': float(s_maxn.value), 'leverage_max': float(f_lev.value)}},\n",
        "            'execution': {'router': {'spread_limit_bps': float(s_spread.value)}, 'sla': {'max_latency_ms': int(i_latency.value)}},\n",
        "        }\n",
        "        effective_cfg = deep_merge(cfg1, overlay)\n",
        "        # Persist\n",
        "        (REPORTS / f'effect_{SELECTED_PROFILE}.toml').write_text(dump_toml(effective_cfg), encoding='utf-8')\n",
        "        with out:\n",
        "            clear_output(wait=True)\n",
        "            print('Applied. Effective config saved to', REPORTS / f'effect_{SELECTED_PROFILE}.toml')\n",
        "            print('top_n =', deep_get(effective_cfg, 'universe.ranking.top_n'))\n",
        "            print('reward.ttl_minutes =', deep_get(effective_cfg, 'reward.ttl_minutes'))\n",
        "            print('max_notional_usd =', deep_get(effective_cfg, 'sizing.limits.max_notional_usd'))\n",
        "            print('spread_limit_bps =', deep_get(effective_cfg, 'execution.router.spread_limit_bps'))\n",
        "            print('latency_ms =', deep_get(effective_cfg, 'execution.sla.max_latency_ms'))\n",
        "            print('leverage_max =', deep_get(effective_cfg, 'sizing.limits.leverage_max'))\n",
        "\n",
        "    apply_btn.on_click(on_apply)\n",
        "    display(widgets.VBox([prof_dd, widgets.HBox([s_topn, s_ttl]), widgets.HBox([s_maxn, s_spread]), widgets.HBox([i_latency, f_lev]), apply_btn, out]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna: Multi-objective Study (maximize EΠ_after_TCA, minimize CVaR95)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "optuna"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб запустити оптимізацію (Optuna).\n",
        "# Це займе кілька хвилин. Після завершення дивись файли у папці reports: optuna_frontier.png, optuna_best_local.json, aurora_suggested.toml.\n",
        "# Define a search space guided by schema/defaults (best-effort if keys exist)\n",
        "space = {\n",
        "    'sizing.limits.max_notional_usd': (100.0, 20000.0, 'float'),\n",
        "    'sizing.limits.leverage_max': (1.0, 5.0, 'float'),\n",
        "    'execution.sla.max_latency_ms': (10, 80, 'int'),\n",
        "    'execution.router.spread_limit_bps': (10.0, 120.0, 'float'),\n",
        "    'tca.adverse_window_s': (0.5, 5.0, 'float'),\n",
        "    'universe.ranking.top_n': (3, 50, 'int'),\n",
        "    'reward.ttl_minutes': (15, 240, 'int'),\n",
        "}\n",
        "\n",
        "def get_series_is(df):\n",
        "    if 'is_bps' in df.columns:\n",
        "        return pd.to_numeric(df['is_bps'], errors='coerce').dropna()\n",
        "    if 'slippage_bps' in df.columns:\n",
        "        return -pd.to_numeric(df['slippage_bps'], errors='coerce').dropna()\n",
        "    return pd.Series(dtype=float)\n",
        "\n",
        "def evaluate_objectives(cfg: dict, df: pd.DataFrame):\n",
        "    # EΠ_after_TCA proxy = mean(IS_bps - fees - impact) if present, else mean(IS_bps)\n",
        "    is_s = get_series_is(df)\n",
        "    if is_s.empty:\n",
        "        e_pi = -1e6\n",
        "        cvar95 = 1e6\n",
        "        return e_pi, cvar95\n",
        "    fees = pd.to_numeric(df.get('fees_bps', 0.0), errors='coerce').fillna(0.0)\n",
        "    impact = pd.to_numeric(df.get('impact_bps', 0.0), errors='coerce').fillna(0.0)\n",
        "    e_pi = float((is_s - fees[:len(is_s)].values - impact[:len(is_s)].values).mean())\n",
        "    # CVaR95 on IS series downside (positive magnitude)\n",
        "    x = is_s.values.astype(float)\n",
        "    if len(x) >= 20:\n",
        "        q = np.quantile(x, 0.05)\n",
        "        tail = x[x<=q]\n",
        "        cvar95 = -float(tail.mean())\n",
        "    else:\n",
        "        cvar95 = 1e6\n",
        "    return e_pi, cvar95\n",
        "\n",
        "def suggest_and_overlay(trial, base_cfg: dict):\n",
        "    chosen = {}\n",
        "    for key, (lo, hi, typ) in space.items():\n",
        "        if typ=='int':\n",
        "            val = trial.suggest_int(key, int(lo), int(hi))\n",
        "        else:\n",
        "            val = trial.suggest_float(key, float(lo), float(hi))\n",
        "        # set nested\n",
        "        parts = key.split('.')\n",
        "        cur = chosen\n",
        "        for p in parts[:-1]:\n",
        "            cur = cur.setdefault(p, {})\n",
        "        cur[parts[-1]] = val\n",
        "    return deep_merge(base_cfg, chosen), chosen\n",
        "\n",
        "# Multi-objective study\n",
        "study = optuna.create_study(directions=['maximize','minimize'], study_name='aurora_multiobj')\n",
        "N_TRIALS = 30  # adjust as needed\n",
        "\n",
        "def objective(trial):\n",
        "    cfg_try, chosen = suggest_and_overlay(trial, effective_cfg)\n",
        "    e_pi, cvar95 = evaluate_objectives(cfg_try, df_logs)\n",
        "    trial.set_user_attr('params_nested', chosen)\n",
        "    trial.set_user_attr('EPI_after_TCA', e_pi)\n",
        "    trial.set_user_attr('CVaR95', cvar95)\n",
        "    return e_pi, cvar95\n",
        "\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "print('Trials complete:', len(study.trials))\n",
        "# Extract Pareto front\n",
        "front = optuna.visualization.plot_pareto_front(study) if px else None\n",
        "# Save matplotlib fallback if plotly not available\n",
        "pts = [(t.values[0], t.values[1]) for t in study.trials if t.values is not None]\n",
        "plt.figure(figsize=(5,4))\n",
        "if pts:\n",
        "    xs, ys = zip(*pts)\n",
        "    plt.scatter(xs, ys, s=14, alpha=0.7)\n",
        "plt.xlabel('EΠ_after_TCA (bps) — maximize')\n",
        "plt.ylabel('CVaR95 (bps) — minimize')\n",
        "plt.title('Optuna Pareto Frontier')\n",
        "plt.tight_layout()\n",
        "out_png = REPORTS / 'optuna_frontier.png'\n",
        "plt.savefig(out_png, dpi=160)\n",
        "print('Saved pareto front to', out_png)\n",
        "\n",
        "# Save top configurations (by hypervolume rank or simply EPI descending, CVaR asc heuristic)\n",
        "trials_sorted = sorted(study.trials, key=lambda t: (-(t.values[0] if t.values else -1e9), (t.values[1] if t.values else 1e9)))\n",
        "top5 = trials_sorted[:5]\n",
        "best_payload = []\n",
        "for t in top5:\n",
        "    best_payload.append({\n",
        "        'EPI_after_TCA': t.user_attrs.get('EPI_after_TCA'),\n",
        "        'CVaR95': t.user_attrs.get('CVaR95'),\n",
        "        'params': t.user_attrs.get('params_nested')\n",
        "    })\n",
        "best_json = REPORTS / 'optuna_best_local.json'\n",
        "best_json.write_text(json.dumps(best_payload, indent=2), encoding='utf-8')\n",
        "print('Saved top configs to', best_json)\n",
        "\n",
        "# Freeze-suggest a TOML config overlay using the best trial (index 0)\n",
        "if top5:\n",
        "    best_cfg = deep_merge(effective_cfg, top5[0].user_attrs.get('params_nested') or {})\n",
        "    (REPORTS / 'aurora_suggested.toml').write_text(dump_toml(best_cfg), encoding='utf-8')\n",
        "    print('Wrote freeze config:', REPORTS / 'aurora_suggested.toml')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OOS Check: Load another day/symbol and re-evaluate top-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "oos"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ІНСТРУКЦІЯ: Натисни ▶, щоб перевірити на інших даних (OOS).\n",
        "# У Colab завантаж інші файли *.jsonl.gz (інший день чи символ). Внизу з'явиться таблиця з оцінками для топ-5 конфігів.\n",
        "# Load OOS logs\n",
        "OOS_FILES = []\n",
        "if IN_COLAB and not OOS_FILES:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print('Upload OOS log files (*.jsonl.gz) ...')\n",
        "        up2 = files.upload()\n",
        "        OOS_FILES = [Path(k) for k in up2.keys()]\n",
        "    except Exception:\n",
        "        pass\n",
        "df_oos = load_logs_to_df(OOS_FILES)\n",
        "print('OOS rows:', len(df_oos))\n",
        "\n",
        "# Re-evaluate top-5 configs saved earlier\n",
        "best_json = REPORTS / 'optuna_best_local.json'\n",
        "if best_json.exists() and len(df_oos)>0:\n",
        "    top = json.loads(best_json.read_text(encoding='utf-8'))\n",
        "    results = []\n",
        "    for item in top:\n",
        "        overlay = item.get('params') or {}\n",
        "        cfg_try = deep_merge(effective_cfg, overlay)\n",
        "        epi, cvar = evaluate_objectives(cfg_try, df_oos)\n",
        "        results.append({'EPI_after_TCA_OOS': epi, 'CVaR95_OOS': cvar, 'params': overlay})\n",
        "    display(pd.DataFrame(results))\n",
        "else:\n",
        "    print('No top-5 results found or OOS logs empty. Run Optuna and provide OOS logs.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Logs (Historical/Synthetic)\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Якщо маєш історичні дані і можеш запустити Aurora симуляцію — обери метод 'aurora_sim'.\n",
        "- Якщо ні — обери 'synthetic' і згенеруй синтетичні логи на ~цільовий обсяг (наприклад, 1000 МБ ≈ 1 ГБ).\n",
        "- Файли збережуться до `/content/drive/MyDrive/datasets/` і будуть підхоплені секцією Load Logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "gen_logs"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path as _P\n",
        "import gzip, json, os, random, time\n",
        "import math\n",
        "\n",
        "OUT_DIR = _P('/content/drive/MyDrive/datasets') if IN_COLAB else (ROOT / 'datasets')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "METHOD = 'synthetic'  # 'synthetic' or 'aurora_sim'\n",
        "TARGET_MB = 1000       # приблизно 1 ГБ стиснених логів (може відрізнятись)\n",
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "\n",
        "def _rand_choice(p_true=0.5):\n",
        "    return 'maker' if random.random() < p_true else 'taker'\n",
        "\n",
        "def gen_synthetic_logs(target_mb: int, out_path: _P):\n",
        "    # Створюємо gzip-потік і пишемо JSONL рядки поки файл не досягне ~target_mb\n",
        "    n = 0\n",
        "    start = time.time()\n",
        "    with gzip.open(out_path, 'wt', encoding='utf-8') as f:\n",
        "        size_mb = 0.0\n",
        "        while True:\n",
        "            cid = f'c{n}'\n",
        "            ts_ms = int(1_710_000_000_000 + n*50)\n",
        "            qty = max(1.0, random.gauss(1.5, 0.5))\n",
        "            filled = qty if random.random() < 0.8 else random.random()*qty\n",
        "            slip = random.gauss(0.0, 3.0)\n",
        "            is_bps = slip * (-1.0) + random.gauss(1.0, 1.0)\n",
        "            fees = abs(random.gauss(0.5, 0.2))\n",
        "            impact = abs(random.gauss(0.7, 0.3))\n",
        "            spread = abs(random.gauss(20.0, 10.0))\n",
        "            rec = {\n",
        "                'event_code': random.choice(['ORDER.SUBMIT','ORDER.ACK','ORDER.FILL','ORDER.CANCEL']),\n",
        "                'cid': cid,\n",
        "                'ts_ms': ts_ms,\n",
        "                'order_qty': qty,\n",
        "                'filled_qty': filled,\n",
        "                'slippage_bps': slip,\n",
        "                'is_bps': is_bps,\n",
        "                'fees_bps': fees,\n",
        "                'impact_bps': impact,\n",
        "                'spread_bps': spread,\n",
        "                'liquidity': _rand_choice(0.6),\n",
        "            }\n",
        "            if random.random() < 0.05:\n",
        "                rec['reason_code'] = random.choice(['SPREAD_GUARD','VOL_GUARD','CVAR_GUARD','LATENCY_GUARD'])\n",
        "            f.write(json.dumps(rec)+'\n')\n",
        "            n += 1\n",
        "            if n % 5000 == 0:\n",
        "                f.flush()\n",
        "                try:\n",
        "                    size_mb = os.path.getsize(out_path) / (1024*1024.0)\n",
        "                except Exception:\n",
        "                    size_mb = 0.0\n",
        "                if size_mb >= target_mb:\n",
        "                    break\n",
        "    print(f'Generated ~{size_mb:.1f} MB compressed, {n} rows, took {time.time()-start:.1f}s -> {out_path}')\n",
        "\n",
        "def gen_with_aurora_sim(target_mb: int, out_dir: _P):\n",
        "    # Спробувати викликати інструменти з репо для локального сімулятора\n",
        "    # (потрібен повний репозиторій у Colab)\n",
        "    import subprocess, sys\n",
        "    # Базово: запустити dryrun, який генерує JSONL у logs/, далі стиснути і перемістити у datasets/\n",
        "    try:\n",
        "        rc = subprocess.run([sys.executable, str(ROOT/'tools'/'sim_local_dryrun.py')], check=False)\n",
        "        print('sim_local_dryrun exit code:', rc.returncode)\n",
        "    except Exception as e:\n",
        "        print('Aurora sim failed, falling back to synthetic. Error:', e)\n",
        "        return gen_synthetic_logs(target_mb, out_dir/('synth_'+str(int(time.time()))+'.jsonl.gz'))\n",
        "    # Знайти логи у ROOT/logs та стиснути у один файл (приблизно цільовий розмір)\n",
        "    parts = sorted((ROOT/'logs').glob('*.jsonl'))\n",
        "    out_path = out_dir/('aurora_sim_'+str(int(time.time()))+'.jsonl.gz')\n",
        "    rows = 0\n",
        "    size_mb = 0.0\n",
        "    with gzip.open(out_path, 'wt', encoding='utf-8') as gz:\n",
        "        for p in parts:\n",
        "            for line in p.read_text(encoding='utf-8').splitlines():\n",
        "                gz.write(line+'\n')\n",
        "                rows += 1\n",
        "                if rows % 10000 == 0:\n",
        "                    gz.flush()\n",
        "                    size_mb = os.path.getsize(out_path)/(1024*1024.0)\n",
        "                    if size_mb >= target_mb:\n",
        "                        break\n",
        "            if size_mb >= target_mb:\n",
        "                break\n",
        "    print(f'Packed ~{size_mb:.1f} MB from Aurora logs into {out_path} (rows={rows})')\n",
        "\n",
        "# Run generation\n",
        "if METHOD == 'synthetic':\n",
        "    out = OUT_DIR/('synth_'+str(int(time.time()))+'.jsonl.gz')\n",
        "    gen_synthetic_logs(TARGET_MB, out)\n",
        "else:\n",
        "    gen_with_aurora_sim(TARGET_MB, OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Audit (All Metrics & Parameters)\n",
        "\n",
        "ІНСТРУКЦІЇ:\n",
        "- Натисни ▶ у наступній комірці — ми зберемо всі ключові метрики і параметри рішень.\n",
        "- Результати збережуться як: `/content/drive/MyDrive/reports/decision_factors.json` і `decision_audit.md`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "decision_audit"},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Collect decision parameters + computed metrics into a single report\n",
        "def pick_keys(d, paths):\n",
        "    out = {}\n",
        "    for p in paths:\n",
        "        out[p] = deep_get(d, p)\n",
        "    return out\n",
        "\n",
        "cfg_paths = [\n",
        "  'risk.limits.dd_day_bps','risk.limits.position_usd','risk.limits.cvar_usd',\n",
        "  'risk.cvar.limit','risk.cvar.alpha','risk.cvar.method','risk.cvar.lookback',\n",
        "  'execution.sla.max_latency_ms','execution.sla.kappa_bps_per_ms','execution.sla.target_fill_prob',\n",
        "  'execution.router.mode','execution.router.mode_default','execution.router.spread_limit_bps',\n",
        "  'execution.fees.default_maker_bps','execution.fees.default_taker_bps',\n",
        "  'sizing.kelly.risk_aversion','sizing.kelly.clip_max','sizing.portfolio.method','sizing.portfolio.cvar_limit',\n",
        "  'sizing.limits.min_notional_usd','sizing.limits.max_notional_usd','sizing.limits.leverage_max',\n",
        "  'reward.ttl_minutes','reward.tp_levels_bps','reward.tp_sizes',\n",
        "  'tca.adverse_window_s','tca.mark_ref','tca.aggregate_interval_s',\n",
        "  'features.scaling.method',\n",
        "  'universe.ranking.wL','universe.ranking.wS','universe.ranking.wP','universe.ranking.wR','universe.ranking.add_thresh','universe.ranking.drop_thresh',\n",
        "  'order_sink.mode','order_sink.sim_local.latency_ms_range','order_sink.sim_local.slip_bps_range'\n",
        "]\n",
        "cfg_selected = pick_keys(effective_cfg if 'effective_cfg' in globals() else CFG, cfg_paths)\n",
        "\n",
        "# Reuse metrics from earlier cell or recompute if needed\n",
        "if 'metrics' not in globals() or not metrics:\n",
        "    metrics, ref_series = compute_metrics(df_logs)\n",
        "\n",
        "audit = {\n",
        "  'config': cfg_selected,\n",
        "  'metrics': metrics,\n",
        "}\n",
        "# Simple correctness checks (heuristics)\n",
        "epi = metrics.get('tca',{}).get('IS_bps')\n",
        "cvar = metrics.get('risk',{}).get('CVaR95')\n",
        "sla_max = deep_get(effective_cfg if 'effective_cfg' in globals() else CFG, 'execution.sla.max_latency_ms', 25)\n",
        "sla_p50 = metrics.get('order_level',{}).get('latency_ms',{}).get('submit_ack',{}).get('p50')\n",
        "notes = []\n",
        "if epi is not None:\n",
        "    notes.append(f'EPI_after_TCA ~ {epi:.2f} bps (higher is better)')\n",
        "if cvar is not None:\n",
        "    notes.append(f'CVaR95 ~ {cvar:.2f} bps (lower is better)')\n",
        "try:\n",
        "    if sla_p50 is not None and sla_p50 > float(sla_max):\n",
        "        notes.append(f'Warning: latency p50 {sla_p50:.1f}ms exceeds SLA {sla_max}ms')\n",
        "except Exception:\n",
        "    pass\n",
        "audit['notes'] = notes\n",
        "\n",
        "# Save\n",
        "(REPORTS/'decision_factors.json').write_text(json.dumps(audit, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "md = ['# Decision Audit', '', '## Notes'] + ['- '+n for n in notes]\n",
        "(REPORTS/'decision_audit.md').write_text('\n'.join(md), encoding='utf-8')\n",
        "print('Wrote:', REPORTS/'decision_factors.json')\n",
        "print('Wrote:', REPORTS/'decision_audit.md')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
