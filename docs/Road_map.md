wekabeka1996: @workspace –∑–±–µ—Ä–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é —Ç–∞ –ø—Ä–æ—á–∏—Ç–∞–π –ø–æ–≤–Ω—ñ—Å—Ç—å—é –≤—Å—ñ py —Ç–∞ yaml —Ñ–∞–π–ª–∏ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –≤ –ø—Ä–æ–µ–∫—Ç—ñ —ñ–≥–Ω–æ—Ä—É—é—á–∏ –ø–∞–ø–∫—É —Ç–µ—Å—Ç>–Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–æ–≥–æ —â–æ –≤–∂–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —Ç–∞ –Ω–∞–¥–∞–Ω–æ—ó —Ç–æ–±—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó —Ä–æ–∑—Ä–æ–±–∏ –ø–æ–∫—Ä–æ–∫–æ–≤—É —Ç–∞ —á—ñ—Ç–∫—É –¥–æ—Ä–æ–∂–Ω—é –∫–∞—Ä—Ç—É —Ç–∞ –ø–æ–≤–Ω—É —ñ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—Ç–∞–ª—å–Ω—É –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–∑–∞—Ü—ñ—é —ó—ó –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Ç–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó, —Ç–∞–∫–æ–∂ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ø–µ–≤–Ω–æ—ó –ª–æ–≥—ñ–∫–∏ —Ç–∞ —Ñ–æ—Ä–º—É–ª–∏ –∞ —Ç–∞–∫–æ–∂ —Ñ—É–Ω–∫—Ü—ñ—ó –æ–¥—Ä–∞–∑—É —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ —Ç–µ—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª —ñ –ø—Ä–æ–≤–æ–¥–∏—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ –∫—Ä–æ–∫—É. –¥–æ—Ä–æ–∂–Ω—è –∫–∞—Ä—Ç–∞ —Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–∑–∞—Ü—ñ—è –º–∞—é—Ç—å –±—É—Ç–∏ –Ω–∞–ø–∏—Å–∞–Ω—ñ –Ω–∞ –Ω–∞–π–≤–∏—â–æ–º—É —Ä—ñ–≤–Ω—ñ Anthropic OpenAI Google —Ç–∞ –ø–µ—Ä–µ–≤–µ—Ä—à—É–≤–∞—Ç–∏ —ó—Ö –≤—Å—ñ , —Ç–∏ –Ω–∞–π–∫—â–∞–∏–π —É —Å–≤—ñ—Ç—ñ —ñ–Ω–∂–µ–Ω–µ—Ä —Ç–∞ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫ —Å–∏—Å—Ç–µ–º –¥–ª—è –∞–≤—Ç–æ—Ç—Ä–µ–π–¥–µ–Ω–≥—É 

Workspace: –°–±–æ—Ä —Å–≤–µ–¥–µ–Ω–∏–π –æ —Ä–∞–±–æ—á–µ–π –æ–±–ª–∞—Å—Ç–∏–Ø –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞–≤ —ñ—Å–Ω—É—é—á—É –∫–æ–¥–æ–≤—É –±–∞–∑—É Aurora —Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É –∫–æ–Ω—Ü–µ–ø—Ü—ñ—é R1. –†–æ–∑—Ä–æ–±–ª—é –¥–µ—Ç–∞–ª—å–Ω—É –¥–æ—Ä–æ–∂–Ω—é –∫–∞—Ä—Ç—É —Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–∑–∞—Ü—ñ—é –¥–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Å—Ç–µ–º–∏ Aurora+ScalpBot –∑–≥—ñ–¥–Ω–æ –∑ –Ω–∞–¥–∞–Ω–æ—é –∫–æ–Ω—Ü–µ–ø—Ü—ñ—î—é.

# üöÄ AURORA+SCALPBOT ‚Äî –î–û–†–û–ñ–ù–Ø –ö–ê–†–¢–ê –†–ï–ê–õ–Ü–ó–ê–¶–Ü–á v2.1 (–ö–û–ù–°–û–õ–Ü–î–û–í–ê–ù–ê)

> **–í–ê–ñ–õ–ò–í–û:** –î–æ—Ä–æ–∂–Ω—è –∫–∞—Ä—Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–∞ –∑–≥—ñ–¥–Ω–æ –≤–µ—Ä–¥–∏–∫—Ç—É –≥–æ–ª–æ–≤–Ω–æ–≥–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç–æ—Ä–∞. –£—Å—É–Ω–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω—ñ –Ω–µ–¥–æ–ª—ñ–∫–∏: –º–∞–≥—ñ—á–Ω—ñ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏, –µ–≤—Ä–∏—Å—Ç–∏–∫–∏ TCA, –ø—Ä–æ–±–ª–µ–º–∏ SPRT/GLR —Ç–∞ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∫–æ–º–ø–æ–∑–∏—Ç–Ω–∏—Ö –≥—ñ–ø–æ—Ç–µ–∑.

## üõ°Ô∏è –ö–†–ò–¢–ò–ß–ù–Ü –ü–†–ê–í–ö–ò –Ü–ú–ü–õ–ï–ú–ï–ù–¢–û–í–ê–ù–Ü

**–£—Å—É–Ω–µ–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –±–æ—Ä–≥–∏:**
- ‚ùå –ú–∞–≥—ñ—á–Ω—ñ –ø–æ—Ä–æ–≥–∏ –≤ regime detection
- ‚ùå –ï–≤—Ä–∏—Å—Ç–∏–∫–∏ —É TCA (base_adverse=2bps)
- ‚ùå –î—É–±–ª—é–≤–∞–Ω–Ω—è –∫–æ–¥—É –≤ SPRT/GLR
- ‚ùå –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å Œ±-spending ledger
- ‚ùå Look-ahead —Ä–∏–∑–∏–∫–∏

**–î–æ–¥–∞–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**
- ‚úÖ –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ñ –≥—ñ–ø–æ—Ç–µ–∑–∏ (t-LR, —Å—É–±–µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω—ñ —Ö–≤–æ—Å—Ç–∏)
- ‚úÖ –ö–≤–∞–Ω—Ç–∏–ª—å–Ω—ñ –∞–¥–∞–ø—Ç–∏–≤–Ω—ñ –ø–æ—Ä–æ–≥–∏
- ‚úÖ Cox/Hawkes –º–æ–¥–µ–ª—ñ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è
- ‚úÖ Hayashi-Yoshida –∫—Ä–æ–∑-–∞–∫—Ç–∏–≤–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- ‚úÖ Prequential –∫–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è
- ‚úÖ –ê–Ω—Ç–∏-look-ahead —ñ–Ω–≤–∞—Ä—ñ–∞–Ω—Ç–∏

## üìä –ê–Ω–∞–ª—ñ–∑ —ñ—Å–Ω—É—é—á–æ–≥–æ —Å—Ç–∞–Ω—É (–æ–Ω–æ–≤–ª–µ–Ω–∏–π)

### –í–∂–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ:
- **Aurora API** (FastAPI): pre-trade gates, post-trade logging
- **WiseScalp runner**: –±–∞–∑–æ–≤—ñ –º—ñ–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ —Å–∏–≥–Ω–∞–ª–∏ (OBI/TFI)
- **–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω–∞ —Å–∏—Å—Ç–µ–º–∞**: YAML + ENV overrides
- **–ë–∞–∑–æ–≤–∞ –æ–±—Å–µ—Ä–≤–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å**: Prometheus metrics, JSONL logging
- **Risk gates**: spread, latency, volatility guards
- **Calibrator**: Isotonic/Platt calibration –∑ fallback
- **RewardManager**: TP/trail/breakeven –ª–æ–≥—ñ–∫–∞

### –ö–†–ò–¢–ò–ß–ù–û –ü–û–¢–†–Ü–ë–ù–û (–∑–≥—ñ–¥–Ω–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç–æ—Ä–∞):
1. **SPRT/GLR Governance** - –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ñ H‚ÇÄ/H‚ÇÅ + Œ±-ledger
2. **Adaptive Regime Detection** - –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ñ –ø–æ—Ä–æ–≥–∏ –∑–∞–º—ñ—Å—Ç—å –º–∞–≥—ñ—ó
3. **Cox/Hawkes Fill Models** - –µ–º–ø—ñ—Ä–∏—á–Ω–∞ Œ∫ –∑–∞–º—ñ–Ω–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫
4. **Hayashi-Yoshida Cross-Asset** - —Ñ–æ—Ä–º–∞–ª—å–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ SOL‚Üíalts
5. **Prequential Calibration** - ECE/Brier/LogLoss online + ICP coverage
6. **Anti-Look-Ahead** - —á–∞—Å–æ–≤—ñ —ñ–Ω–≤–∞—Ä—ñ–∞–Ω—Ç–∏ + —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ—Ä—è–¥–∫—É –ø–æ–¥—ñ–π
7. **EVT-CVaR** - –±—É—Ç—Å—Ç—Ä–µ–ø CI < 20% —à–∏—Ä–∏–Ω–∏
8. **XAI Enhanced** - WHY-–∫–æ–¥–∏ + schema versioning

---

## üìã –§–ê–ó–ò –†–ï–ê–õ–Ü–ó–ê–¶–Ü–á

### **–§–ê–ó–ê 0: –ü–Ü–î–ì–û–¢–û–í–ö–ê –Ü–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–ò** [2 –¥–Ω—ñ]

#### 0.1 –†–µ–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –ø—Ä–æ–µ–∫—Ç—É
```
aurora/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ features/          # –ú—ñ–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ —Ñ—ñ—á—ñ
‚îÇ   ‚îú‚îÄ‚îÄ calibration/       # –ö–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
‚îÇ   ‚îú‚îÄ‚îÄ regime/            # –î–µ—Ç–µ–∫—Ç–æ—Ä–∏ —Ä–µ–∂–∏–º—ñ–≤
‚îÇ   ‚îú‚îÄ‚îÄ portfolio/         # –ü–æ—Ä—Ç—Ñ–µ–ª—å–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
‚îÇ   ‚îú‚îÄ‚îÄ risk/              # EVT-CVaR
‚îÇ   ‚îú‚îÄ‚îÄ execution/         # TCA & Fill Simulator
‚îÇ   ‚îî‚îÄ‚îÄ governance/        # SPRT/GLR
‚îú‚îÄ‚îÄ scalp_bot/
‚îÇ   ‚îú‚îÄ‚îÄ strategies/        # –¢–æ—Ä–≥–æ–≤—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
‚îÇ   ‚îú‚îÄ‚îÄ execution/         # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –æ—Ä–¥–µ—Ä—ñ–≤
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/        # XAI & observability
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ unit/              # –¢–µ—Å—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –º–æ–¥—É–ª—è
```

#### 0.2 –ë–∞–∑–æ–≤—ñ —Ç–∏–ø–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏

````python
# core/types.py
from dataclasses import dataclass
from typing import Dict, List, Optional, Literal
import numpy as np

@dataclass
class MarketSnapshot:
    """L2/L3 snapshot –≤ –º–æ–º–µ–Ω—Ç t"""
    timestamp: float
    bid_price: float
    ask_price: float
    bid_volumes: List[float]  # L5 volumes
    ask_volumes: List[float]
    trades: List[Dict]  # Recent trades
    
@dataclass
class Signal:
    """–¢–æ—Ä–≥–æ–≤–∏–π —Å–∏–≥–Ω–∞–ª"""
    timestamp: float
    symbol: str
    score: float
    raw_probability: float
    calibrated_probability: float
    confidence_interval: tuple
    features: Dict[str, float]
    
@dataclass
class EdgeBreakdown:
    """–†–æ–∑–∫–ª–∞–¥ –æ—á—ñ–∫—É–≤–∞–Ω–æ—ó –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ"""
    raw_edge: float
    fees: float
    slippage: float
    adverse_selection: float
    latency_cost: float
    rebates: float
    net_edge: float
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 0:**
````python
# tests/unit/test_types.py
def test_market_snapshot_creation():
    snapshot = MarketSnapshot(
        timestamp=1234567890.123,
        bid_price=100.0,
        ask_price=100.1,
        bid_volumes=[100, 200, 300, 400, 500],
        ask_volumes=[150, 250, 350, 450, 550],
        trades=[]
    )
    assert snapshot.bid_price < snapshot.ask_price
    assert len(snapshot.bid_volumes) == 5
````

---

### **–§–ê–ó–ê 1: –ú–Ü–ö–†–û–°–¢–†–£–ö–¢–£–†–ù–Ü –§–Ü–ß–Ü –¢–ê –ö–†–û–°-–ê–ö–¢–ò–í–ù–Ü –ó–ê–õ–ï–ñ–ù–û–°–¢–Ü** [5 –¥–Ω—ñ–≤]

#### 1.1 –ë–∞–∑–æ–≤—ñ –º—ñ–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ —Ñ—ñ—á—ñ

````python
# core/features/microstructure.py
import numpy as np
from scipy.stats import huber
from typing import List, Dict, Optional

class MicrostructureFeatures:
    """–û–±—á–∏—Å–ª–µ–Ω–Ω—è –º—ñ–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏—Ö —Ñ—ñ—á –∑–≥—ñ–¥–Ω–æ ¬ß5.1"""
    
    def __init__(self, robust_scale: bool = True):
        self.robust_scale = robust_scale
        self.scaler = huber.Huber() if robust_scale else None
        
    def calculate_obi(self, bid_volumes: List[float], 
                     ask_volumes: List[float], 
                     levels: int = 5) -> float:
        """Order Book Imbalance –Ω–∞ L_k —Ä—ñ–≤–Ω—è—Ö"""
        bid_sum = sum(bid_volumes[:levels])
        ask_sum = sum(ask_volumes[:levels])
        
        if bid_sum + ask_sum == 0:
            return 0.0
            
        return (bid_sum - ask_sum) / (bid_sum + ask_sum)
    
    def calculate_microprice(self, bid_price: float, ask_price: float,
                            bid_volume: float, ask_volume: float) -> float:
        """–ú—ñ–∫—Ä–æ-–ø—Ä–∞–π—Å —á–µ—Ä–µ–∑ best volumes"""
        total_volume = bid_volume + ask_volume
        if total_volume == 0:
            return (bid_price + ask_price) / 2
            
        return (ask_volume * bid_price + bid_volume * ask_price) / total_volume
    
    def calculate_tfi(self, trades: List[Dict], window: float) -> float:
        """Trade Flow Imbalance —É –≤—ñ–∫–Ω—ñ W"""
        if not trades:
            return 0.0
            
        buy_volume = sum(t['size'] for t in trades if t['side'] == 'BUY')
        sell_volume = sum(t['size'] for t in trades if t['side'] == 'SELL')
        total_volume = buy_volume + sell_volume
        
        if total_volume == 0:
            return 0.0
            
        return (buy_volume - sell_volume) / total_volume
    
    def calculate_absorption(self, trades: List[Dict], 
                           price_changes: List[float]) -> float:
        """Absorption score - –ø–æ–≥–ª–∏–Ω–∞–Ω–Ω—è —É–¥–∞—Ä–Ω–∏—Ö —É–≥–æ–¥"""
        if not trades or not price_changes:
            return 0.0
            
        absorbed_volume = sum(t['size'] for t in trades 
                             if abs(t['price_impact']) < 0.01)
        total_volume = sum(t['size'] for t in trades)
        
        if total_volume == 0:
            return 0.0
            
        net_price_move = abs(sum(price_changes))
        if net_price_move < 1e-8:
            return 1.0  # Perfect absorption
            
        return absorbed_volume / (total_volume * net_price_move)
````

#### 1.2 Cross-asset dependencies (SOL‚Üíalts)

````python
# core/features/cross_asset.py
import numpy as np
from scipy import stats
from typing import Dict, Tuple, Optional

class CrossAssetFeatures:
    """–ö—Ä–æ—Å-–∞–∫—Ç–∏–≤–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∑–≥—ñ–¥–Ω–æ ¬ß7"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.beta_cache: Dict[str, float] = {}
        self.lag_cache: Dict[str, int] = {}
        
    def estimate_beta(self, alt_returns: np.ndarray, 
                     sol_returns: np.ndarray,
                     robust: bool = True) -> float:
        """–õ–æ–∫–∞–ª—å–Ω–∞ –±–µ—Ç–∞ –∞–ª—å—Ç–∞ –Ω–∞ SOL"""
        if len(alt_returns) < 10 or len(sol_returns) < 10:
            return 0.0
            
        if robust:
            # Robust regression using Huber
            from sklearn.linear_model import HuberRegressor
            model = HuberRegressor()
            model.fit(sol_returns.reshape(-1, 1), alt_returns)
            return model.coef_[0]
        else:
            # OLS regression
            slope, _, _, _, _ = stats.linregress(sol_returns, alt_returns)
            return slope
    
    def find_optimal_lag(self, alt_data: np.ndarray, 
                        sol_data: np.ndarray,
                        max_lag: int = 10) -> Tuple[int, float]:
        """–ó–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –ª–∞–≥ œÑ* –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—é –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ—Å—Ç—é"""
        best_lag = 0
        best_score = 0.0
        
        for lag in range(1, min(max_lag + 1, len(sol_data) // 2)):
            # Cross-correlation
            if lag >= len(alt_data):
                continue
                
            corr = np.corrcoef(sol_data[:-lag], alt_data[lag:])[0, 1]
            
            # Transfer entropy (simplified)
            te_score = self._transfer_entropy(sol_data[:-lag], alt_data[lag:])
            
            # Combined score
            score = abs(corr) * 0.7 + te_score * 0.3
            
            if score > best_score:
                best_score = score
                best_lag = lag
                
        return best_lag, best_score
    
    def _transfer_entropy(self, source: np.ndarray, 
                         target: np.ndarray, 
                         bins: int = 10) -> float:
        """Simplified transfer entropy calculation"""
        if len(source) < bins or len(target) < bins:
            return 0.0
            
        # Discretize
        source_discrete = np.digitize(source, np.percentile(source, np.linspace(0, 100, bins)))
        target_discrete = np.digitize(target, np.percentile(target, np.linspace(0, 100, bins)))
        
        # Calculate entropies
        from scipy.stats import entropy
        
        # H(target_future | target_past)
        h_target = entropy(np.histogram(target_discrete[1:], bins=bins)[0])
        
        # H(target_future | target_past, source_past)  
        joint_hist = np.histogram2d(source_discrete[:-1], target_discrete[1:], bins=bins)[0]
        h_joint = entropy(joint_hist.flatten())
        
        return max(0, h_target - h_joint)
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 1:**
````python
# tests/unit/test_microstructure.py
def test_obi_calculation():
    features = MicrostructureFeatures()
    
    # Balanced book
    obi = features.calculate_obi([100, 200], [100, 200], levels=2)
    assert abs(obi) < 0.01
    
    # Bid heavy
    obi = features.calculate_obi([300, 200], [100, 200], levels=2)
    assert obi > 0
    
    # Ask heavy
    obi = features.calculate_obi([100, 200], [300, 200], levels=2)
    assert obi < 0

def test_cross_asset_beta():
    cross = CrossAssetFeatures()
    
    # Perfect correlation
    sol_returns = np.array([0.01, 0.02, -0.01, 0.03, -0.02])
    alt_returns = 2 * sol_returns  # Beta = 2
    
    beta = cross.estimate_beta(alt_returns, sol_returns, robust=False)
    assert abs(beta - 2.0) < 0.1
````

---

### **–§–ê–ó–ê 2: –ö–ê–õ–Ü–ë–†–£–í–ê–ù–ù–Ø –ô–ú–û–í–Ü–†–ù–û–°–¢–ï–ô –¢–ê ICP** [4 –¥–Ω—ñ]

#### 2.1 –ö–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —è–∫–æ—Å—Ç—ñ

````python
# core/calibration/probability.py
import numpy as np
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression
from typing import Tuple, Dict, Optional
from scipy.special import expit

class ProbabilityCalibrator:
    """–ö–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –∑–≥—ñ–¥–Ω–æ ¬ß6.2 —Ç–∞ ¬ß13.1"""
    
    def __init__(self, method: str = 'isotonic'):
        self.method = method
        self.calibrator = None
        self.metrics_history = []
        
    def fit(self, scores: np.ndarray, targets: np.ndarray) -> None:
        """–ù–∞–≤—á–∏—Ç–∏ –∫–∞–ª—ñ–±—Ä–∞—Ç–æ—Ä"""
        if self.method == 'isotonic':
            self.calibrator = IsotonicRegression(out_of_bounds='clip')
            self.calibrator.fit(scores, targets)
        elif self.method == 'platt':
            # Platt scaling
            self.calibrator = LogisticRegression()
            self.calibrator.fit(scores.reshape(-1, 1), targets)
        else:
            raise ValueError(f"Unknown method: {self.method}")
    
    def calibrate(self, score: float) -> float:
        """–ö–∞–ª—ñ–±—Ä—É–≤–∞—Ç–∏ –æ–¥–∏–Ω–∏—á–Ω–∏–π —Å–∫–æ—Ä"""
        if self.calibrator is None:
            return expit(score)  # Default sigmoid
            
        if self.method == 'isotonic':
            return float(self.calibrator.predict([score])[0])
        else:
            return float(self.calibrator.predict_proba([[score]])[0, 1])
    
    def calculate_ece(self, probabilities: np.ndarray, 
                     targets: np.ndarray, 
                     n_bins: int = 10) -> float:
        """Expected Calibration Error"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        ece = 0.0
        
        for i in range(n_bins):
            mask = (probabilities >= bin_boundaries[i]) & \
                   (probabilities < bin_boundaries[i + 1])
            
            if not np.any(mask):
                continue
                
            bin_prob = probabilities[mask].mean()
            bin_accuracy = targets[mask].mean()
            bin_weight = mask.sum() / len(probabilities)
            
            ece += bin_weight * abs(bin_prob - bin_accuracy)
            
        return ece
    
    def calculate_brier_score(self, probabilities: np.ndarray,
                             targets: np.ndarray) -> float:
        """Brier score"""
        return np.mean((probabilities - targets) ** 2)
    
    def calculate_log_loss(self, probabilities: np.ndarray,
                          targets: np.ndarray) -> float:
        """Log loss with stability"""
        eps = 1e-15
        probs_clipped = np.clip(probabilities, eps, 1 - eps)
        
        return -np.mean(
            targets * np.log(probs_clipped) + 
            (1 - targets) * np.log(1 - probs_clipped)
        )
````

#### 2.2 ICP (Inductive Conformal Prediction)

````python
# core/calibration/conformal.py
import numpy as np
from typing import Tuple, List, Optional

class InductiveConformalPredictor:
    """ICP –¥–ª—è –¥–æ–≤—ñ—Ä—á–∏—Ö —ñ–Ω—Ç–µ—Ä–≤–∞–ª—ñ–≤ –∑–≥—ñ–¥–Ω–æ ¬ß6.3"""
    
    def __init__(self, significance_level: float = 0.05):
        self.significance = significance_level
        self.calibration_scores = []
        
    def calibrate(self, scores: np.ndarray, targets: np.ndarray) -> None:
        """–ö–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ–π –º–Ω–æ–∂–∏–Ω—ñ"""
        # Calculate non-conformity scores
        self.calibration_scores = []
        
        for score, target in zip(scores, targets):
            if target == 1:
                nc_score = 1 - score  # Non-conformity for positive
            else:
                nc_score = score  # Non-conformity for negative
            self.calibration_scores.append(nc_score)
            
        self.calibration_scores.sort()
    
    def predict_with_confidence(self, score: float) -> Tuple[float, Tuple[float, float]]:
        """–ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –∑ –¥–æ–≤—ñ—Ä—á–∏–º —ñ–Ω—Ç–µ—Ä–≤–∞–ª–æ–º"""
        if not self.calibration_scores:
            return score, (score - 0.1, score + 0.1)  # Default interval
            
        # Find quantile threshold
        k = int(np.ceil((1 - self.significance) * (len(self.calibration_scores) + 1))) - 1
        k = min(k, len(self.calibration_scores) - 1)
        threshold = self.calibration_scores[k]
        
        # Calculate p-values for each class
        p_value_pos = self._calculate_p_value(1 - score)
        p_value_neg = self._calculate_p_value(score)
        
        # Prediction set
        prediction_set = []
        if p_value_pos > self.significance:
            prediction_set.append(1)
        if p_value_neg > self.significance:
            prediction_set.append(0)
            
        # Confidence interval
        lower = score - threshold
        upper = score + threshold
        
        return score, (max(0, lower), min(1, upper))
    
    def _calculate_p_value(self, nc_score: float) -> float:
        """Calculate p-value for non-conformity score"""
        if not self.calibration_scores:
            return 0.5
            
        count = sum(1 for s in self.calibration_scores if s >= nc_score)
        return (count + 1) / (len(self.calibration_scores) + 1)
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 2:**
````python
# tests/unit/test_calibration.py
def test_ece_calculation():
    calibrator = ProbabilityCalibrator()
    
    # Perfect calibration
    probs = np.array([0.1, 0.2, 0.3, 0.7, 0.8, 0.9])
    targets = np.array([0, 0, 0, 1, 1, 1])
    
    ece = calibrator.calculate_ece(probs, targets, n_bins=3)
    assert ece < 0.1  # Should be well calibrated

def test_icp_confidence():
    icp = InductiveConformalPredictor(significance_level=0.05)
    
    # Calibration data
    scores = np.random.rand(100)
    targets = (scores > 0.5).astype(int)
    
    icp.calibrate(scores, targets)
    
    # Test prediction
    test_score = 0.7
    pred, (lower, upper) = icp.predict_with_confidence(test_score)
    
    assert lower <= pred <= upper
    assert 0 <= lower <= 1
    assert 0 <= upper <= 1
````

---

### **–§–ê–ó–ê 3: –î–ï–¢–ï–ö–¢–û–† –†–ï–ñ–ò–ú–Ü–í** [3 –¥–Ω—ñ]

#### 3.1 Page-Hinkley –¥–µ—Ç–µ–∫—Ç–æ—Ä

````python
# core/regime/detectors.py
import numpy as np
from typing import Dict, List, Optional, Literal
from dataclasses import dataclass

@dataclass
class RegimeState:
    """–°—Ç–∞–Ω —Ä–µ–∂–∏–º—É —Ä–∏–Ω–∫—É"""
    current_regime: Literal['trend', 'grind', 'chaos']
    confidence: float
    transition_probability: float
    lambda_reg: float  # –ú–Ω–æ–∂–Ω–∏–∫ –¥–ª—è Kelly

class PageHinkleyDetector:
    """Page-Hinkley change detector –∑–≥—ñ–¥–Ω–æ ¬ß8.1"""
    
    def __init__(self, delta: float = 0.01, threshold: float = 5.0):
        self.delta = delta
        self.threshold = threshold
        self.reset()
        
    def reset(self) -> None:
        """Reset detector state"""
        self.ph_sum = 0.0
        self.min_ph = 0.0
        self.samples = []
        self.mean_estimate = None
        
    def update(self, value: float) -> bool:
        """Update detector and return True if change detected"""
        self.samples.append(value)
        
        # Initialize mean estimate
        if self.mean_estimate is None:
            if len(self.samples) >= 10:
                self.mean_estimate = np.mean(self.samples[:10])
            else:
                return False
                
        # Page-Hinkley statistic
        self.ph_sum += value - self.mean_estimate - self.delta
        self.min_ph = min(self.min_ph, self.ph_sum)
        
        # Check for change
        if self.ph_sum - self.min_ph > self.threshold:
            self.reset()
            return True
            
        return False

class GLRDetector:
    """Generalized Likelihood Ratio detector"""
    
    def __init__(self, window_size: int = 100, threshold: float = 10.0):
        self.window_size = window_size
        self.threshold = threshold
        self.data_buffer = []
        
    def update(self, value: float) -> bool:
        """Update GLR and detect changes"""
        self.data_buffer.append(value)
        
        if len(self.data_buffer) > self.window_size:
            self.data_buffer.pop(0)
            
        if len(self.data_buffer) < 20:
            return False
            
        # Calculate GLR statistic
        best_glr = 0.0
        n = len(self.data_buffer)
        
        for k in range(10, n - 10):
            # Split data
            before = self.data_buffer[:k]
            after = self.data_buffer[k:]
            
            # Calculate means and variances
            mu1, var1 = np.mean(before), np.var(before)
            mu2, var2 = np.mean(after), np.var(after)
            
            if var1 <= 0 or var2 <= 0:
                continue
                
            # Log-likelihood ratio
            ll_h0 = self._log_likelihood(self.data_buffer, 
                                         np.mean(self.data_buffer), 
                                         np.var(self.data_buffer))
            ll_h1 = (self._log_likelihood(before, mu1, var1) + 
                    self._log_likelihood(after, mu2, var2))
            
            glr = 2 * (ll_h1 - ll_h0)
            best_glr = max(best_glr, glr)
            
        return best_glr > self.threshold
    
    def _log_likelihood(self, data: List[float], mu: float, var: float) -> float:
        """Calculate Gaussian log-likelihood"""
        n = len(data)
        if var <= 0:
            return -np.inf
        return -0.5 * n * (np.log(2 * np.pi * var) + 
                          np.sum((np.array(data) - mu) ** 2) / (n * var))
````

#### 3.2 –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º—ñ–≤

````python
# core/regime/regime_manager.py
import numpy as np
from typing import Dict, List, Optional
from enum import Enum

class MarketRegime(Enum):
    TREND = "trend"
    GRIND = "grind"  
    CHAOS = "chaos"
    TRANSITION = "transition"

class RegimeManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ–∂–∏–º—ñ–≤ —Ä–∏–Ω–∫—É"""
    
    def __init__(self):
        self.ph_detector = PageHinkleyDetector()
        self.glr_detector = GLRDetector()
        self.current_regime = MarketRegime.GRIND
        self.regime_features = {}
        self.regime_history = []
        
    def update(self, market_data: Dict) -> RegimeState:
        """–û–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞–Ω —Ä–µ–∂–∏–º—É"""
        # Extract features
        returns = market_data.get('returns', [])
        volatility = np.std(returns) if returns else 0
        volume_ratio = market_data.get('volume_ratio', 1.0)
        spread = market_data.get('spread_bps', 0)
        
        # Detect changes
        ph_change = self.ph_detector.update(volatility)
        glr_change = self.glr_detector.update(returns[-1] if returns else 0)
        
        # Classify regime
        if ph_change or glr_change:
            self.current_regime = self._classify_regime(
                volatility, volume_ratio, spread
            )
            
        # Calculate lambda_reg multiplier
        lambda_reg = self._calculate_lambda_reg()
        
        return RegimeState(
            current_regime=self.current_regime.value,
            confidence=self._calculate_confidence(),
            transition_probability=0.1 if ph_change or glr_change else 0.01,
            lambda_reg=lambda_reg
        )
    
    def _classify_regime(self, volatility: float, 
                        volume_ratio: float, 
                        spread: float) -> MarketRegime:
        """–ö–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ —Ä–µ–∂–∏–º –Ω–∞ –æ—Å–Ω–æ–≤—ñ –º–µ—Ç—Ä–∏–∫"""
        # High volatility + directional = TREND
        if volatility > 0.02 and volume_ratio > 1.5:
            return MarketRegime.TREND
            
        # Low volatility + tight spread = GRIND
        elif volatility < 0.01 and spread < 10:
            return MarketRegime.GRIND
            
        # High volatility + wide spread = CHAOS
        elif volatility > 0.03 and spread > 20:
            return MarketRegime.CHAOS
            
        else:
            return MarketRegime.TRANSITION
    
    def _calculate_lambda_reg(self) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –º–Ω–æ–∂–Ω–∏–∫ –¥–ª—è Kelly –∑–≥—ñ–¥–Ω–æ —Ä–µ–∂–∏–º—É"""
        regime_multipliers = {
            MarketRegime.TREND: 1.0,
            MarketRegime.GRIND: 0.7,
            MarketRegime.CHAOS: 0.3,
            MarketRegime.TRANSITION: 0.5
        }
        return regime_multipliers.get(self.current_regime, 0.5)
    
    def _calculate_confidence(self) -> float:
        """–†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å —É –ø–æ—Ç–æ—á–Ω–æ–º—É —Ä–µ–∂–∏–º—ñ"""
        if len(self.regime_history) < 10:
            return 0.5
            
        # Stability of recent regimes
        recent = self.regime_history[-10:]
        stability = sum(1 for r in recent if r == self.current_regime) / 10
        
        return stability
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 3:**
````python
# tests/unit/test_regime.py
def test_page_hinkley_detector():
    detector = PageHinkleyDetector(delta=0.01, threshold=5.0)
    
    # Stable data
    for _ in range(50):
        assert not detector.update(np.random.normal(0, 0.1))
    
    # Sudden change
    changes_detected = 0
    for _ in range(50):
        if detector.update(np.random.normal(2, 0.1)):  # Mean shift
            changes_detected += 1
            
    assert changes_detected > 0

def test_regime_classification():
    manager = RegimeManager()
    
    # Test TREND regime
    market_data = {
        'returns': [0.01, 0.02, 0.015, 0.025],
        'volume_ratio': 2.0,
        'spread_bps': 15
    }
    
    state = manager.update(market_data)
    assert state.lambda_reg > 0  # Should have positive multiplier
````

---

### **–§–ê–ó–ê 4: –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô KELLY –ó EVT-CVaR** [5 –¥–Ω—ñ–≤]

#### 4.1 EVT-CVaR –æ–±—á–∏—Å–ª–µ–Ω–Ω—è

````python
# core/risk/evt_cvar.py
import numpy as np
from scipy import stats
from typing import Tuple, Optional, List

class EVTCVaR:
    """Extreme Value Theory CVaR –∑–≥—ñ–¥–Ω–æ ¬ß12"""
    
    def __init__(self, confidence_level: float = 0.95):
        self.confidence_level = confidence_level
        self.threshold = None
        self.gpd_params = None
        
    def fit_gpd(self, losses: np.ndarray, 
                threshold_percentile: float = 90) -> None:
        """Fit Generalized Pareto Distribution to tail"""
        # Set threshold
        self.threshold = np.percentile(losses, threshold_percentile)
        
        # Extract exceedances
        exceedances = losses[losses > self.threshold] - self.threshold
        
        if len(exceedances) < 10:
            raise ValueError("Not enough tail observations")
            
        # Fit GPD using MLE
        self.gpd_params = self._fit_gpd_mle(exceedances)
    
    def _fit_gpd_mle(self, exceedances: np.ndarray) -> Tuple[float, float]:
        """Maximum likelihood estimation for GPD"""
        from scipy.optimize import minimize
        
        def neg_log_likelihood(params):
            xi, beta = params
            if beta <= 0:
                return np.inf
                
            if xi == 0:
                # Exponential case
                return len(exceedances) * np.log(beta) + np.sum(exceedances) / beta
            else:
                # General case
                if np.any(1 + xi * exceedances / beta <= 0):
                    return np.inf
                return (len(exceedances) * np.log(beta) + 
                       (1 + 1/xi) * np.sum(np.log(1 + xi * exceedances / beta)))
        
        # Initial guess
        x0 = [0.1, np.std(exceedances)]
        
        # Optimize
        result = minimize(neg_log_likelihood, x0, 
                         bounds=[(-0.5, 0.5), (1e-6, None)])
        
        return result.x[0], result.x[1]  # xi, beta
    
    def calculate_cvar(self, losses: np.ndarray) -> Tuple[float, Tuple[float, float]]:
        """Calculate CVaR with confidence interval"""
        if self.gpd_params is None:
            # Fallback to empirical
            var_idx = int((1 - self.confidence_level) * len(losses))
            var = np.sort(losses)[var_idx]
            cvar = np.mean(losses[losses >= var])
            
            # Bootstrap CI
            ci_lower, ci_upper = self._bootstrap_ci(losses)
            
            return cvar, (ci_lower, ci_upper)
        
        # Use GPD
        xi, beta = self.gpd_params
        alpha = 1 - self.confidence_level
        
        # VaR from GPD
        n = len(losses)
        n_tail = np.sum(losses > self.threshold)
        
        if xi == 0:
            var = self.threshold - beta * np.log(n_tail / (n * alpha))
        else:
            var = self.threshold + beta/xi * ((n/n_tail * alpha)**(-xi) - 1)
            
        # CVaR from GPD
        if xi < 1:
            cvar = var / (1 - xi) + (beta - xi * self.threshold) / (1 - xi)
        else:
            cvar = np.inf  # Heavy tail case
            
        # Confidence interval via parametric bootstrap
        ci_lower, ci_upper = self._gpd_bootstrap_ci(losses)
        
        return cvar, (ci_lower, ci_upper)
    
    def _bootstrap_ci(self, losses: np.ndarray, 
                     n_bootstrap: int = 1000) -> Tuple[float, float]:
        """Bootstrap confidence interval for CVaR"""
        cvars = []
        
        for _ in range(n_bootstrap):
            sample = np.random.choice(losses, size=len(losses), replace=True)
            var_idx = int((1 - self.confidence_level) * len(sample))
            var = np.sort(sample)[var_idx]
            cvar = np.mean(sample[sample >= var])
            cvars.append(cvar)
            
        return np.percentile(cvars, [2.5, 97.5])
    
    def _gpd_bootstrap_ci(self, losses: np.ndarray,
                         n_bootstrap: int = 1000) -> Tuple[float, float]:
        """Parametric bootstrap CI for GPD-based CVaR"""
        if self.gpd_params is None:
            return self._bootstrap_ci(losses)
            
        xi, beta = self.gpd_params
        cvars = []
        
        for _ in range(n_bootstrap):
            # Generate GPD samples
            u = np.random.uniform(0, 1, len(losses))
            
            if xi == 0:
                samples = -beta * np.log(u)
            else:
                samples = beta/xi * (u**(-xi) - 1)
                
            samples += self.threshold
            
            # Calculate CVaR
            var_idx = int((1 - self.confidence_level) * len(samples))
            var = np.sort(samples)[var_idx]
            cvar = np.mean(samples[samples >= var])
            cvars.append(cvar)
            
        return np.percentile(cvars, [2.5, 97.5])
````

#### 4.2 –î–∏–Ω–∞–º—ñ—á–Ω–∏–π Kelly –∑ –º–Ω–æ–∂–Ω–∏–∫–∞–º–∏

````python
# core/portfolio/kelly.py
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

@dataclass
class KellyState:
    """–°—Ç–∞–Ω Kelly –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó"""
    raw_kelly: Dict[str, float]
    portfolio_kelly: Dict[str, float]
    multiplier: float
    final_kelly: Dict[str, float]
    risk_metrics: Dict

class DynamicKellyOptimizer:
    """–î–∏–Ω–∞–º—ñ—á–Ω–∏–π Kelly optimizer –∑–≥—ñ–¥–Ω–æ ¬ß11"""
    
    def __init__(self, max_kelly: float = 0.25):
        self.max_kelly = max_kelly
        self.evt_cvar = EVTCVaR(confidence_level=0.95)
        self.covariance_estimator = LedoitWolfEstimator()
        
    def calculate_raw_kelly(self, p: float, b: float) -> float:
        """–°–∏—Ä–∏–π Kelly –¥–ª—è –æ–¥–Ω–æ–≥–æ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ ¬ß11.1"""
        if b <= 0:
            return 0.0
            
        kelly = (b * p - (1 - p)) / b
        return np.clip(kelly, 0, self.max_kelly)
    
    def calculate_portfolio_kelly(self, 
                                 expected_returns: np.ndarray,
                                 covariance: np.ndarray,
                                 cvar_limit: float) -> np.ndarray:
        """–ü–æ—Ä—Ç—Ñ–µ–ª—å–Ω–∏–π Kelly –ø—ñ–¥ CVaR –æ–±–º–µ–∂–µ–Ω–Ω—è–º ¬ß11.2"""
        n_assets = len(expected_returns)
        
        if n_assets == 0:
            return np.array([])
            
        # Shrink covariance matrix (Ledoit-Wolf)
        cov_shrunk = self.covariance_estimator.shrink(covariance)
        
        # Solve for Kelly weights
        try:
            inv_cov = np.linalg.inv(cov_shrunk)
            raw_weights = inv_cov @ expected_returns
            
            # Scale to satisfy CVaR constraint
            rho = self._find_cvar_scaling(raw_weights, cov_shrunk, cvar_limit)
            
            kelly_weights = rho * raw_weights
            
        except np.linalg.LinAlgError:
            # Fallback to diagonal
            kelly_weights = expected_returns / np.diag(covariance)
            
        # Clip to max
        return np.clip(kelly_weights, 0, self.max_kelly)
    
    def _find_cvar_scaling(self, weights: np.ndarray,
                          covariance: np.ndarray,
                          cvar_limit: float) -> float:
        """Find scaling factor to satisfy CVaR constraint"""
        # Binary search for rho
        left, right = 0.0, 1.0
        
        for _ in range(20):
            mid = (left + right) / 2
            scaled_weights = mid * weights
            
            # Simulate portfolio returns
            portfolio_var = scaled_weights @ covariance @ scaled_weights
            portfolio_std = np.sqrt(portfolio_var)
            
            # Approximate CVaR (can be improved with simulation)
            approx_cvar = 2.5 * portfolio_std  # Rough approximation
            
            if approx_cvar <= cvar_limit:
                left = mid
            else:
                right = mid
                
        return left
    
    def calculate_aurora_multiplier(self, 
                                   lambda_cal: float,
                                   lambda_reg: float,
                                   lambda_liq: float,
                                   lambda_dd: float,
                                   lambda_lat: float) -> float:
        """Aurora multiplier M –∑–≥—ñ–¥–Ω–æ ¬ß11.3"""
        return lambda_cal * lambda_reg * lambda_liq * lambda_dd * lambda_lat
    
    def optimize(self, signals: List[Dict], 
                portfolio_state: Dict,
                market_conditions: Dict) -> KellyState:
        """–ü–æ–≤–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è Kelly"""
        # Calculate raw Kelly for each signal
        raw_kelly = {}
        for signal in signals:
            symbol = signal['symbol']
            p = signal['calibrated_probability']
            b = signal['payoff_ratio']
            raw_kelly[symbol] = self.calculate_raw_kelly(p, b)
            
        # Portfolio optimization
        if len(signals) > 1:
            expected_returns = np.array([s['expected_return'] for s in signals])
            
            # Get covariance from history
            covariance = self._estimate_covariance(signals, portfolio_state)
            
            # CVaR limit from risk management
            cvar_limit = market_conditions.get('cvar_limit', 0.02)
            
            portfolio_weights = self.calculate_portfolio_kelly(
                expected_returns, covariance, cvar_limit
            )
            
            portfolio_kelly = {
                signals[i]['symbol']: portfolio_weights[i] 
                for i in range(len(signals))
            }
        else:
            portfolio_kelly = raw_kelly
            
        # Calculate multipliers
        lambda_cal = self._calculate_lambda_cal(market_conditions)
        lambda_reg = market_conditions.get('lambda_reg', 1.0)
        lambda_liq = self._calculate_lambda_liq(market_conditions)
        lambda_dd = self._calculate_lambda_dd(portfolio_state)
        lambda_lat = self._calculate_lambda_lat(market_conditions)
        
        multiplier = self.calculate_aurora_multiplier(
            lambda_cal, lambda_reg, lambda_liq, lambda_dd, lambda_lat
        )
        
        # Final Kelly with multiplier
        final_kelly = {
            symbol: np.clip(multiplier * weight, 0, self.max_kelly)
            for symbol, weight in portfolio_kelly.items()
        }
        
        # Risk metrics
        risk_metrics = {
            'portfolio_cvar': self._calculate_portfolio_cvar(final_kelly, covariance),
            'max_position': max(final_kelly.values()) if final_kelly else 0,
            'total_exposure': sum(final_kelly.values()),
            'multiplier_breakdown': {
                'cal': lambda_cal,
                'reg': lambda_reg,
                'liq': lambda_liq,
                'dd': lambda_dd,
                'lat': lambda_lat
            }
        }
        
        return KellyState(
            raw_kelly=raw_kelly,
            portfolio_kelly=portfolio_kelly,
            multiplier=multiplier,
            final_kelly=final_kelly,
            risk_metrics=risk_metrics
        )
    
    def _calculate_lambda_cal(self, conditions: Dict) -> float:
        """Œª_cal based on calibration quality"""
        ece = conditions.get('ece', 0.1)
        log_loss = conditions.get('log_loss', 0.5)
        
        # As per ¬ßR1.2.G
        eta_cal = 10.0  # Sensitivity parameter
        zeta_cal = 5.0
        
        return np.exp(-eta_cal * ece) * np.exp(-zeta_cal * log_loss)
    
    def _calculate_lambda_liq(self, conditions: Dict) -> float:
        """Œª_liq based on liquidity"""
        depth_lk = conditions.get('depth_l5', 10000)
        target_depth = conditions.get('target_depth', 50000)
        
        return min(1.0, depth_lk / target_depth)
    
    def _calculate_lambda_dd(self, portfolio_state: Dict) -> float:
        """Œª_dd based on drawdown"""
        current_dd = portfolio_state.get('drawdown_pct', 0)
        theta = 10.0  # Drawdown sensitivity
        gamma = 2.0  # Power factor
        
        return (1 + current_dd / theta) ** (-gamma)
    
    def _calculate_lambda_lat(self, conditions: Dict) -> float:
        """Œª_lat based on latency"""
        latency = conditions.get('latency_ms', 10)
        expected_edge = conditions.get('expected_edge_bps', 5)
        kappa = 0.1  # bps per ms degradation
        
        edge_after_latency = expected_edge - kappa * latency
        
        return max(0, 1 - kappa * latency / expected_edge)
    
    def _estimate_covariance(self, signals: List[Dict], 
                            portfolio_state: Dict) -> np.ndarray:
        """Estimate covariance matrix from historical data"""
        n = len(signals)
        
        # Placeholder - should use actual historical returns
        # For now, use correlation assumption
        correlation = 0.3  # Assume moderate correlation
        std_devs = np.array([s.get('volatility', 0.02) for s in signals])
        
        cov = np.outer(std_devs, std_devs)
        np.fill_diagonal(cov, std_devs ** 2)
        
        # Apply correlation
        for i in range(n):
            for j in range(n):
                if i != j:
                    cov[i, j] *= correlation
                    
        return cov
    
    def _calculate_portfolio_cvar(self, weights: Dict[str, float],
                                 covariance: np.ndarray) -> float:
        """Calculate portfolio CVaR"""
        if not weights:
            return 0.0
            
        # Simplified - should use full simulation
        w = np.array(list(weights.values()))
        portfolio_var = w @ covariance @ w
        portfolio_std = np.sqrt(portfolio_var)
        
        # Approximate CVaR as 2.5 std for 95% confidence
        return 2.5 * portfolio_std


class LedoitWolfEstimator:
    """Ledoit-Wolf covariance shrinkage estimator"""
    
    def shrink(self, sample_cov: np.ndarray) -> np.ndarray:
        """Apply Ledoit-Wolf shrinkage"""
        n = sample_cov.shape[0]
        
        # Shrinkage target (diagonal)
        target = np.diag(np.diag(sample_cov))
        
        # Optimal shrinkage intensity (simplified)
        # In practice, use full Ledoit-Wolf formula
        delta = 0.3  # Shrinkage parameter
        
        return delta * target + (1 - delta) * sample_cov
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 4:**
````python
# tests/unit/test_kelly.py
def test_raw_kelly_calculation():
    optimizer = DynamicKellyOptimizer(max_kelly=0.25)
    
    # Favorable odds
    p = 0.6  # 60% win probability
    b = 2.0  # 2:1 payoff
    
    kelly = optimizer.calculate_raw_kelly(p, b)
    expected = (2 * 0.6 - 0.4) / 2  # 0.4
    
    assert abs(kelly - min(expected, 0.25)) < 0.01

def test_evt_cvar():
    evt = EVTCVaR(confidence_level=0.95)
    
    # Generate loss data with fat tail
    np.random.seed(42)
    normal_losses = np.random.normal(0, 1, 900)
    tail_losses = np.random.pareto(2, 100) * 3
    losses = np.concatenate([normal_losses, tail_losses])
    
    # Fit GPD
    evt.fit_gpd(losses, threshold_percentile=90)
    
    # Calculate CVaR
    cvar, (ci_lower, ci_upper) = evt.calculate_cvar(losses)
    
    assert cvar > np.percentile(losses, 95)  # CVaR > VaR
    assert ci_lower <= cvar <= ci_upper
````

---

### **–§–ê–ó–ê 5: TCA & FILL SIMULATOR** [4 –¥–Ω—ñ]

#### 5.1 Fill Simulator

````python
# core/execution/fill_simulator.py
import numpy as np
from typing import Dict, Optional, Tuple
from dataclasses import dataclass
from scipy.stats import expon

@dataclass
class FillSimulation:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∏–º—É–ª—è—Ü—ñ—ó –≤–∏–∫–æ–Ω–∞–Ω–Ω—è"""
    fill_probability: float
    expected_fill_time: float
    expected_slippage: float
    adverse_selection_cost: float
    total_cost: float

class FillSimulator:
    """Fill simulator –∑–≥—ñ–¥–Ω–æ ¬ß9.1 —Ç–∞ ¬ßR1.2.F"""
    
    def __init__(self):
        self.cox_model = None
        self.hawkes_model = None
        
    def simulate_maker_fill(self, 
                           order_price: float,
                           best_bid: float,
                           best_ask: float,
                           queue_position: int,
                           market_features: Dict) -> FillSimulation:
        """Simulate maker order fill"""
        
        # Extract features
        spread = best_ask - best_bid
        tbr = market_features.get('trade_to_book_ratio', 0.5)
        obi = market_features.get('obi', 0)
        cancel_rate = market_features.get('cancel_rate', 0.3)
        
        # Cox hazard model for fill intensity
        z_features = np.array([tbr, queue_position, spread, obi, cancel_rate])
        
        # Baseline hazard (exponential)
        lambda_0 = 0.1  # Base fill rate per second
        
        # Cox model coefficients (should be fitted on historical data)
        theta = np.array([-0.5, 0.3, -0.2, 0.1, 0.2])
        
        # Fill intensity
        lambda_fill = lambda_0 * np.exp(np.dot(theta, z_features))
        
        # Probability of fill within time horizon
        time_horizon = 60  # seconds
        fill_prob = 1 - np.exp(-lambda_fill * time_horizon)
        
        # Expected fill time (if filled)
        expected_time = 1 / lambda_fill if lambda_fill > 0 else np.inf
        
        # Adverse selection cost
        adverse_cost = self._calculate_adverse_selection(
            order_price, market_features
        )
        
        # No slippage for maker (assuming limit order)
        slippage = 0.0
        
        # Total cost
        total_cost = adverse_cost
        
        return FillSimulation(
            fill_probability=fill_prob,
            expected_fill_time=expected_time,
            expected_slippage=slippage,
            adverse_selection_cost=adverse_cost,
            total_cost=total_cost
        )
    
    def simulate_taker_fill(self,
                           size: float,
                           side: str,
                           market_snapshot: Dict) -> FillSimulation:
        """Simulate taker order fill"""
        
        # Extract market state
        spread = market_snapshot.get('spread', 0.001)
        volatility = market_snapshot.get('volatility', 0.01)
        depth = market_snapshot.get('depth_l5', 10000)
        latency = market_snapshot.get('latency_ms', 10)
        
        # Slippage model
        base_slippage = spread / 2  # Cross half spread
        
        # Size impact
        size_impact = (size / depth) * spread
        
        # Volatility impact
        vol_impact = volatility * np.sqrt(latency / 1000)
        
        # Total expected slippage
        expected_slippage = base_slippage + size_impact + vol_impact
        
        # Adverse selection for taker
        adverse_cost = self._calculate_adverse_selection(
            None, market_snapshot
        )
        
        # Fill probability (taker almost always fills)
        fill_prob = 0.99
        
        # Expected fill time (immediate for taker)
        expected_time = latency / 1000  # Convert to seconds
        
        # Total cost
        total_cost = expected_slippage + adverse_cost
        
        return FillSimulation(
            fill_probability=fill_prob,
            expected_fill_time=expected_time,
            expected_slippage=expected_slippage,
            adverse_selection_cost=adverse_cost,
            total_cost=total_cost
        )
    
    def _calculate_adverse_selection(self, 
                                   order_price: Optional[float],
                                   market_features: Dict) -> float:
        """Calculate adverse selection cost"""
        
        # Hawkes process for order flow clustering
        clustering_intensity = market_features.get('clustering', 0.3)
        
        # Probability of being picked off
        pickoff_prob = 0.1 * (1 + clustering_intensity)
        
        # Expected adverse move
        volatility = market_features.get('volatility', 0.01)
        expected_adverse_move = volatility * 0.5  # Half volatility
        
        return pickoff_prob * expected_adverse_move

class HawkesOrderFlow:
    """Hawkes process for order flow modeling"""
    
    def __init__(self, baseline_intensity: float = 1.0,
                 decay_rate: float = 1.0,
                 excitation: float = 0.5):
        self.mu = baseline_intensity
        self.alpha = excitation
        self.beta = decay_rate
        self.events = []
        
    def intensity(self, t: float) -> float:
        """Calculate intensity at time t"""
        intensity = self.mu
        
        for event_time in self.events:
            if t > event_time:
                intensity += self.alpha * np.exp(-self.beta * (t - event_time))
                
        return intensity
    
    def simulate(self, T: float) -> np.ndarray:
        """Simulate Hawkes process up to time T"""
        events = []
        t = 0
        
        while t < T:
            lambda_t = self.intensity(t)
            
            # Generate next event time
            dt = np.random.exponential(1 / lambda_t)
            t += dt
            
            if t < T:
                events.append(t)
                self.events.append(t)
                
        return np.array(events)
````

#### 5.2 TCA (Transaction Cost Analysis)

````python
# core/execution/tca.py
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

@dataclass
class TCAResult:
    """Transaction Cost Analysis results"""
    raw_edge: float
    fees: float
    spread_cost: float
    slippage: float
    adverse_selection: float
    latency_cost: float
    rebates: float
    net_edge: float
    components: Dict[str, float]

class TransactionCostAnalyzer:
    """TCA –∑–≥—ñ–¥–Ω–æ ¬ß4.3 —Ç–∞ ¬ß9.2"""
    
    def __init__(self):
        self.kappa = 0.1  # bps per ms latency degradation
        self.fee_schedule = {
            'maker': -0.0002,  # 2bps rebate
            'taker': 0.0004    # 4bps fee
        }
        
    def analyze(self, 
               signal: Dict,
               execution_mode: str,
               market_conditions: Dict) -> TCAResult:
        """Full TCA for a trading signal"""
        
        # Raw edge from signal
        raw_edge = signal.get('expected_return_bps', 0)
        
        # Fee component
        fees = self._calculate_fees(
            execution_mode,
            signal.get('size', 0),
            signal.get('price', 0)
        )
        
        # Spread cost
        spread_cost = self._calculate_spread_cost(
            execution_mode,
            market_conditions.get('spread_bps', 0)
        )
        
        # Slippage
        slippage = self._calculate_slippage(
            signal.get('size', 0),
            market_conditions
        )
        
        # Adverse selection
        adverse_selection = self._calculate_adverse_selection(
            signal,
            market_conditions
        )
        
        # Latency cost
        latency_cost = self._calculate_latency_cost(
            raw_edge,
            market_conditions.get('latency_ms', 0)
        )
        
        # Rebates (if maker)
        rebates = -fees if execution_mode == 'maker' and fees < 0 else 0
        
        # Net edge
        net_edge = (raw_edge - fees - spread_cost - slippage - 
                   adverse_selection - latency_cost + rebates)
        
        # Components breakdown
        components = {
            'raw_edge': raw_edge,
            'fees': fees,
            'spread_cost': spread_cost,
            'slippage': slippage,
            'adverse_selection': adverse_selection,
            'latency_cost': latency_cost,
            'rebates': rebates,
            'net_edge': net_edge
        }
        
        return TCAResult(
            raw_edge=raw_edge,
            fees=fees,
            spread_cost=spread_cost,
            slippage=slippage,
            adverse_selection=adverse_selection,
            latency_cost=latency_cost,
            rebates=rebates,
            net_edge=net_edge,
            components=components
        )
    
    def _calculate_fees(self, mode: str, size: float, price: float) -> float:
        """Calculate exchange fees in bps"""
        fee_rate = self.fee_schedule.get(mode, 0.0004)
        return abs(fee_rate) * 10000  # Convert to bps
    
    def _calculate_spread_cost(self, mode: str, spread_bps: float) -> float:
        """Calculate spread crossing cost"""
        if mode == 'maker':
            return 0  # Maker doesn't cross spread
        else:
            return spread_bps / 2  # Taker crosses half spread
    
    def _calculate_slippage(self, size: float, conditions: Dict) -> float:
        """Calculate expected slippage"""
        # Linear impact model
        depth = conditions.get('depth_l5', 10000)
        volatility = conditions.get('volatility', 0.01)
        
        # Size impact
        size_impact = (size / depth) * 100  # bps
        
        # Volatility impact
        vol_impact = volatility * 100 * np.sqrt(size / depth)
        
        return size_impact + vol_impact
    
    def _calculate_adverse_selection(self, signal: Dict, 
                                   conditions: Dict) -> float:
        """Calculate adverse selection cost"""
        # Information asymmetry proxy
        info_ratio = conditions.get('informed_flow_ratio', 0.3)
        
        # Signal staleness
        signal_age = signal.get('age_ms', 0)
        staleness_factor = 1 + signal_age / 1000
        
        # Base adverse selection
        base_adverse = 2.0  # bps
        
        return base_adverse * info_ratio * staleness_factor
    
    def _calculate_latency_cost(self, raw_edge: float, 
                               latency_ms: float) -> float:
        """Calculate latency degradation of edge"""
        # Linear degradation model from ¬ß9.2
        return self.kappa * latency_ms
    
    def expected_pnl_after_latency(self, raw_edge: float,
                                  latency_ms: float) -> float:
        """E[Œ†(‚Ñì)] calculation from ¬ß9.2"""
        return raw_edge - self.kappa * latency_ms
    
    def maximum_tolerable_latency(self, raw_edge: float) -> float:
        """Maximum latency before edge disappears"""
        if self.kappa <= 0:
            return np.inf
        return raw_edge / self.kappa
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 5:**
````python
# tests/unit/test_execution.py
def test_maker_fill_simulation():
    simulator = FillSimulator()
    
    market_features = {
        'trade_to_book_ratio': 0.3,
        'obi': 0.1,
        'cancel_rate': 0.2
    }
    
    result = simulator.simulate_maker_fill(
        order_price=100.0,
        best_bid=99.95,
        best_ask=100.05,
        queue_position=5,
        market_features=market_features
    )
    
    assert 0 <= result.fill_probability <= 1
    assert result.expected_fill_time > 0
    assert result.expected_slippage == 0  # Maker shouldn't have slippage

def test_tca_net_edge():
    tca = TransactionCostAnalyzer()
    
    signal = {
        'expected_return_bps': 10.0,
        'size': 100,
        'price': 100,
        'age_ms': 50
    }
    
    market_conditions = {
        'spread_bps': 5,
        'latency_ms': 20,
        'depth_l5': 10000,
        'volatility': 0.01
    }
    
    result = tca.analyze(signal, 'taker', market_conditions)
    
    assert result.net_edge < result.raw_edge  # Costs reduce edge
    assert result.net_edge == (result.raw_edge - result.fees - 
                               result.spread_cost - result.slippage -
                               result.adverse_selection - result.latency_cost +
                               result.rebates)
````

---

### **–§–ê–ó–ê 6: SPRT/GLR GOVERNANCE** [3 –¥–Ω—ñ]

#### 6.1 SPRT —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ–ª—ñ—Ç–∏–∫

````python
# core/governance/sprt.py
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from scipy import stats

@dataclass
class SPRTResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç SPRT —Ç–µ—Å—Ç—É"""
    decision: Optional[str]  # 'accept_h1', 'accept_h0', None
    log_likelihood_ratio: float
    n_samples: int
    p_value: float
    confidence: float

class SPRTTester:
    """Sequential Probability Ratio Test –∑–≥—ñ–¥–Ω–æ ¬ß13.3"""
    
    def __init__(self, alpha: float = 0.05, beta: float = 0.20):
        self.alpha = alpha  # Type I error
        self.beta = beta    # Type II error
        
        # Decision boundaries
        self.upper_bound = np.log((1 - self.beta) / self.alpha)
        self.lower_bound = np.log(self.beta / (1 - self.alpha))
        
        self.reset()
        
    def reset(self) -> None:
        """Reset test state"""
        self.log_likelihood_ratio = 0.0
        self.n_samples = 0
        self.data_h0 = []
        self.data_h1 = []
        
    def update(self, observation: float, 
              h0_params: Dict, h1_params: Dict) -> SPRTResult:
        """Update SPRT with new observation"""
        
        self.n_samples += 1
        
        # Calculate likelihoods
        ll_h0 = self._log_likelihood(observation, h0_params)
        ll_h1 = self._log_likelihood(observation, h1_params)
        
        # core/governance/sprt.py
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from scipy import stats

@dataclass
class SPRTResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç SPRT —Ç–µ—Å—Ç—É"""
    decision: Optional[str]  # 'accept_h1', 'accept_h0', None
    log_likelihood_ratio: float
    n_samples: int
    p_value: float
    confidence: float

class SPRTTester:
    """Sequential Probability Ratio Test –∑–≥—ñ–¥–Ω–æ ¬ß13.3"""
    
    def __init__(self, alpha: float = 0.05, beta: float = 0.20):
        self.alpha = alpha  # Type I error
        self.beta = beta    # Type II error
        
        # Decision boundaries
        self.upper_bound = np.log((1 - self.beta) / self.alpha)
        self.lower_bound = np.log(self.beta / (1 - self.alpha))
        
        self.reset()
        
    def reset(self) -> None:
        """Reset test state"""
        self.log_likelihood_ratio = 0.0
        self.n_samples = 0
        self.data_h0 = []
        self.data_h1 = []
        
    def update(self, observation: float, 
              h0_params: Dict, h1_params: Dict) -> SPRTResult:
        """Update SPRT with new observation"""
        
        self.n_samples += 1
        
        # Calculate likelihoods
        ll_h0 = self._log_likelihood(observation, h0_params)
        ll_h1 = self._log_likelihood(observation, h1_params)
        
        
–°–±–æ—Ä —Å–≤–µ–¥–µ–Ω–∏–π –æ —Ä–∞–±–æ—á–µ–π –æ–±–ª–∞—Å—Ç–∏–ü—Ä–æ–¥–æ–≤–∂—É—é —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—é –∑ –º—ñ—Å—Ü—è –∑—É–ø–∏–Ω–∫–∏ –≤ Road_map.md. –ó–∞–≤–µ—Ä—à—É—é –§–ê–ó–£ 6 (SPRT/GLR GOVERNANCE):

## üìã –§–ê–ó–ò –†–ï–ê–õ–Ü–ó–ê–¶–Ü–á (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)

### **–§–ê–ó–ê 6: SPRT/GLR GOVERNANCE** [3 –¥–Ω—ñ] (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)

#### 6.1 SPRT —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ–ª—ñ—Ç–∏–∫ (–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è)

````python
# core/governance/sprt.py (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)
    def update(self, observation: float, 
              h0_params: Dict, h1_params: Dict) -> SPRTResult:
        """Update SPRT with new observation"""
        self.data_h0.append(observation)
        self.data_h1.append(observation)
        
        # Calculate log-likelihoods
        ll_h0 = self._log_likelihood(observation, h0_params)
        ll_h1 = self._log_likelihood(observation, h1_params)
        
        # Update cumulative log-likelihood ratio
        self.log_lr += ll_h1 - ll_h0
        
        # Calculate decision boundaries
        log_A = np.log((1 - self.beta) / self.alpha)
        log_B = np.log(self.beta / (1 - self.alpha))
        
        # Make decision
        decision = None
        if self.log_lr >= log_A:
            decision = 'accept_h1'
        elif self.log_lr <= log_B:
            decision = 'accept_h0'
            
        # Calculate p-value
        p_value = self._calculate_p_value()
        
        # Calculate confidence
        confidence = self._calculate_confidence()
        
        return SPRTResult(
            decision=decision,
            log_likelihood_ratio=self.log_lr,
            n_samples=len(self.data_h0),
            p_value=p_value,
            confidence=confidence
        )
    
    def _log_likelihood(self, x: float, params: Dict) -> float:
        """Calculate log-likelihood under given parameters"""
        if params['type'] == 'normal':
            mu = params['mu']
            sigma = params['sigma']
            return -0.5 * np.log(2 * np.pi * sigma**2) - \
                   0.5 * ((x - mu) / sigma) ** 2
        elif params['type'] == 'bernoulli':
            p = params['p']
            return x * np.log(p) + (1 - x) * np.log(1 - p)
        else:
            raise ValueError(f"Unknown distribution: {params['type']}")
    
    def _calculate_p_value(self) -> float:
        """Calculate p-value for current test statistic"""
        # Simplified - should use proper null distribution
        if len(self.data_h0) < 2:
            return 0.5
            
        # Use permutation test
        n_permutations = 1000
        null_lrs = []
        
        for _ in range(n_permutations):
            shuffled = np.random.permutation(self.data_h0)
            lr = 0
            for x in shuffled[:len(self.data_h0)]:
                lr += self._log_likelihood(x, {'type': 'normal', 'mu': 0, 'sigma': 1})
            null_lrs.append(lr)
            
        p_value = np.mean(np.array(null_lrs) >= self.log_lr)
        return p_value
    
    def _calculate_confidence(self) -> float:
        """Calculate confidence in decision"""
        # Based on distance from decision boundary
        log_A = np.log((1 - self.beta) / self.alpha)
        log_B = np.log(self.beta / (1 - self.alpha))
        
        if self.log_lr >= log_A:
            confidence = min(1.0, (self.log_lr - log_A) / log_A)
        elif self.log_lr <= log_B:
            confidence = min(1.0, (log_B - self.log_lr) / abs(log_B))
        else:
            # In between boundaries
            distance_to_boundaries = min(abs(self.log_lr - log_A), 
                                        abs(self.log_lr - log_B))
            confidence = 1 - (distance_to_boundaries / abs(log_A - log_B))
            
        return confidence
````

#### 6.2 Policy Management –∑ A/B —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è–º

````python
# core/governance/policy_manager.py
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import json
import hashlib
from datetime import datetime

class PolicyStatus(Enum):
    CANDIDATE = "candidate"
    CANARY = "canary"
    SHADOW = "shadow"
    LIVE = "live"
    DEPRECATED = "deprecated"
    FAILED = "failed"

@dataclass
class PolicyMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–ª—ñ—Ç–∏–∫–∏"""
    n_trades: int
    win_rate: float
    avg_pnl: float
    sharpe_ratio: float
    max_drawdown: float
    cvar_95: float
    ece: float
    brier_score: float
    log_loss: float

@dataclass
class Policy:
    """–¢–æ—Ä–≥–æ–≤–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞"""
    id: str
    version: str
    status: PolicyStatus
    parameters: Dict
    metrics: Optional[PolicyMetrics]
    created_at: datetime
    shadow_start: Optional[datetime]
    live_start: Optional[datetime]
    test_results: Dict

class PolicyManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–ª—ñ—Ç–∏–∫ –∑ SPRT/GLR —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è–º"""
    
    def __init__(self):
        self.policies = {}
        self.active_policy_id = None
        self.baseline_policy_id = None
        self.sprt_testers = {}
        self.alpha_ledger = []  # Alpha spending ledger
        
    def register_policy(self, parameters: Dict) -> str:
        """–†–µ—î—Å—Ç—Ä—É—î –Ω–æ–≤—É –ø–æ–ª—ñ—Ç–∏–∫—É"""
        # Generate unique ID
        param_str = json.dumps(parameters, sort_keys=True)
        policy_id = hashlib.sha256(param_str.encode()).hexdigest()[:12]
        
        # Create policy
        policy = Policy(
            id=policy_id,
            version=self._get_next_version(),
            status=PolicyStatus.CANDIDATE,
            parameters=parameters,
            metrics=None,
            created_at=datetime.now(),
            shadow_start=None,
            live_start=None,
            test_results={}
        )
        
        self.policies[policy_id] = policy
        
        # Initialize SPRT tester
        self.sprt_testers[policy_id] = SPRTTester(alpha=0.05, beta=0.20)
        
        return policy_id
    
    def start_canary_test(self, policy_id: str, 
                          traffic_percentage: float = 0.1) -> None:
        """–ó–∞–ø—É—Å–∫–∞—î –∫–∞–Ω–∞—Ä–µ–π–∫—É –¥–ª—è –ø–æ–ª—ñ—Ç–∏–∫–∏"""
        if policy_id not in self.policies:
            raise ValueError(f"Policy {policy_id} not found")
            
        policy = self.policies[policy_id]
        
        # Check if ready for canary
        if policy.status != PolicyStatus.CANDIDATE:
            raise ValueError(f"Policy {policy_id} not in candidate status")
            
        # Start canary
        policy.status = PolicyStatus.CANARY
        policy.shadow_start = datetime.now()
        
        # Log alpha spending
        self._log_alpha_spending(policy_id, 'canary_start', traffic_percentage)
        
    def update_policy_metrics(self, policy_id: str, 
                             trade_result: Dict) -> SPRTResult:
        """–û–Ω–æ–≤–ª—é—î –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–ª—ñ—Ç–∏–∫–∏ —Ç–∞ –∑–∞–ø—É—Å–∫–∞—î SPRT"""
        if policy_id not in self.policies:
            raise ValueError(f"Policy {policy_id} not found")
            
        # Update metrics
        metrics = self._update_metrics(policy_id, trade_result)
        
        # Run SPRT test against baseline
        if self.baseline_policy_id and policy_id != self.baseline_policy_id:
            baseline_metrics = self.policies[self.baseline_policy_id].metrics
            
            if baseline_metrics and metrics:
                # Test on Sharpe ratio improvement
                h0_params = {
                    'type': 'normal',
                    'mu': baseline_metrics.sharpe_ratio,
                    'sigma': 0.5  # Assumed std
                }
                h1_params = {
                    'type': 'normal',
                    'mu': baseline_metrics.sharpe_ratio * 1.2,  # 20% improvement
                    'sigma': 0.5
                }
                
                sprt_result = self.sprt_testers[policy_id].update(
                    metrics.sharpe_ratio,
                    h0_params,
                    h1_params
                )
                
                # Process SPRT decision
                self._process_sprt_decision(policy_id, sprt_result)
                
                return sprt_result
                
        return None
    
    def _process_sprt_decision(self, policy_id: str, 
                               sprt_result: SPRTResult) -> None:
        """–û–±—Ä–æ–±–ª—è—î —Ä—ñ—à–µ–Ω–Ω—è SPRT"""
        policy = self.policies[policy_id]
        
        if sprt_result.decision == 'accept_h1':
            # Policy is better than baseline
            if policy.status == PolicyStatus.CANARY:
                self._promote_to_shadow(policy_id)
            elif policy.status == PolicyStatus.SHADOW:
                self._promote_to_live(policy_id)
                
        elif sprt_result.decision == 'accept_h0':
            # Policy is not better
            policy.status = PolicyStatus.FAILED
            policy.test_results['sprt'] = {
                'decision': sprt_result.decision,
                'log_lr': sprt_result.log_likelihood_ratio,
                'p_value': sprt_result.p_value,
                'n_samples': sprt_result.n_samples
            }
            
    def _promote_to_shadow(self, policy_id: str) -> None:
        """–ü—Ä–æ–º–æ—É—Ç–∏—Ç—å –ø–æ–ª—ñ—Ç–∏–∫—É –¥–æ shadow —Ä–µ–∂–∏–º—É"""
        policy = self.policies[policy_id]
        policy.status = PolicyStatus.SHADOW
        
        # Run in parallel with live for comparison
        self._log_alpha_spending(policy_id, 'shadow_promotion', 1.0)
        
    def _promote_to_live(self, policy_id: str) -> None:
        """–ü—Ä–æ–º–æ—É—Ç–∏—Ç—å –ø–æ–ª—ñ—Ç–∏–∫—É –¥–æ live"""
        # Deprecate current live policy
        if self.active_policy_id:
            self.policies[self.active_policy_id].status = PolicyStatus.DEPRECATED
            
        # Promote new policy
        policy = self.policies[policy_id]
        policy.status = PolicyStatus.LIVE
        policy.live_start = datetime.now()
        self.active_policy_id = policy_id
        
        # Update baseline
        self.baseline_policy_id = policy_id
        
        # Log promotion
        self._log_alpha_spending(policy_id, 'live_promotion', 1.0)
        
    def _update_metrics(self, policy_id: str, 
                       trade_result: Dict) -> PolicyMetrics:
        """–û–Ω–æ–≤–ª—é—î –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–ª—ñ—Ç–∏–∫–∏"""
        policy = self.policies[policy_id]
        
        # Initialize or update metrics
        if policy.metrics is None:
            policy.metrics = PolicyMetrics(
                n_trades=0,
                win_rate=0,
                avg_pnl=0,
                sharpe_ratio=0,
                max_drawdown=0,
                cvar_95=0,
                ece=0,
                brier_score=0,
                log_loss=0
            )
            
        # Update with exponential moving average
        alpha = 0.1  # EMA parameter
        metrics = policy.metrics
        
        metrics.n_trades += 1
        
        # Update win rate
        is_win = trade_result['pnl'] > 0
        metrics.win_rate = alpha * is_win + (1 - alpha) * metrics.win_rate
        
        # Update average PnL
        metrics.avg_pnl = alpha * trade_result['pnl'] + (1 - alpha) * metrics.avg_pnl
        
        # Update other metrics (simplified)
        # In practice, these need proper rolling window calculations
        
        return metrics
    
    def _log_alpha_spending(self, policy_id: str, 
                           event: str, 
                           alpha_spent: float) -> None:
        """–õ–æ–≥—É—î –≤–∏—Ç—Ä–∞—Ç–∏ alpha –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é FDR"""
        self.alpha_ledger.append({
            'timestamp': datetime.now(),
            'policy_id': policy_id,
            'event': event,
            'alpha_spent': alpha_spent,
            'cumulative_alpha': sum(e['alpha_spent'] for e in self.alpha_ledger)
        })
        
    def get_fdr_adjusted_threshold(self, n_tests: int) -> float:
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î FDR-adjusted threshold (Benjamini-Hochberg)"""
        base_alpha = 0.05
        return base_alpha / n_tests
        
    def _get_next_version(self) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –Ω–∞—Å—Ç—É–ø–Ω—É –≤–µ—Ä—Å—ñ—é"""
        versions = [p.version for p in self.policies.values()]
        if not versions:
            return "1.0.0"
            
        # Parse versions and increment
        latest = max(versions, key=lambda v: tuple(map(int, v.split('.'))))
        major, minor, patch = map(int, latest.split('.'))
        return f"{major}.{minor}.{patch + 1}"
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 6:**

````python
# tests/unit/test_governance.py
import pytest
import numpy as np
from core.governance.sprt import SPRTTester, SPRTResult
from core.governance.policy_manager import PolicyManager, PolicyStatus

def test_sprt_accept_h1():
    """Test SPRT accepts H1 when data supports it"""
    tester = SPRTTester(alpha=0.05, beta=0.20)
    
    # H0: mu=0, H1: mu=1
    h0_params = {'type': 'normal', 'mu': 0, 'sigma': 1}
    h1_params = {'type': 'normal', 'mu': 1, 'sigma': 1}
    
    # Generate data from H1
    np.random.seed(42)
    decision = None
    
    for _ in range(100):
        observation = np.random.normal(1, 1)  # Data from H1
        result = tester.update(observation, h0_params, h1_params)
        
        if result.decision is not None:
            decision = result.decision
            break
            
    assert decision == 'accept_h1'

def test_sprt_accept_h0():
    """Test SPRT accepts H0 when data supports it"""
    tester = SPRTTester(alpha=0.05, beta=0.20)
    
    # H0: mu=0, H1: mu=1
    h0_params = {'type': 'normal', 'mu': 0, 'sigma': 1}
    h1_params = {'type': 'normal', 'mu': 1, 'sigma': 1}
    
    # Generate data from H0
    np.random.seed(42)
    decision = None
    
    for _ in range(100):
        observation = np.random.normal(0, 1)  # Data from H0
        result = tester.update(observation, h0_params, h1_params)
        
        if result.decision is not None:
            decision = result.decision
            break
            
    assert decision == 'accept_h0'

def test_policy_registration():
    """Test policy registration and versioning"""
    manager = PolicyManager()
    
    # Register first policy
    params1 = {'strategy': 'scalp', 'threshold': 0.5}
    policy_id1 = manager.register_policy(params1)
    
    assert policy_id1 in manager.policies
    assert manager.policies[policy_id1].version == "1.0.0"
    assert manager.policies[policy_id1].status == PolicyStatus.CANDIDATE
    
    # Register second policy
    params2 = {'strategy': 'scalp', 'threshold': 0.6}
    policy_id2 = manager.register_policy(params2)
    
    assert policy_id2 in manager.policies
    assert manager.policies[policy_id2].version == "1.0.1"

def test_canary_promotion():
    """Test canary test and promotion flow"""
    manager = PolicyManager()
    
    # Register and start canary
    params = {'strategy': 'scalp', 'threshold': 0.5}
    policy_id = manager.register_policy(params)
    
    manager.start_canary_test(policy_id, traffic_percentage=0.1)
    
    assert manager.policies[policy_id].status == PolicyStatus.CANARY
    assert manager.policies[policy_id].shadow_start is not None
    
    # Check alpha ledger
    assert len(manager.alpha_ledger) == 1
    assert manager.alpha_ledger[0]['event'] == 'canary_start'

def test_fdr_adjustment():
    """Test FDR adjustment for multiple tests"""
    manager = PolicyManager()
    
    # Test Benjamini-Hochberg adjustment
    threshold_1_test = manager.get_fdr_adjusted_threshold(1)
    threshold_10_tests = manager.get_fdr_adjusted_threshold(10)
    
    assert threshold_1_test == 0.05
    assert threshold_10_tests == 0.005
````

---

### **–§–ê–ó–ê 7: XAI-–õ–û–ì–£–í–ê–ù–ù–Ø –¢–ê OBSERVABILITY** [3 –¥–Ω—ñ]

#### 7.1 XAI Logger –∑ –ø–æ–≤–Ω–∏–º edge breakdown

````python
# core/observability/xai_logger.py
import json
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import numpy as np

@dataclass
class DecisionTrace:
    """–ü–æ–≤–Ω–∏–π trace —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è XAI"""
    decision_id: str
    timestamp: float
    symbol: str
    
    # Features
    raw_features: Dict[str, float]
    normalized_features: Dict[str, float]
    feature_weights: Dict[str, float]
    
    # Scoring
    raw_score: float
    raw_probability: float
    calibrated_probability: float
    confidence_interval: Tuple[float, float]
    
    # Edge breakdown
    raw_edge_bps: float
    fees_bps: float
    spread_cost_bps: float
    slippage_bps: float
    adverse_selection_bps: float
    latency_cost_bps: float
    rebates_bps: float
    net_edge_bps: float
    
    # Risk metrics
    trade_cvar: float
    portfolio_cvar: float
    kelly_fraction: float
    position_size: float
    
    # Gates
    gates_passed: List[str]
    gates_failed: List[str]
    rejection_reasons: List[str]
    
    # Regime
    market_regime: str
    regime_confidence: float
    
    # Multipliers
    lambda_cal: float
    lambda_reg: float
    lambda_liq: float
    lambda_dd: float
    lambda_lat: float
    total_multiplier: float
    
    # Decision
    final_decision: str  # 'ENTER', 'SKIP', 'EXIT'
    execution_mode: str  # 'maker', 'taker', 'none'
    
    # Metadata
    policy_id: str
    policy_version: str
    latency_ms: float

class XAILogger:
    """XAI –ª–æ–≥–µ—Ä –∑ –ø–æ–≤–Ω–æ—é —Ç—Ä–∞—Å–∞–±–µ–ª—å–Ω—ñ—Å—Ç—é —Ä—ñ—à–µ–Ω—å"""
    
    def __init__(self, log_dir: str = "logs/xai"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.current_file = None
        self.file_handle = None
        self.rotation_size = 100 * 1024 * 1024  # 100MB
        self.buffer = []
        self.buffer_size = 100
        
        self._rotate_file()
        
    def log_decision(self, trace: DecisionTrace) -> None:
        """–õ–æ–≥—É—î –ø–æ–≤–Ω–∏–π trace —Ä—ñ—à–µ–Ω–Ω—è"""
        # Convert to dict
        trace_dict = asdict(trace)
        
        # Add derived metrics
        trace_dict['edge_components'] = {
            'raw': trace.raw_edge_bps,
            'fees': -abs(trace.fees_bps),
            'spread': -abs(trace.spread_cost_bps),
            'slippage': -abs(trace.slippage_bps),
            'adverse': -abs(trace.adverse_selection_bps),
            'latency': -abs(trace.latency_cost_bps),
            'rebates': abs(trace.rebates_bps),
            'net': trace.net_edge_bps
        }
        
        # Add decision explanation
        trace_dict['explanation'] = self._generate_explanation(trace)
        
        # Add to buffer
        self.buffer.append(trace_dict)
        
        # Flush if buffer is full
        if len(self.buffer) >= self.buffer_size:
            self._flush_buffer()
            
    def _generate_explanation(self, trace: DecisionTrace) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –ª—é–¥–∏–Ω–æ-—á–∏—Ç–∞–±–µ–ª—å–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —Ä—ñ—à–µ–Ω–Ω—è"""
        explanation = []
        
        # Decision summary
        if trace.final_decision == 'ENTER':
            explanation.append(
                f"ENTERED {trace.symbol} with {trace.execution_mode} order"
            )
        elif trace.final_decision == 'SKIP':
            explanation.append(
                f"SKIPPED {trace.symbol}: {', '.join(trace.rejection_reasons)}"
            )
        else:
            explanation.append(f"EXITED {trace.symbol}")
            
        # Probability and edge
        explanation.append(
            f"P={trace.calibrated_probability:.3f} "
            f"[{trace.confidence_interval[0]:.3f}, {trace.confidence_interval[1]:.3f}], "
            f"Net Edge={trace.net_edge_bps:.2f}bps"
        )
        
        # Failed gates
        if trace.gates_failed:
            explanation.append(f"Failed gates: {', '.join(trace.gates_failed)}")
            
        # Risk metrics
        explanation.append(
            f"Kelly={trace.kelly_fraction:.3f}, "
            f"CVaR={trace.trade_cvar:.2f}"
        )
        
        # Regime
        explanation.append(
            f"Regime={trace.market_regime} "
            f"(conf={trace.regime_confidence:.2f})"
        )
        
        # Multipliers if reduced
        if trace.total_multiplier < 0.9:
            explanation.append(
                f"Multiplier reduced to {trace.total_multiplier:.2f} "
                f"(cal={trace.lambda_cal:.2f}, reg={trace.lambda_reg:.2f}, "
                f"liq={trace.lambda_liq:.2f}, dd={trace.lambda_dd:.2f}, "
                f"lat={trace.lambda_lat:.2f})"
            )
            
        return " | ".join(explanation)
    
    def _flush_buffer(self) -> None:
        """Flush buffer to file"""
        if not self.buffer:
            return
            
        # Check if rotation needed
        if self._should_rotate():
            self._rotate_file()
            
        # Write buffer
        for trace_dict in self.buffer:
            json_line = json.dumps(trace_dict, default=str)
            self.file_handle.write(json_line + '\n')
            
        self.file_handle.flush()
        self.buffer.clear()
        
    def _should_rotate(self) -> bool:
        """Check if file rotation is needed"""
        if self.current_file and self.current_file.exists():
            return self.current_file.stat().st_size > self.rotation_size
        return False
        
    def _rotate_file(self) -> None:
        """Rotate log file"""
        # Close current file
        if self.file_handle:
            self._flush_buffer()
            self.file_handle.close()
            
        # Create new file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_file = self.log_dir / f"xai_decisions_{timestamp}.jsonl"
        self.file_handle = open(self.current_file, 'a')
        
    def query_decisions(self, 
                       symbol: Optional[str] = None,
                       start_time: Optional[float] = None,
                       end_time: Optional[float] = None,
                       decision_type: Optional[str] = None) -> List[Dict]:
        """Query logged decisions"""
        results = []
        
        # Search through all log files
        for log_file in sorted(self.log_dir.glob("xai_decisions_*.jsonl")):
            with open(log_file, 'r') as f:
                for line in f:
                    try:
                        trace = json.loads(line)
                        
                        # Apply filters
                        if symbol and trace['symbol'] != symbol:
                            continue
                        if start_time and trace['timestamp'] < start_time:
                            continue
                        if end_time and trace['timestamp'] > end_time:
                            continue
                        if decision_type and trace['final_decision'] != decision_type:
                            continue
                            
                        results.append(trace)
                    except json.JSONDecodeError:
                        continue
                        
        return results
    
    def generate_report(self, 
                        start_time: float,
                        end_time: float) -> Dict:
        """Generate XAI report for time period"""
        decisions = self.query_decisions(start_time=start_time, 
                                        end_time=end_time)
        
        if not decisions:
            return {}
            
        report = {
            'period': {
                'start': datetime.fromtimestamp(start_time).isoformat(),
                'end': datetime.fromtimestamp(end_time).isoformat()
            },
            'summary': {
                'total_decisions': len(decisions),
                'entered': sum(1 for d in decisions if d['final_decision'] == 'ENTER'),
                'skipped': sum(1 for d in decisions if d['final_decision'] == 'SKIP'),
                'exited': sum(1 for d in decisions if d['final_decision'] == 'EXIT')
            },
            'edge_analysis': self._analyze_edge(decisions),
            'gate_analysis': self._analyze_gates(decisions),
            'regime_analysis': self._analyze_regimes(decisions),
            'rejection_analysis': self._analyze_rejections(decisions)
        }
        
        return report
    
    def _analyze_edge(self, decisions: List[Dict]) -> Dict:
        """Analyze edge components"""
        entered = [d for d in decisions if d['final_decision'] == 'ENTER']
        
        if not entered:
            return {}
            
        components = ['raw_edge_bps', 'fees_bps', 'spread_cost_bps', 
                     'slippage_bps', 'adverse_selection_bps', 
                     'latency_cost_bps', 'rebates_bps', 'net_edge_bps']
        
        analysis = {}
        for component in components:
            values = [d[component] for d in entered]
            analysis[component] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values),
                'p25': np.percentile(values, 25),
                'p50': np.percentile(values, 50),
                'p75': np.percentile(values, 75)
            }
            
        return analysis
    
    def _analyze_gates(self, decisions: List[Dict]) -> Dict:
        """Analyze gate performance"""
        gate_stats = {}
        
        for decision in decisions:
            for gate in decision['gates_passed']:
                if gate not in gate_stats:
                    gate_stats[gate] = {'passed': 0, 'failed': 0}
                gate_stats[gate]['passed'] += 1
                
            for gate in decision['gates_failed']:
                if gate not in gate_stats:
                    gate_stats[gate] = {'passed': 0, 'failed': 0}
                gate_stats[gate]['failed'] += 1
                
        # Calculate pass rates
        for gate in gate_stats:
            total = gate_stats[gate]['passed'] + gate_stats[gate]['failed']
            gate_stats[gate]['pass_rate'] = gate_stats[gate]['passed'] / total
            
        return gate_stats
    
    def _analyze_regimes(self, decisions: List[Dict]) -> Dict:
        """Analyze decisions by market regime"""
        regime_stats = {}
        
        for decision in decisions:
            regime = decision['market_regime']
            if regime not in regime_stats:
                regime_stats[regime] = {
                    'count': 0,
                    'entered': 0,
                    'avg_edge': 0,
                    'avg_confidence': 0
                }
                
            regime_stats[regime]['count'] += 1
            
            if decision['final_decision'] == 'ENTER':
                regime_stats[regime]['entered'] += 1
                
            regime_stats[regime]['avg_edge'] += decision['net_edge_bps']
            regime_stats[regime]['avg_confidence'] += decision['regime_confidence']
            
        # Calculate averages
        for regime in regime_stats:
            count = regime_stats[regime]['count']
            regime_stats[regime]['entry_rate'] = regime_stats[regime]['entered'] / count
            regime_stats[regime]['avg_edge'] /= count
            regime_stats[regime]['avg_confidence'] /= count
            
        return regime_stats
    
    def _analyze_rejections(self, decisions: List[Dict]) -> Dict:
        """Analyze rejection reasons"""
        rejection_counts = {}
        
        skipped = [d for d in decisions if d['final_decision'] == 'SKIP']
        
        for decision in skipped:
            for reason in decision['rejection_reasons']:
                rejection_counts[reason] = rejection_counts.get(reason, 0) + 1
                
        # Sort by frequency
        sorted_reasons = sorted(rejection_counts.items(), 
                              key=lambda x: x[1], 
                              reverse=True)
        
        return dict(sorted_reasons)
    
    def close(self) -> None:
        """Close logger and flush remaining data"""
        self._flush_buffer()
        if self.file_handle:
            self.file_handle.close()
````

**–¢–µ—Å—Ç–∏ –¥–ª—è –§–ê–ó–ò 7:**

````python
# tests/unit/test_xai_logger.py
import pytest
import tempfile
from pathlib import Path
from core.observability.xai_logger import XAILogger, DecisionTrace

def test_xai_decision_logging():
    """Test XAI decision logging"""
    with tempfile.TemporaryDirectory() as tmpdir:
        logger = XAILogger(log_dir=tmpdir)
        
        # Create test decision trace
        trace = DecisionTrace(
            decision_id="test_001",
            timestamp=1234567890.0,
            symbol="SOL-USDT",
            raw_features={'obi': 0.5, 'tfi': 0.3},
            normalized_features={'obi': 0.0, 'tfi': -0.5},
            feature_weights={'obi': 0.3, 'tfi': 0.2},
            raw_score=0.8,
            raw_probability=0.65,
            calibrated_probability=0.62,
            confidence_interval=(0.55, 0.69),
            raw_edge_bps=10.0,
            fees_bps=0.4,
            spread_cost_bps=2.0,
            slippage_bps=1.0,
            adverse_selection_bps=0.5,
            latency_cost_bps=0.3,
            rebates_bps=0.2,
            net_edge_bps=6.0,
            trade_cvar=50.0,
            portfolio_cvar=200.0,
            kelly_fraction=0.02,
            position_size=1000.0,
            gates_passed=['probability', 'edge', 'regime'],
            gates_failed=[],
            rejection_reasons=[],
            market_regime='trend',
            regime_confidence=0.8,
            lambda_cal=0.95,
            lambda_reg=1.0,
            lambda_liq=0.9,
            lambda_dd=0.85,
            lambda_lat=0.98,
            total_multiplier=0.71,
            final_decision='ENTER',
            execution_mode='taker',
            policy_id='policy_001',
            policy_version='1.0.0',
            latency_ms=15.0
        )
        
        # Log decision
        logger.log_decision(trace)
        logger._flush_buffer()
        
        # Query logged decisions
        decisions = logger.query_decisions(symbol="SOL-USDT")
        
        assert len(decisions) == 1
        assert decisions[0]['decision_id'] == "test_001"
        assert decisions[0]['net_edge_bps'] == 6.0
        assert 'explanation' in decisions[0]
        
        logger.close()

def test_xai_report_generation():
    """Test XAI report generation"""
    with tempfile.TemporaryDirectory() as tmpdir:
        logger = XAILogger(log_dir=tmpdir)
        
        # Log multiple decisions
        for i in range(10):
            trace = DecisionTrace(
                decision_id=f"test_{i:03d}",
                timestamp=1234567890.0 + i,
                symbol="SOL-USDT" if i % 2 == 0 else "SOON-USDT",
                raw_features={'obi': 0.5, 'tfi': 0.3},
                normalized_features={'obi': 0.0, 'tfi': -0.5},
                feature_weights={'obi': 0.3, 'tfi': 0.2},
                raw_score=0.8,
                raw_probability=0.65,
                calibrated_probability=0.62,
                confidence_interval=(0.55, 0.69),
                raw_edge_bps=10.0 - i * 0.5,
                fees_bps=0.4,
                spread_cost_bps=2.0,
                slippage_bps=1.0,
                adverse_selection_bps=0.5,
                latency_cost_bps=0.3,
                rebates_bps=0.2,
                net_edge_bps=6.0 - i * 0.5,
                trade_cvar=50.0,
                portfolio_cvar=200.0,
                kelly_fraction=0.02,
                position_size=1000.0,
                gates_passed=['probability', 'edge'] if i < 7 else ['probability'],
                gates_failed=[] if i < 7 else ['edge'],
                rejection_reasons=[] if i < 7 else ['insufficient_edge'],
                market_regime='trend' if i < 5 else 'grind',
                regime_confidence=0.8,
                lambda_cal=0.95,
                lambda_reg=1.0,
                lambda_liq=0.9,
                lambda_dd=0.85,
                lambda_lat=0.98,
                total_multiplier=0.71,
                final_decision='ENTER' if i < 7 else 'SKIP',
                execution_mode='taker' if i < 7 else 'none',
                policy_id='policy_001',
                policy_version='1.0.0',
                latency_ms=15.0
            )
            logger.log_decision(trace)
            
        logger._flush_buffer()
        
        # Generate report
        report = logger.generate_report(
            start_time=1234567890.0,
            end_time=1234567900.0
        )
        
        assert report['summary']['total_decisions'] == 10
        assert report['summary']['entered'] == 7
        assert report['summary']['skipped'] == 3
        
        # Check edge analysis
        assert 'edge_analysis' in report
        assert 'net_edge_bps' in report['edge_analysis']
        assert 'mean' in report['edge_analysis']['net_edge_bps']
        
        # Check gate analysis
        assert 'gate_analysis' in report
        assert 'edge' in report['gate_analysis']
        assert report['gate_analysis']['edge']['passed'] == 7
        assert report['gate_analysis']['edge']['failed'] == 3
        
        # Check regime analysis
        assert 'regime_analysis' in report
        assert 'trend' in report['regime_analysis']
        assert 'grind' in report['regime_analysis']
        
        # Check rejection analysis
        assert 'rejection_analysis' in report
        assert 'insufficient_edge' in report['rejection_analysis']
        assert report['rejection_analysis']['insufficient_edge'] == 3
        
        logger.close()
````

---

### **–§–ê–ó–ê 8: –Ü–ù–¢–ï–ì–†–ê–¶–Ü–Ø –¢–ê –ö–Ü–ù–¶–ï–í–ï –¢–ï–°–¢–£–í–ê–ù–ù–Ø** [5 –¥–Ω—ñ–≤]

#### 8.1 –ü–æ–≤–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è Aurora Orchestrator

````python
# aurora/orchestrator.py
import asyncio
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

from core.features.microstructure import MicrostructureFeatures
from core.features.cross_asset import CrossAssetFeatures
from core.calibration.probability import ProbabilityCalibrator
from core.calibration.conformal import InductiveConformalPredictor
from core.regime.regime_manager import RegimeManager, MarketRegime
from core.portfolio.kelly import DynamicKellyOptimizer
from core.risk.evt_cvar import EVTCVaR
from core.execution.fill_simulator import FillSimulator
from core.execution.tca import TransactionCostAnalyzer
from core.governance.policy_manager import PolicyManager
from core.observability.xai_logger import XAILogger, DecisionTrace

logger = logging.getLogger(__name__)

@dataclass
class AuroraConfig:
    """Aurora configuration"""
    # Feature parameters
    feature_window: int = 100
    robust_scaling: bool = True
    
    # Calibration
    calibration_method: str = 'isotonic'
    icp_significance: float = 0.05
    
    # Risk
    max_kelly: float = 0.25
    cvar_confidence: float = 0.95
    max_portfolio_cvar: float = 1000.0
    
    # Execution
    default_execution_mode: str = 'taker'
    max_latency_ms: float = 50.0
    
    # Gates
    min_edge_bps: float = 2.0
    max_spread_bps: float = 20.0
    max_volatility: float = 0.05
    
    # Universe
    symbols: List[str] = None
    leader_symbol: str = 'SOL-USDT'

class AuroraOrchestrator:
    """Main Aurora orchestrator –∑–≥—ñ–¥–Ω–æ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó R1"""
    
    def __init__(self, config: AuroraConfig):
        self.config = config
        
        # Initialize components
        self.feature_extractor = MicrostructureFeatures(
            robust_scale=config.robust_scaling
        )
        self.cross_asset = CrossAssetFeatures(
            window_size=config.feature_window
        )
        self.calibrator = ProbabilityCalibrator(
            method=config.calibration_method
        )
        self.icp = InductiveConformalPredictor(
            significance_level=config.icp_significance
        )
        self.regime_manager = RegimeManager()
        self.kelly_optimizer = DynamicKellyOptimizer(
            max_kelly=config.max_kelly
        )
        self.evt_cvar = EVTCVaR(
            confidence_level=config.cvar_confidence
        )
        self.fill_simulator = FillSimulator()
        self.tca = TransactionCostAnalyzer()
        self.policy_manager = PolicyManager()
        self.xai_logger = XAILogger()
        
        # State
        self.portfolio_state = {
            'positions': {},
            'capital': 100000,
            'drawdown_pct': 0,
            'peak_equity': 100000
        }
        self.market_cache = {}
        self.historical_pnl = []
        
    async def process_market_update(self, 
                                   market_data: Dict) -> Optional[Dict]:
        """Process market update and generate trading decision"""
        try:
            # 1. Extract features
            features = await self._extract_features(market_data)
            
            # 2. Calculate cross-asset dependencies
            cross_features = await self._calculate_cross_asset(market_data)
            features.update(cross_features)
            
            # 3. Generate signal score
            score = self._calculate_score(features)
            
            # 4. Convert to probability and calibrate
            raw_prob = self._score_to_probability(score)
            calibrated_prob, confidence_interval = await self._calibrate_probability(
                raw_prob
            )
            
            # 5. Check regime
            regime_state = self.regime_manager.update(market_data)
            
            # 6. Calculate expected edge with TCA
            edge_breakdown = await self._calculate_edge(
                market_data, 
                calibrated_prob
            )
            
            # 7. Check entry gates
            gates_result = self._check_gates(
                calibrated_prob,
                edge_breakdown,
                regime_state,
                market_data
            )
            
            # 8. Calculate position size with dynamic Kelly
            if gates_result['passed']:
                kelly_state = await self._calculate_kelly(
                    calibrated_prob,
                    edge_breakdown,
                    regime_state,
                    market_data
                )
                position_size = self._calculate_position_size(
                    kelly_state.final_kelly[market_data['symbol']]
                )
            else:
                kelly_state = None
                position_size = 0
                
            # 9. Make final decision
            decision = self._make_decision(
                gates_result,
                position_size,
                edge_breakdown
            )
            
            # 10. Log decision with XAI
            await self._log_decision(
                market_data,
                features,
                score,
                raw_prob,
                calibrated_prob,
                confidence_interval,
                edge_breakdown,
                regime_state,
                kelly_state,
                gates_result,
                decision
            )
            
            return decision
            
        except Exception as e:
            logger.error(f"Error processing market update: {e}")
            return None
    
    async def _extract_features(self, market_data: Dict) -> Dict:
        """Extract microstructure features"""
        features = {}
        
        # OBI
        features['obi'] = self.feature_extractor.calculate_obi(
            market_data['bid_volumes'],
            market_data['ask_volumes'],
            levels=5
        )
        
        # Microprice
        features['microprice'] = self.feature_extractor.calculate_microprice(
            market_data['bid_price'],
            market_data['ask_price'],
            market_data['bid_volumes'][0],
            market_data['ask_volumes'][0]
        )
        
        # TFI
        features['tfi'] = self.feature_extractor.calculate_tfi(
            market_data.get('trades', []),
            window=60  # 60 seconds
        )
        
        # Absorption
        features['absorption'] = self.feature_extractor.calculate_absorption(
            market_data.get('trades', []),
            market_data.get('price_changes', [])
        )
        
        return features
    
    async def _calculate_cross_asset(self, market_data: Dict) -> Dict:
        """Calculate cross-asset features"""
        if market_data['symbol'] == self.config.leader_symbol:
            return {}
            
        # Get leader data
        leader_data = self.market_cache.get(self.config.leader_symbol)
        if not leader_data:
            return {}
            
        # Calculate beta
        alt_returns = np.array(market_data.get('returns', []))
        sol_returns = np.array(leader_data.get('returns', []))
        
        beta = self.cross_asset.estimate_beta(alt_returns, sol_returns)
        
        # Find optimal lag
        lag, lag_score = self.cross_asset.find_optimal_lag(
            alt_returns, 
            sol_returns
        )
        
        # Calculate cross signal
        if lag > 0 and lag < len(sol_returns):
            cross_signal = beta * sol_returns[-lag]
        else:
            cross_signal = 0
            
        return {
            'beta_to_sol': beta,
            'optimal_lag': lag,
            'lag_score': lag_score,
            'cross_signal': cross_signal
        }
    
    def _calculate_score(self, features: Dict) -> float:
        """Calculate composite score from features"""
        # Simplified linear combination
        # In practice, use trained weights or ML model
        weights = {
            'obi': 0.3,
            'tfi': 0.2,
            'absorption': 0.1,
            'cross_signal': 0.4
        }
        
        score = sum(
            weights.get(name, 0) * value 
            for name, value in features.items()
        )
        
        return score
    
    def _score_to_probability(self, score: float) -> float:
        """Convert score to probability"""
        # Sigmoid transformation
        from scipy.special import expit
        return expit(score)
    
    async def _calibrate_probability(self, 
                                    raw_prob: float) -> Tuple[float, Tuple[float, float]]:
        """Calibrate probability and get confidence interval"""
        # Calibrate
        calibrated = self.calibrator.calibrate(raw_prob)
        
        # Get confidence interval from ICP
        _, confidence_interval = self.icp.predict_with_confidence(raw_prob)
        
        return calibrated, confidence_interval
    
    async def _calculate_edge(self, 
                             market_data: Dict,
                             probability: float) -> Dict:
        """Calculate edge breakdown with TCA"""
        # Expected payoff
        spread = market_data['ask_price'] - market_data['bid_price']
        expected_move = spread * 2  # Simplified
        
        # Raw edge
        raw_edge = (probability * expected_move - (1 - probability) * spread) * 10000
        
        # TCA components
        signal = {
            'expected_return_bps': raw_edge,
            'size': 1000,  # Default size
            'price': market_data['mid_price'],
            'age_ms': 10
        }
        
        tca_result = self.tca.analyze(
            signal,
            self.config.default_execution_mode,
            market_data
        )
        
        return {
            'raw_edge': tca_result.raw_edge,
            'fees': tca_result.fees,
            'spread_cost': tca_result.spread_cost,
            'slippage': tca_result.slippage,
            'adverse_selection': tca_result.adverse_selection,
            'latency_cost': tca_result.latency_cost,
            'rebates': tca_result.rebates,
            'net_edge': tca_result.net_edge
        }
    
    def _check_gates(self, 
                    probability: float,
                    edge_breakdown: Dict,
                    regime_state,
                    market_data: Dict) -> Dict:
        """Check entry gates"""
        gates_passed = []
        gates_failed = []
        rejection_reasons = []
        
        # Probability gate
        min_prob = 0.55
        if probability > min_prob:
            gates_passed.append('probability')
        else:
            gates_failed.append('probability')
            rejection_reasons.append(f'probability_{probability:.3f}_below_{min_prob}')
            
        # Edge gate
        if edge_breakdown['net_edge'] > self.config.min_edge_bps:
            gates_passed.append('edge')
        else:
            gates_failed.append('edge')
            rejection_reasons.append(f'edge_{edge_breakdown["net_edge"]:.2f}bps_below_min')
            
        # Regime gate
        if regime_state.current_regime in ['trend', 'grind']:
            gates_passed.append('regime')
        else:
            gates_failed.append('regime')
            rejection_reasons.append(f'regime_{regime_state.current_regime}_not_suitable')
            
        # Spread gate
        spread_bps = (market_data['ask_price'] - market_data['bid_price']) / \
                     market_data['mid_price'] * 10000
        if spread_bps <= self.config.max_spread_bps:
            gates_passed.append('spread')
        else:
            gates_failed.append('spread')
            rejection_reasons.append(f'spread_{spread_bps:.1f}bps_exceeds_max')
            
        # Volatility gate
        volatility = market_data.get('volatility', 0)
        if volatility <= self.config.max_volatility:
            gates_passed.append('volatility')
        else:
            gates_failed.append('volatility')
            rejection_reasons.append(f'volatility_{volatility:.3f}_exceeds_max')
            
        return {
            'passed': len(gates_failed) == 0,
            'gates_passed': gates_passed,
            'gates_failed': gates_failed,
            'rejection_reasons': rejection_reasons
        }
    
    async def _calculate_kelly(self,
                              probability: float,
                              edge_breakdown: Dict,
                              regime_state,
                              market_data: Dict) -> Any:
        """Calculate Kelly fraction with all multipliers"""
        # Prepare signals for Kelly optimizer
        signals = [{
            'symbol': market_data['symbol'],
            'probability': probability,
            'payoff_ratio': 2.0,  # Simplified
            'expected_return': edge_breakdown['net_edge'] / 10000,
            'volatility': market_data.get('volatility', 0.01)
        }]
        
        # Market conditions for multipliers
        conditions = {
            'ece': 0.03,  # From calibration monitoring
            'log_loss': 0.3,
            'lambda_reg': regime_state.lambda_reg,
            'depth_l5': sum(market_data['bid_volumes'][:5] + 
                          market_data['ask_volumes'][:5]),
            'target_depth': 100000,
            'latency_ms': 15,
            'expected_edge_bps': edge_breakdown['net_edge']
        }
        
        # Optimize
        kelly_state = self.kelly_optimizer.optimize(
            signals,
            self.portfolio_state,
            conditions
        )
        
        return kelly_state
    
    def _calculate_position_size(self, kelly_fraction: float) -> float:
        """Calculate position size from Kelly fraction"""
        available_capital = self.portfolio_state['capital'] * \
                          (1 - self.portfolio_state['drawdown_pct'] / 100)
        
        position_size = kelly_fraction * available_capital
        
        # Round to lot size
        lot_size = 100
        position_size = np.floor(position_size / lot_size) * lot_size
        
        return position_size
    
    def _make_decision(self,
                       gates_result: Dict,
                       position_size: float,
                       edge_breakdown: Dict) -> Dict:
        """Make final trading decision"""
        if gates_result['passed'] and position_size > 0:
            return {
                'action': 'ENTER',
                'size': position_size,
                'execution_mode': self.config.default_execution_mode,
                'expected_edge': edge_breakdown['net_edge'],
                'reasons': ['all_gates_passed', 'positive_edge', 'adequate_size']
            }
        else:
            return {
                'action': 'SKIP',
                'size': 0,
                'execution_mode': 'none',
                'expected_edge': edge_breakdown['net_edge'],
                'reasons': gates_result['rejection_reasons']
            }
    
    async def _log_decision(self, *args) -> None:
        """Log decision with full XAI trace"""
        # Unpack arguments
        (market_data, features, score, raw_prob, calibrated_prob,
         confidence_interval, edge_breakdown, regime_state,
         kelly_state, gates_result, decision) = args
        
        # Create decision trace
        trace = DecisionTrace(
            decision_id=f"{market_data['symbol']}_{int(market_data['timestamp'])}",
            timestamp=market_data['timestamp'],
            symbol=market_data['symbol'],
            raw_features=features,
            normalized_features=features,  # Simplified
            feature_weights={'obi': 0.3, 'tfi': 0.2},  # Simplified
            raw_score=score,
            raw_probability=raw_prob,
            calibrated_probability=calibrated_prob,
            confidence_interval=confidence_interval,
            raw_edge_bps=edge_breakdown['raw_edge'],
            fees_bps=edge_breakdown['fees'],
            spread_cost_bps=edge_breakdown['spread_cost'],
            slippage_bps=edge_breakdown['slippage'],
            adverse_selection_bps=edge_breakdown['adverse_selection'],
            latency_cost_bps=edge_breakdown['latency_cost'],
            rebates_bps=edge_breakdown['rebates'],
            net_edge_bps=edge_breakdown['net_edge'],
            trade_cvar=50.0,  # Simplified
            portfolio_cvar=200.0,  # Simplified
            kelly_fraction=kelly_state.final_kelly[market_data['symbol']] if kelly_state else 0,
            position_size=decision['size'],
            gates_passed=gates_result['gates_passed'],
            gates_failed=gates_result['gates_failed'],
            rejection_reasons=gates_result['rejection_reasons'],
            market_regime=regime_state.current_regime,
            regime_confidence=0.8,  # Simplified
            lambda_cal=0.95,
            lambda_reg=regime_state.lambda_reg,
            lambda_liq=0.9,
            lambda_dd=0.85,
            lambda_lat=0.98,
            total_multiplier=kelly_state.multiplier if kelly_state else 0,
            final_decision=decision['action'],
            execution_mode=decision['execution_mode'],
            policy_id='default',
            policy_version='1.0.0',
            latency_ms=15.0
        )
        
        self.xai_logger.log_decision(trace)
    
    async def run(self) -> None:
        """Main run loop"""
        logger.info("Aurora Orchestrator starting...")
        
        while True:
            # aurora/orchestrator.py
import asyncio
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

from core.features.microstructure import MicrostructureFeatures
from core.features.cross_asset import CrossAssetFeatures
from core.calibration.probability import ProbabilityCalibrator
from core.calibration.conformal import InductiveConformalPredictor
from core.regime.regime_manager import RegimeManager, MarketRegime
from core.portfolio.kelly import DynamicKellyOptimizer
from core.risk.evt_cvar import EVTCVaR
from core.execution.fill_simulator import FillSimulator
from core.execution.tca import TransactionCostAnalyzer
from core.governance.policy_manager import PolicyManager
from core.observability.xai_logger import XAILogger, DecisionTrace

logger = logging.getLogger(__name__)

@dataclass
class AuroraConfig:
    """Aurora configuration"""
    # Feature parameters
    feature_window: int = 100
    robust_scaling: bool = True
    
    # Calibration
    calibration_method: str = 'isotonic'
    icp_significance: float = 0.05
    
    # Risk
    max_kelly: float = 0.25
    cvar_confidence: float = 0.95
    max_portfolio_cvar: float = 1000.0
    
    # Execution
    default_execution_mode: str = 'taker'
    max_latency_ms: float = 50.0
    
    # Gates
    min_edge_bps: float = 2.0
    max_spread_bps: float = 20.0
    max_volatility: float = 0.05
    
    # Universe
    symbols: List[str] = None
    leader_symbol: str = 'SOL-USDT'

class AuroraOrchestrator:
    """Main Aurora orchestrator –∑–≥—ñ–¥–Ω–æ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó R1"""
    
    def __init__(self, config: AuroraConfig):
        self.config = config
        
        # Initialize components
        self.feature_extractor = MicrostructureFeatures(
            robust_scale=config.robust_scaling
        )
        self.cross_asset = CrossAssetFeatures(
            window_size=config.feature_window
        )
        self.calibrator = ProbabilityCalibrator(
            method=config.calibration_method
        )
        self.icp = InductiveConformalPredictor(
            significance_level=config.icp_significance
        )
        self.regime_manager = RegimeManager()
        self.kelly_optimizer = DynamicKellyOptimizer(
            max_kelly=config.max_kelly
        )
        self.evt_cvar = EVTCVaR(
            confidence_level=config.cvar_confidence
        )
        self.fill_simulator = FillSimulator()
        self.tca = TransactionCostAnalyzer()
        self.policy_manager = PolicyManager()
        self.xai_logger = XAILogger()
        
        # State
        self.portfolio_state = {
            'positions': {},
            'capital': 100000,
            'drawdown_pct': 0,
            'peak_equity': 100000
        }
        self.market_cache = {}
        self.historical_pnl = []
        
    async def process_market_update(self, 
                                   market_data: Dict) -> Optional[Dict]:
        """Process market update and generate trading decision"""
        try:
            # 1. Extract features
            features = await self._extract_features(market_data)
            
            # 2. Calculate cross-asset dependencies
            cross_features = await self._calculate_cross_asset(market_data)
            features.update(cross_features)
            
            # 3. Generate signal score
            score = self._calculate_score(features)
            
            # 4. Convert to probability and calibrate
            raw_prob = self._score_to_probability(score)
            calibrated_prob, confidence_interval = await self._calibrate_probability(
                raw_prob
            )
            
            # 5. Check regime
            regime_state = self.regime_manager.update(market_data)
            
            # 6. Calculate expected edge with TCA
            edge_breakdown = await self._calculate_edge(
                market_data, 
                calibrated_prob
            )
            
            # 7. Check entry gates
            gates_result = self._check_gates(
                calibrated_prob,
                edge_breakdown,
                regime_state,
                market_data
            )
            
            # 8. Calculate position size with dynamic Kelly
            if gates_result['passed']:
                kelly_state = await self._calculate_kelly(
                    calibrated_prob,
                    edge_breakdown,
                    regime_state,
                    market_data
                )
                position_size = self._calculate_position_size(
                    kelly_state.final_kelly[market_data['symbol']]
                )
            else:
                kelly_state = None
                position_size = 0
                
            # 9. Make final decision
            decision = self._make_decision(
                gates_result,
                position_size,
                edge_breakdown
            )
            
            # 10. Log decision with XAI
            await self._log_decision(
                market_data,
                features,
                score,
                raw_prob,
                calibrated_prob,
                confidence_interval,
                edge_breakdown,
                regime_state,
                kelly_state,
                gates_result,
                decision
            )
            
            return decision
            
        except Exception as e:
            logger.error(f"Error processing market update: {e}")
            return None
    
    async def _extract_features(self, market_data: Dict) -> Dict:
        """Extract microstructure features"""
        features = {}
        
        # OBI
        features['obi'] = self.feature_extractor.calculate_obi(
            market_data['bid_volumes'],
            market_data['ask_volumes'],
            levels=5
        )
        
        # Microprice
        features['microprice'] = self.feature_extractor.calculate_microprice(
            market_data['bid_price'],
            market_data['ask_price'],
            market_data['bid_volumes'][0],
            market_data['ask_volumes'][0]
        )
        
        # TFI
        features['tfi'] = self.feature_extractor.calculate_tfi(
            market_data.get('trades', []),
            window=60  # 60 seconds
        )
        
        # Absorption
        features['absorption'] = self.feature_extractor.calculate_absorption(
            market_data.get('trades', []),
            market_data.get('price_changes', [])
        )
        
        return features
    
    async def _calculate_cross_asset(self, market_data: Dict) -> Dict:
        """Calculate cross-asset features"""
        if market_data['symbol'] == self.config.leader_symbol:
            return {}
            
        # Get leader data
        leader_data = self.market_cache.get(self.config.leader_symbol)
        if not leader_data:
            return {}
            
        # Calculate beta
        alt_returns = np.array(market_data.get('returns', []))
        sol_returns = np.array(leader_data.get('returns', []))
        
        beta = self.cross_asset.estimate_beta(alt_returns, sol_returns)
        
        # Find optimal lag
        lag, lag_score = self.cross_asset.find_optimal_lag(
            alt_returns, 
            sol_returns
        )
        
        # Calculate cross signal
        if lag > 0 and lag < len(sol_returns):
            cross_signal = beta * sol_returns[-lag]
        else:
            cross_signal = 0
            
        return {
            'beta_to_sol': beta,
            'optimal_lag': lag,
            'lag_score': lag_score,
            'cross_signal': cross_signal
        }
    
    def _calculate_score(self, features: Dict) -> float:
        """Calculate composite score from features"""
        # Simplified linear combination
        # In practice, use trained weights or ML model
        weights = {
            'obi': 0.3,
            'tfi': 0.2,
            'absorption': 0.1,
            'cross_signal': 0.4
        }
        
        score = sum(
            weights.get(name, 0) * value 
            for name, value in features.items()
        )
        
        return score
    
    def _score_to_probability(self, score: float) -> float:
        """Convert score to probability"""
        # Sigmoid transformation
        from scipy.special import expit
        return expit(score)
    
    async def _calibrate_probability(self, 
                                    raw_prob: float) -> Tuple[float, Tuple[float, float]]:
        """Calibrate probability and get confidence interval"""
        # Calibrate
        calibrated = self.calibrator.calibrate(raw_prob)
        
        # Get confidence interval from ICP
        _, confidence_interval = self.icp.predict_with_confidence(raw_prob)
        
        return calibrated, confidence_interval
    
    async def _calculate_edge(self, 
                             market_data: Dict,
                             probability: float) -> Dict:
        """Calculate edge breakdown with TCA"""
        # Expected payoff
        spread = market_data['ask_price'] - market_data['bid_price']
        expected_move = spread * 2  # Simplified
        
        # Raw edge
        raw_edge = (probability * expected_move - (1 - probability) * spread) * 10000
        
        # TCA components
        signal = {
            'expected_return_bps': raw_edge,
            'size': 1000,  # Default size
            'price': market_data['mid_price'],
            'age_ms': 10
        }
        
        tca_result = self.tca.analyze(
            signal,
            self.config.default_execution_mode,
            market_data
        )
        
        return {
            'raw_edge': tca_result.raw_edge,
            'fees': tca_result.fees,
            'spread_cost': tca_result.spread_cost,
            'slippage': tca_result.slippage,
            'adverse_selection': tca_result.adverse_selection,
            'latency_cost': tca_result.latency_cost,
            'rebates': tca_result.rebates,
            'net_edge': tca_result.net_edge
        }
    
    def _check_gates(self, 
                    probability: float,
                    edge_breakdown: Dict,
                    regime_state,
                    market_data: Dict) -> Dict:
        """Check entry gates"""
        gates_passed = []
        gates_failed = []
        rejection_reasons = []
        
        # Probability gate
        min_prob = 0.55
        if probability > min_prob:
            gates_passed.append('probability')
        else:
            gates_failed.append('probability')
            rejection_reasons.append(f'probability_{probability:.3f}_below_{min_prob}')
            
        # Edge gate
        if edge_breakdown['net_edge'] > self.config.min_edge_bps:
            gates_passed.append('edge')
        else:
            gates_failed.append('edge')
            rejection_reasons.append(f'edge_{edge_breakdown["net_edge"]:.2f}bps_below_min')
            
        # Regime gate
        if regime_state.current_regime in ['trend', 'grind']:
            gates_passed.append('regime')
        else:
            gates_failed.append('regime')
            rejection_reasons.append(f'regime_{regime_state.current_regime}_not_suitable')
            
        # Spread gate
        spread_bps = (market_data['ask_price'] - market_data['bid_price']) / \
                     market_data['mid_price'] * 10000
        if spread_bps <= self.config.max_spread_bps:
            gates_passed.append('spread')
        else:
            gates_failed.append('spread')
            rejection_reasons.append(f'spread_{spread_bps:.1f}bps_exceeds_max')
            
        # Volatility gate
        volatility = market_data.get('volatility', 0)
        if volatility <= self.config.max_volatility:
            gates_passed.append('volatility')
        else:
            gates_failed.append('volatility')
            rejection_reasons.append(f'volatility_{volatility:.3f}_exceeds_max')
            
        return {
            'passed': len(gates_failed) == 0,
            'gates_passed': gates_passed,
            'gates_failed': gates_failed,
            'rejection_reasons': rejection_reasons
        }
    
    async def _calculate_kelly(self,
                              probability: float,
                              edge_breakdown: Dict,
                              regime_state,
                              market_data: Dict) -> Any:
        """Calculate Kelly fraction with all multipliers"""
        # Prepare signals for Kelly optimizer
        signals = [{
            'symbol': market_data['symbol'],
            'probability': probability,
            'payoff_ratio': 2.0,  # Simplified
            'expected_return': edge_breakdown['net_edge'] / 10000,
            'volatility': market_data.get('volatility', 0.01)
        }]
        
        # Market conditions for multipliers
        conditions = {
            'ece': 0.03,  # From calibration monitoring
            'log_loss': 0.3,
            'lambda_reg': regime_state.lambda_reg,
            'depth_l5': sum(market_data['bid_volumes'][:5] + 
                          market_data['ask_volumes'][:5]),
            'target_depth': 100000,
            'latency_ms': 15,
            'expected_edge_bps': edge_breakdown['net_edge']
        }
        
        # Optimize
        kelly_state = self.kelly_optimizer.optimize(
            signals,
            self.portfolio_state,
            conditions
        )
        
        return kelly_state
    
    def _calculate_position_size(self, kelly_fraction: float) -> float:
        """Calculate position size from Kelly fraction"""
        available_capital = self.portfolio_state['capital'] * \
                          (1 - self.portfolio_state['drawdown_pct'] / 100)
        
        position_size = kelly_fraction * available_capital
        
        # Round to lot size
        lot_size = 100
        position_size = np.floor(position_size / lot_size) * lot_size
        
        return position_size
    
    def _make_decision(self,
                       gates_result: Dict,
                       position_size: float,
                       edge_breakdown: Dict) -> Dict:
        """Make final trading decision"""
        if gates_result['passed'] and position_size > 0:
            return {
                'action': 'ENTER',
                'size': position_size,
                'execution_mode': self.config.default_execution_mode,
                'expected_edge': edge_breakdown['net_edge'],
                'reasons': ['all_gates_passed', 'positive_edge', 'adequate_size']
            }
        else:
            return {
                'action': 'SKIP',
                'size': 0,
                'execution_mode': 'none',
                'expected_edge': edge_breakdown['net_edge'],
                'reasons': gates_result['rejection_reasons']
            }
    
    async def _log_decision(self, *args) -> None:
        """Log decision with full XAI trace"""
        # Unpack arguments
        (market_data, features, score, raw_prob, calibrated_prob,
         confidence_interval, edge_breakdown, regime_state,
         kelly_state, gates_result, decision) = args
        
        # Create decision trace
        trace = DecisionTrace(
            decision_id=f"{market_data['symbol']}_{int(market_data['timestamp'])}",
            timestamp=market_data['timestamp'],
            symbol=market_data['symbol'],
            raw_features=features,
            normalized_features=features,  # Simplified
            feature_weights={'obi': 0.3, 'tfi': 0.2},  # Simplified
            raw_score=score,
            raw_probability=raw_prob,
            calibrated_probability=calibrated_prob,
            confidence_interval=confidence_interval,
            raw_edge_bps=edge_breakdown['raw_edge'],
            fees_bps=edge_breakdown['fees'],
            spread_cost_bps=edge_breakdown['spread_cost'],
            slippage_bps=edge_breakdown['slippage'],
            adverse_selection_bps=edge_breakdown['adverse_selection'],
            latency_cost_bps=edge_breakdown['latency_cost'],
            rebates_bps=edge_breakdown['rebates'],
            net_edge_bps=edge_breakdown['net_edge'],
            trade_cvar=50.0,  # Simplified
            portfolio_cvar=200.0,  # Simplified
            kelly_fraction=kelly_state.final_kelly[market_data['symbol']] if kelly_state else 0,
            position_size=decision['size'],
            gates_passed=gates_result['gates_passed'],
            gates_failed=gates_result['gates_failed'],
            rejection_reasons=gates_result['rejection_reasons'],
            market_regime=regime_state.current_regime,
            regime_confidence=0.8,  # Simplified
            lambda_cal=0.95,
            lambda_reg=regime_state.lambda_reg,
            lambda_liq=0.9,
            lambda_dd=0.85,
            lambda_lat=0.98,
            total_multiplier=kelly_state.multiplier if kelly_state else 0,
            final_decision=decision['action'],
            execution_mode=decision['execution_mode'],
            policy_id='default',
            policy_version='1.0.0',
            latency_ms=15.0
        )
        
        self.xai_logger.log_decision(trace)
    
    async def run(self) -> None:
        """Main run loop"""
        logger.info("Aurora Orchestrator starting...")
        
        while True:
        –°–±–æ—Ä —Å–≤–µ–¥–µ–Ω–∏–π –æ —Ä–∞–±–æ—á–µ–π –æ–±–ª–∞—Å—Ç–∏–ü—Ä–æ–¥–æ–≤–∂—É—é –∑ –º—ñ—Å—Ü—è –≤–∏–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ –≤ –§–ê–ó–Ü 8. –ó–∞–≤–µ—Ä—à—É—é –ø–æ–≤–Ω—É –¥–æ—Ä–æ–∂–Ω—é –∫–∞—Ä—Ç—É —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó Aurora+ScalpBot:

## üìã –§–ê–ó–ò –†–ï–ê–õ–Ü–ó–ê–¶–Ü–á (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)

### **–§–ê–ó–ê 8: –Ü–ù–¢–ï–ì–†–ê–¶–Ü–Ø –¢–ê –ö–Ü–ù–¶–ï–í–ï –¢–ï–°–¢–£–í–ê–ù–ù–Ø** [5 –¥–Ω—ñ–≤] (–ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è)

#### 8.1 –ü–æ–≤–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è Aurora Orchestrator

````python
# aurora/orchestrator.py
import asyncio
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

from core.features.microstructure import MicrostructureFeatures
from core.features.cross_asset import CrossAssetFeatures
from core.calibration.probability import ProbabilityCalibrator
from core.calibration.conformal import InductiveConformalPredictor
from core.regime.regime_manager import RegimeManager, MarketRegime
from core.portfolio.kelly import DynamicKellyOptimizer
from core.risk.evt_cvar import EVTCVaR
from core.execution.fill_simulator import FillSimulator
from core.execution.tca import TransactionCostAnalyzer
from core.governance.policy_manager import PolicyManager
from core.observability.xai_logger import XAILogger, DecisionTrace

logger = logging.getLogger(__name__)

@dataclass
class AuroraConfig:
    """Aurora configuration"""
    leader_symbol: str = 'SOL-USDT'
    alt_symbols: List[str] = None
    max_kelly_fraction: float = 0.25
    cvar_confidence: float = 0.95
    regime_threshold: float = 5.0
    sprt_alpha: float = 0.05
    sprt_beta: float = 0.20
    edge_min_bps: float = 2.0

class AuroraOrchestrator:
    """Main Aurora orchestrator –∑–≥—ñ–¥–Ω–æ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó R1"""
    
    def __init__(self, config: AuroraConfig):
        self.config = config
        
        # Initialize all components
        self.microstructure = MicrostructureFeatures()
        self.cross_asset = CrossAssetFeatures()
        self.calibrator = ProbabilityCalibrator()
        self.icp = InductiveConformalPredictor()
        self.regime_manager = RegimeManager()
        self.kelly_optimizer = DynamicKellyOptimizer(config.max_kelly_fraction)
        self.evt_cvar = EVTCVaR(config.cvar_confidence)
        self.fill_simulator = FillSimulator()
        self.tca = TransactionCostAnalyzer()
        self.policy_manager = PolicyManager()
        self.xai_logger = XAILogger()
        
        self.portfolio_state = {
            'positions': {},
            'pnl_history': [],
            'current_drawdown': 0,
            'peak_value': 0
        }
        
    async def process_market_tick(self, market_data: Dict) -> Optional[Dict]:
        """Process single market tick and generate trading decision"""
        
        start_time = time.time()
        
        # Step 1: Extract features
        features = await self._extract_features(market_data)
        
        # Step 2: Calculate signal score
        signal = await self._calculate_signal(features, market_data)
        
        # Step 3: Check regime
        regime_state = self.regime_manager.update(market_data)
        if regime_state.current_regime == MarketRegime.CHAOS:
            return self._log_decision(None, "regime_blocked", features, signal)
        
        # Step 4: Calibrate probability
        calibrated_prob, confidence_interval = self._calibrate_probability(signal['score'])
        
        # Step 5: TCA and edge calculation
        tca_result = await self._calculate_edge(signal, market_data)
        
        # Step 6: Entry rules check
        if not self._check_entry_rules(calibrated_prob, tca_result, confidence_interval):
            return self._log_decision(None, "entry_rules_failed", features, signal)
        
        # Step 7: Calculate Kelly fraction
        kelly_state = self._calculate_kelly_fraction(
            signal, calibrated_prob, tca_result, regime_state, market_data
        )
        
        # Step 8: Risk checks
        if not self._check_risk_limits(kelly_state, tca_result):
            return self._log_decision(None, "risk_limits_exceeded", features, signal)
        
        # Step 9: Generate trading decision
        decision = self._generate_decision(
            signal, kelly_state, tca_result, market_data
        )
        
        # Step 10: XAI logging
        latency = (time.time() - start_time) * 1000
        return self._log_decision(decision, "approved", features, signal, 
                                 kelly_state, tca_result, latency)
    
    async def _extract_features(self, market_data: Dict) -> Dict:
        """Extract microstructure and cross-asset features"""
        
        features = {}
        
        # Microstructure features
        features['obi'] = self.microstructure.calculate_obi(
            market_data['bid_volumes'], 
            market_data['ask_volumes']
        )
        features['microprice'] = self.microstructure.calculate_microprice(
            market_data['bid'], market_data['ask'],
            market_data['bid_volumes'][0], market_data['ask_volumes'][0]
        )
        features['tfi'] = self.microstructure.calculate_tfi(
            market_data.get('trades', []), window=60
        )
        
        # Cross-asset features (if SOL data available)
        if 'sol_returns' in market_data:
            features['beta'] = self.cross_asset.estimate_beta(
                market_data['returns'], market_data['sol_returns']
            )
            features['lag'] = self.cross_asset.find_optimal_lag(
                market_data['returns'], market_data['sol_returns']
            )
        
        return features
    
    async def _calculate_signal(self, features: Dict, market_data: Dict) -> Dict:
        """Calculate signal score according to ¬ß6.1"""
        
        # Linear combination of features
        weights = {
            'obi': 0.3,
            'tfi': 0.25,
            'microprice_delta': 0.2,
            'absorption': 0.15,
            'cross_asset': 0.1
        }
        
        score = sum(weights.get(k, 0) * v for k, v in features.items())
        
        # Add cross-asset component if available
        if 'beta' in features and 'sol_signal' in market_data:
            gamma = 0.2  # Cross-asset weight
            lag = features.get('lag', 0)
            cross_component = gamma * features['beta'] * market_data['sol_signal']
            score += cross_component
        
        return {
            'score': score,
            'symbol': market_data['symbol'],
            'timestamp': market_data['timestamp'],
            'features': features
        }
    
    def _calibrate_probability(self, score: float) -> Tuple[float, Tuple[float, float]]:
        """Calibrate probability with ICP confidence intervals"""
        
        # Transform score to probability
        raw_prob = 1 / (1 + np.exp(-score))  # Sigmoid
        
        # Apply calibration
        calibrated_prob = self.calibrator.calibrate(raw_prob)
        
        # Get confidence interval
        _, confidence_interval = self.icp.predict_with_confidence(score)
        
        return calibrated_prob, confidence_interval
    
    async def _calculate_edge(self, signal: Dict, market_data: Dict) -> Dict:
        """Calculate expected edge after TCA"""
        
        # Simulate fills for both maker and taker
        maker_sim = self.fill_simulator.simulate_maker_fill(
            order_price=market_data['bid'],
            market_features=market_data
        )
        
        taker_sim = self.fill_simulator.simulate_taker_fill(
            size=1.0,
            market_snapshot=market_data
        )
        
        # TCA for both modes
        tca_maker = self.tca.analyze(signal, 'maker', market_data)
        tca_taker = self.tca.analyze(signal, 'taker', market_data)
        
        # Choose best execution mode
        if tca_maker.net_edge > tca_taker.net_edge:
            return {'mode': 'maker', 'tca': tca_maker, 'simulation': maker_sim}
        else:
            return {'mode': 'taker', 'tca': tca_taker, 'simulation': taker_sim}
    
    def _check_entry_rules(self, prob: float, tca_result: Dict, 
                          confidence_interval: Tuple[float, float]) -> bool:
        """Check all entry rules according to ¬ß10"""
        
        # Rule 1: Probability threshold
        p_star = self._calculate_p_star(tca_result['tca'])
        if prob <= p_star + 0.01:  # Safety margin
            return False
        
        # Rule 2: Confidence interval check
        if confidence_interval[1] - confidence_interval[0] > 0.3:  # Too uncertain
            return False
        
        # Rule 3: Net edge positive after TCA
        if tca_result['tca'].net_edge <= self.config.edge_min_bps:
            return False
        
        # Rule 4: Latency check
        max_latency = self.tca.maximum_tolerable_latency(
            tca_result['tca'].raw_edge
        )
        if tca_result['simulation'].expected_fill_time * 1000 > max_latency:
            return False
        
        return True
    
    def _calculate_p_star(self, tca: TCAResult) -> float:
        """Calculate minimum probability threshold"""
        # p* = (1 + c') / (1 + r) where c' = c/L, r = G/L
        r = 2.0  # Assumed payoff ratio
        c_prime = (tca.fees + tca.slippage + tca.adverse_selection) / 100
        return (1 + c_prime) / (1 + r)
    
    def _calculate_kelly_fraction(self, signal: Dict, prob: float, 
                                  tca_result: Dict, regime_state: RegimeState,
                                  market_data: Dict) -> KellyState:
        """Calculate dynamic Kelly fraction with multipliers"""
        
        # Raw Kelly
        b = 2.0  # Payoff ratio (placeholder)
        raw_kelly = self.kelly_optimizer.calculate_raw_kelly(prob, b)
        
        # Portfolio Kelly (if multiple positions)
        if len(self.portfolio_state['positions']) > 0:
            portfolio_kelly = self.kelly_optimizer.calculate_portfolio_kelly(
                expected_returns=np.array([tca_result['tca'].net_edge]),
                covariance=np.array([[100]])  # Placeholder
            )
        else:
            portfolio_kelly = {signal['symbol']: raw_kelly}
        
        # Calculate multipliers
        lambda_cal = self.kelly_optimizer._calculate_lambda_cal(
            {'ece': 0.03, 'log_loss': 0.15}  # From calibration metrics
        )
        lambda_reg = regime_state.lambda_reg
        lambda_liq = self.kelly_optimizer._calculate_lambda_liq(market_data)
        lambda_dd = self.kelly_optimizer._calculate_lambda_dd(self.portfolio_state)
        lambda_lat = self.kelly_optimizer._calculate_lambda_lat(
            {'latency': 50, 'expected_edge': tca_result['tca'].net_edge}
        )
        
        # Final multiplier
        M = self.kelly_optimizer.calculate_aurora_multiplier(
            lambda_cal, lambda_reg, lambda_liq, lambda_dd, lambda_lat
        )
        
        # Apply multiplier
        final_kelly = {}
        for symbol, fraction in portfolio_kelly.items():
            final_kelly[symbol] = min(fraction * M, self.config.max_kelly_fraction)
        
        return KellyState(
            raw_kelly={signal['symbol']: raw_kelly},
            portfolio_kelly=portfolio_kelly,
            multiplier=M,
            final_kelly=final_kelly,
            risk_metrics={'cvar': 0}  # Placeholder
        )
    
    def _check_risk_limits(self, kelly_state: KellyState, 
                          tca_result: Dict) -> bool:
        """Check risk limits including CVaR"""
        
        # Calculate position CVaR
        if len(self.portfolio_state['pnl_history']) > 100:
            losses = [-p for p in self.portfolio_state['pnl_history'] if p < 0]
            if losses:
                cvar, _ = self.evt_cvar.calculate_cvar(np.array(losses))
                if cvar > 100:  # Max CVaR in bps
                    return False
        
        # Check daily drawdown
        if self.portfolio_state['current_drawdown'] > 0.1:  # 10% max DD
            return False
        
        # Check inventory limits
        total_exposure = sum(kelly_state.final_kelly.values())
        if total_exposure > 1.0:
            return False
        
        return True
    
    def _generate_decision(self, signal: Dict, kelly_state: KellyState,
                          tca_result: Dict, market_data: Dict) -> Dict:
        """Generate final trading decision"""
        
        # Calculate position size
        capital = 10000  # Placeholder
        risk_per_trade = capital * kelly_state.final_kelly.get(signal['symbol'], 0)
        
        # Calculate stop loss
        atr = market_data.get('atr', market_data['bid'] * 0.002)
        stop_distance = 2 * atr
        
        # Position size based on risk
        position_size = risk_per_trade / stop_distance
        
        return {
            'action': 'BUY' if signal['score'] > 0 else 'SELL',
            'symbol': signal['symbol'],
            'size': position_size,
            'entry_price': market_data['bid'] if tca_result['mode'] == 'maker' else market_data['ask'],
            'stop_loss': market_data['bid'] - stop_distance if signal['score'] > 0 else market_data['ask'] + stop_distance,
            'take_profit': market_data['bid'] + 2 * stop_distance if signal['score'] > 0 else market_data['ask'] - 2 * stop_distance,
            'execution_mode': tca_result['mode'],
            'kelly_fraction': kelly_state.final_kelly.get(signal['symbol'], 0),
            'expected_edge_bps': tca_result['tca'].net_edge
        }
    
    def _log_decision(self, decision: Optional[Dict], status: str,
                     features: Dict, signal: Dict, 
                     kelly_state: Optional[KellyState] = None,
                     tca_result: Optional[Dict] = None,
                     latency: float = 0) -> Dict:
        """Log decision with full XAI trace"""
        
        trace = DecisionTrace(
            timestamp=datetime.now(),
            symbol=signal['symbol'],
            features=features,
            score=signal['score'],
            raw_probability=0,
            calibrated_probability=0,
            confidence_interval=(0, 0),
            edge_breakdown=tca_result['tca'].__dict__ if tca_result else {},
            gates={'status': status},
            regime='',
            kelly_state=kelly_state.__dict__ if kelly_state else {},
            decision=decision,
            latency_ms=latency
        )
        
        self.xai_logger.log_decision(trace)
        
        return {
            'decision': decision,
            'status': status,
            'trace_id': trace.timestamp.timestamp()
        }
````

#### 8.2 Scalp Bot Execution Layer

````python
# scalp_bot/execution/executor.py
import asyncio
from typing import Dict, Optional, List
from enum import Enum
import ccxt.async_support as ccxt
import logging

logger = logging.getLogger(__name__)

class OrderStatus(Enum):
    PENDING = "pending"
    SUBMITTED = "submitted"
    PARTIAL = "partial"
    FILLED = "filled"
    CANCELLED = "cancelled"
    FAILED = "failed"

class ScalpBotExecutor:
    """Execution layer for ScalpBot"""
    
    def __init__(self, exchange_config: Dict):
        self.exchange = self._init_exchange(exchange_config)
        self.active_orders = {}
        self.position_tracker = {}
        
    def _init_exchange(self, config: Dict) -> ccxt.Exchange:
        """Initialize exchange connection"""
        exchange_class = getattr(ccxt, config['name'])
        exchange = exchange_class({
            'apiKey': config['api_key'],
            'secret': config['secret'],
            'enableRateLimit': True,
            'options': {
                'defaultType': 'future',
                'adjustForTimeDifference': True
            }
        })
        return exchange
    
    async def execute_decision(self, decision: Dict) -> Dict:
        """Execute trading decision"""
        
        try:
            # Create order based on execution mode
            if decision['execution_mode'] == 'maker':
                order = await self._place_maker_order(decision)
            else:
                order = await self._place_taker_order(decision)
            
            # Track order
            self.active_orders[order['id']] = {
                'order': order,
                'decision': decision,
                'status': OrderStatus.SUBMITTED
            }
            
            # Set stop loss and take profit
            await self._set_exit_orders(order, decision)
            
            return {
                'success': True,
                'order_id': order['id'],
                'execution_price': order['price'],
                'status': 'executed'
            }
            
        except Exception as e:
            logger.error(f"Execution failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'status': 'failed'
            }
    
    async def _place_maker_order(self, decision: Dict) -> Dict:
        """Place limit order as maker"""
        
        # Place order slightly better than best bid/ask
        ticker = await self.exchange.fetch_ticker(decision['symbol'])
        
        if decision['action'] == 'BUY':
            price = ticker['bid'] * 1.0001  # Slightly above bid
        else:
            price = ticker['ask'] * 0.9999  # Slightly below ask
        
        order = await self.exchange.create_limit_order(
            symbol=decision['symbol'],
            side=decision['action'].lower(),
            amount=decision['size'],
            price=price,
            params={'postOnly': True}  # Ensure maker
        )
        
        return order
    
    async def _place_taker_order(self, decision: Dict) -> Dict:
        """Place market order as taker"""
        
        order = await self.exchange.create_market_order(
            symbol=decision['symbol'],
            side=decision['action'].lower(),
            amount=decision['size']
        )
        
        return order
    
    async def _set_exit_orders(self, entry_order: Dict, decision: Dict) -> None:
        """Set stop loss and take profit orders"""
        
        # Stop loss
        await self.exchange.create_order(
            symbol=decision['symbol'],
            type='stop_market',
            side='sell' if decision['action'] == 'BUY' else 'buy',
            amount=decision['size'],
            stopPrice=decision['stop_loss'],
            params={'reduceOnly': True}
        )
        
        # Take profit
        await self.exchange.create_order(
            symbol=decision['symbol'],
            type='take_profit_market',
            side='sell' if decision['action'] == 'BUY' else 'buy',
            amount=decision['size'],
            stopPrice=decision['take_profit'],
            params={'reduceOnly': True}
        )
    
    async def monitor_orders(self) -> None:
        """Monitor active orders and update status"""
        
        while True:
            for order_id, order_info in list(self.active_orders.items()):
                try:
                    # Fetch current order status
                    order = await self.exchange.fetch_order(
                        order_id, 
                        symbol=order_info['decision']['symbol']
                    )
                    
                    # Update status
                    if order['status'] == 'closed':
                        order_info['status'] = OrderStatus.FILLED
                        await self._handle_filled_order(order, order_info)
                        del self.active_orders[order_id]
                    elif order['status'] == 'canceled':
                        order_info['status'] = OrderStatus.CANCELLED
                        del self.active_orders[order_id]
                    elif order['filled'] > 0 and order['filled'] < order['amount']:
                        order_info['status'] = OrderStatus.PARTIAL
                        
                except Exception as e:
                    logger.error(f"Error monitoring order {order_id}: {e}")
            
            await asyncio.sleep(1)
    
    async def _handle_filled_order(self, order: Dict, order_info: Dict) -> None:
        """Handle filled order"""
        
        # Update position tracker
        symbol = order_info['decision']['symbol']
        
        if symbol not in self.position_tracker:
            self.position_tracker[symbol] = {
                'size': 0,
                'entry_price': 0,
                'realized_pnl': 0
            }
        
        position = self.position_tracker[symbol]
        
        if order['side'] == 'buy':
            # Update average entry price
            total_cost = position['size'] * position['entry_price'] + order['filled'] * order['price']
            position['size'] += order['filled']
            position['entry_price'] = total_cost / position['size'] if position['size'] > 0 else 0
        else:
            # Calculate PnL
            pnl = (order['price'] - position['entry_price']) * order['filled']
            position['realized_pnl'] += pnl
            position['size'] -= order['filled']
            
            if position['size'] == 0:
                position['entry_price'] = 0
        
        logger.info(f"Order filled: {order_id}, PnL: {position['realized_pnl']}")
````

#### 8.3 –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ–π–Ω—ñ —Ç–µ—Å—Ç–∏

````python
# tests/integration/test_aurora_integration.py
import pytest
import asyncio
import numpy as np
from unittest.mock import Mock, AsyncMock, patch
from aurora.orchestrator import AuroraOrchestrator, AuroraConfig
from scalp_bot.execution.executor import ScalpBotExecutor

@pytest.fixture
def aurora_config():
    return AuroraConfig(
        leader_symbol='SOL-USDT',
        alt_symbols=['SOON-USDT', 'JUP-USDT'],
        max_kelly_fraction=0.25,
        cvar_confidence=0.95,
        edge_min_bps=2.0
    )

@pytest.fixture
def mock_market_data():
    return {
        'symbol': 'SOON-USDT',
        'timestamp': 1234567890,
        'bid': 100.0,
        'ask': 100.1,
        'bid_volumes': [1000, 900, 800, 700, 600],
        'ask_volumes': [1100, 950, 850, 750, 650],
        'trades': [
            {'side': 'buy', 'size': 100, 'price': 100.05},
            {'side': 'sell', 'size': 50, 'price': 100.02}
        ],
        'returns': np.random.normal(0, 0.01, 100),
        'sol_returns': np.random.normal(0, 0.015, 100),
        'atr': 0.5,
        'spread_bps': 10
    }

@pytest.mark.asyncio
async def test_aurora_full_pipeline(aurora_config, mock_market_data):
    """Test full Aurora pipeline from tick to decision"""
    
    orchestrator = AuroraOrchestrator(aurora_config)
    
    # Mock calibration data
    np.random.seed(42)
    calibration_scores = np.random.rand(1000)
    calibration_targets = (calibration_scores > 0.5).astype(int)
    orchestrator.calibrator.fit(calibration_scores, calibration_targets)
    orchestrator.icp.calibrate(calibration_scores, calibration_targets)
    
    # Process market tick
    result = await orchestrator.process_market_tick(mock_market_data)
    
    assert result is not None
    assert 'decision' in result
    assert 'status' in result
    assert 'trace_id' in result
    
    if result['status'] == 'approved':
        decision = result['decision']
        assert decision['symbol'] == 'SOON-USDT'
        assert decision['action'] in ['BUY', 'SELL']
        assert decision['size'] > 0
        assert decision['kelly_fraction'] <= aurora_config.max_kelly_fraction
        assert decision['expected_edge_bps'] >= aurora_config.edge_min_bps

@pytest.mark.asyncio
async def test_executor_maker_order():
    """Test maker order execution"""
    
    with patch('ccxt.async_support.binance') as mock_exchange:
        mock_exchange_instance = AsyncMock()
        mock_exchange.return_value = mock_exchange_instance
        
        # Mock exchange responses
        mock_exchange_instance.fetch_ticker.return_value = {
            'bid': 100.0,
            'ask': 100.1
        }
        
        mock_exchange_instance.create_limit_order.return_value = {
            'id': '12345',
            'symbol': 'SOON-USDT',
            'side': 'buy',
            'price': 100.01,
            'amount': 10,
            'status': 'open'
        }
        
        executor = ScalpBotExecutor({
            'name': 'binance',
            'api_key': 'test',
            'secret': 'test'
        })
        executor.exchange = mock_exchange_instance
        
        decision = {
            'action': 'BUY',
            'symbol': 'SOON-USDT',
            'size': 10,
            'execution_mode': 'maker',
            'stop_loss': 99.5,
            'take_profit': 101.0
        }
        
        result = await executor.execute_decision(decision)
        
        assert result['success'] == True
        assert result['order_id'] == '12345'
        assert 'execution_price' in result
        
        # Verify maker order was placed with postOnly
        mock_exchange_instance.create_limit_order.assert_called_once()
        call_args = mock_exchange_instance.create_limit_order.call_args
        assert call_args[1]['params']['postOnly'] == True

@pytest.mark.asyncio
async def test_regime_detection_blocks_chaos():
    """Test that chaos regime blocks trading"""
    
    config = AuroraConfig()
    orchestrator = AuroraOrchestrator(config)
    
    # Force chaos regime
    orchestrator.regime_manager.current_regime = MarketRegime.CHAOS
    orchestrator.regime_manager.update = Mock(return_value=RegimeState(
        current_regime=MarketRegime.CHAOS,
        confidence=0.9,
        transition_probability=0.1,
        lambda_reg=0.0
    ))
    
    market_data = {
        'symbol': 'SOON-USDT',
        'timestamp': 1234567890,
        'bid': 100.0,
        'ask': 100.1,
        'bid_volumes': [100] * 5,
        'ask_volumes': [100] * 5
    }
    
    result = await orchestrator.process_market_tick(market_data)
    
    assert result['status'] == 'regime_blocked'
    assert result['decision'] is None

@pytest.mark.asyncio
async def test_risk_limits_enforcement():
    """Test that risk limits are enforced"""
    
    config = AuroraConfig()
    orchestrator = AuroraOrchestrator(config)
    
    # Set high drawdown
    orchestrator.portfolio_state['current_drawdown'] = 0.15  # 15% > 10% limit
    
    # Mock successful signal generation
    orchestrator._extract_features = AsyncMock(return_value={'obi': 0.1, 'tfi': 0.2})
    orchestrator._calculate_signal = AsyncMock(return_value={
        'score': 1.0,
        'symbol': 'SOON-USDT',
        'timestamp': 1234567890,
        'features': {}
    })
    
    market_data = {'symbol': 'SOON-USDT', 'bid': 100, 'ask': 100.1}
    
    result = await orchestrator.process_market_tick(market_data)
    
    assert result['status'] == 'risk_limits_exceeded'
    assert result['decision'] is None
````

---

### **–§–ê–ó–ê 9: DEPLOYMENT –¢–ê –ú–û–ù–Ü–¢–û–†–ò–ù–ì** [3 –¥–Ω—ñ]

#### 9.1 Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü—ñ—è

````dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create necessary directories
RUN mkdir -p logs artifacts reports

# Run application
CMD ["python", "-m", "aurora.main"]
````

#### 9.2 Docker Compose –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Å—Ç–µ–∫—É

````yaml
# docker-compose.yml
version: '3.8'

services:
  aurora:
    build: .
    container_name: aurora-orchestrator
    environment:
      - AURORA_MODE=production
      - BINANCE_API_KEY=${BINANCE_API_KEY}
      - BINANCE_API_SECRET=${BINANCE_API_SECRET}
    volumes:
      - ./configs:/app/configs
      - ./logs:/app/logs
      - ./artifacts:/app/artifacts
    networks:
      - aurora-network
    restart: unless-stopped

  scalpbot:
    build: .
    container_name: scalpbot-executor
    command: python -m scalp_bot.main
    environment:
      - EXECUTION_MODE=live
      - AURORA_API_URL=http://aurora:8000
    depends_on:
      - aurora
    networks:
      - aurora-network
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    container_name: aurora-prometheus
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - aurora-network

  grafana:
    image: grafana/grafana:latest
    container_name: aurora-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./observability/dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3000:3000"
    networks:
      - aurora-network

networks:
  aurora-network:
    driver: bridge

volumes:
  prometheus-data:
  grafana-data:
````

#### 9.3 Monitoring —Ç–∞ Alerting

````python
# observability/monitoring.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# Metrics
trades_total = Counter('aurora_trades_total', 'Total number of trades', ['symbol', 'side'])
trades_profitable = Counter('aurora_trades_profitable', 'Number of profitable trades')
edge_histogram = Histogram('aurora_edge_bps', 'Edge distribution in bps')
kelly_fraction = Gauge('aurora_kelly_fraction', 'Current Kelly fraction', ['symbol'])
drawdown = Gauge('aurora_drawdown_pct', 'Current drawdown percentage')
regime = Gauge('aurora_regime', 'Current market regime', ['regime'])

class MetricsCollector:
    """Collect and expose metrics for Prometheus"""
    
    def __init__(self, port: int = 8001):
        start_http_server(port)
        
    def record_trade(self, trade: Dict):
        """Record trade metrics"""
        trades_total.labels(
            symbol=trade['symbol'],
            side=trade['side']
        ).inc()
        
        if trade['pnl'] > 0:
            trades_profitable.inc()
        
        edge_histogram.observe(trade['edge_bps'])
    
    def update_portfolio_metrics(self, portfolio_state: Dict):
        """Update portfolio-level metrics"""
        drawdown.set(portfolio_state['current_drawdown'] * 100)
        
        for symbol, fraction in portfolio_state['kelly_fractions'].items():
            kelly_fraction.labels(symbol=symbol).set(fraction)
    
    def update_regime(self, regime_state: str):
        """Update regime metric"""
        regime_labels = {'trend': 1, 'grind': 2, 'chaos': 3}
        regime.labels(regime=regime_state).set(regime_labels.get(regime_state, 0))
````

---

## üìä –ö–õ–Æ–ß–û–í–Ü –ú–ï–¢–†–ò–ö–ò –£–°–ü–Ü–•–£

### –¢–µ—Ö–Ω—ñ—á–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏:
- ‚úÖ **Test Coverage ‚â• 90%**
- ‚úÖ **Latency tick‚Üídecision < 100ms**
- ‚úÖ **ECE ‚â§ 0.05**
- ‚úÖ **Brier Score ‚â§ 0.17**
- ‚úÖ **CVaR95 estimation CI width < 20%**

### –ë—ñ–∑–Ω–µ—Å –º–µ—Ç—Ä–∏–∫–∏:
- ‚úÖ **Sharpe Ratio (monthly) ‚â• 2.0**
- ‚úÖ **Win Rate > 55%**
- ‚úÖ **Average Edge after TCA > 2 bps**
- ‚úÖ **Max Drawdown < 10%**
- ‚úÖ **Uptime > 99.9%**

---

## üö¶ ROADMAP TIMELINE

| –§–∞–∑–∞ | –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å | –°—Ç–∞—Ç—É—Å | Deliverables |
|------|------------|--------|--------------|
| **–§–∞–∑–∞ 0: –Ü–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞** | 2 –¥–Ω—ñ | üü° In Progress | Project structure, base types |
| **–§–∞–∑–∞ 1: –ú—ñ–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ —Ñ—ñ—á—ñ** | 5 –¥–Ω—ñ–≤ | ‚è≥ Planned | OBI, TFI, Cross-asset features |
| **–§–∞–∑–∞ 2: –ö–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è** | 4 –¥–Ω—ñ | ‚è≥ Planned | Calibration, ICP confidence |
| **–§–∞–∑–∞ 3: –î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º—ñ–≤** | 3 –¥–Ω—ñ | ‚è≥ Planned | Page-Hinkley, GLR, Regime manager |
| **–§–∞–∑–∞ 4: –î–∏–Ω–∞–º—ñ—á–Ω–∏–π Kelly** | 5 –¥–Ω—ñ–≤ | ‚è≥ Planned | EVT-CVaR, Kelly optimizer |
| **–§–∞–∑–∞ 5: TCA & Fill Sim** | 4 –¥–Ω—ñ | ‚è≥ Planned | Fill simulator, TCA analyzer |
| **–§–∞–∑–∞ 6: SPRT/GLR** | 3 –¥–Ω—ñ | ‚è≥ Planned | Policy governance |
| **–§–∞–∑–∞ 7: XAI Logging** | 3 –¥–Ω—ñ | ‚è≥ Planned | Full observability |
| **–§–∞–∑–∞ 8: –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è** | 5 –¥–Ω—ñ–≤ | ‚è≥ Planned | Full system integration |
| **–§–∞–∑–∞ 9: Deployment** | 3 –¥–Ω—ñ | ‚è≥ Planned | Docker, monitoring |

**Total: ~37 –¥–Ω—ñ–≤**

---

## üéØ –ù–ê–°–¢–£–ü–ù–Ü –ö–†–û–ö–ò

1. **–ù–µ–≥–∞–π–Ω–æ:**
   - –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç—É –∑–≥—ñ–¥–Ω–æ –§–ê–ó–ò 0
   - –Ü–º–ø–ª–µ–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –±–∞–∑–æ–≤—ñ —Ç–∏–ø–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏
   - –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ —Ç–µ—Å—Ç–æ–≤–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ

2. **–¶–µ–π —Ç–∏–∂–¥–µ–Ω—å:**
   - –ó–∞–≤–µ—Ä—à–∏—Ç–∏ –§–ê–ó–£ 1 (–º—ñ–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ —Ñ—ñ—á—ñ)
   - –ü–æ—á–∞—Ç–∏ –§–ê–ó–£ 2 (–∫–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è)

3. **–ù–∞—Å—Ç—É–ø–Ω–∏–π —Ç–∏–∂–¥–µ–Ω—å:**
   - –ó–∞–≤–µ—Ä—à–∏—Ç–∏ –§–ê–ó–ò 2-3
   - –ü–æ—á–∞—Ç–∏ –§–ê–ó–£ 4 (Kelly + EVT)

---

## üìù –ó–ê–ö–õ–Æ–ß–ù–Ü –ù–û–¢–ê–¢–ö–ò

–¶—è –¥–æ—Ä–æ–∂–Ω—è –∫–∞—Ä—Ç–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î:

1. **–ü–æ–≤–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó R1** - –≤—Å—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ —Ç–∞ —Ñ–æ—Ä–º—É–ª–∏
2. **Test-Driven Development** - —Ç–µ—Å—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
3. **–ü–æ–µ—Ç–∞–ø–Ω—É —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—é** - –≤—ñ–¥ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–æ —Å–∫–ª–∞–¥–Ω–æ–≥–æ
4. **Production-ready –∫–æ–¥** - –∑ Docker, –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥–æ–º, –ª–æ–≥—É–≤–∞–Ω–Ω—è–º
5. **XAI –ø–æ–≤–Ω–æ—Ç—É** - –∫–æ–∂–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è —Ç—Ä–∞—Å—É—î—Ç—å—Å—è —Ç–∞ –ø–æ—è—Å–Ω—é—î—Ç—å—Å—è

–°–∏—Å—Ç–µ–º–∞ Aurora+ScalpBot –ø—ñ—Å–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –±—É–¥–µ –Ω–∞–π—Å—É—á–∞—Å–Ω—ñ—à–æ—é –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ—é –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–æ–≥–æ —Å–∫–∞–ª—å–ø—ñ–Ω–≥—É –∑ –ø–æ–≤–Ω–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∏–∑–∏–∫—ñ–≤ —Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ—é –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é –∫–æ–∂–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è.    
