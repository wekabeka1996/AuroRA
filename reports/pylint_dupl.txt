************* Module Command line or configuration file
Command line or configuration file:1:0: E0013: Plugin 'pylint.extensions.dupl_code' is impossible to load, is it installed ? ('No module named 'pylint.extensions.dupl_code'') (bad-plugin-value)
************* Module archive/kelly_py
archive/kelly_py:1:0: F0001: No module named archive/kelly_py (fatal)
************* Module (1).py
(1).py:1:0: F0001: No module named (1).py (fatal)
************* Module repo_core_signal_fdr
archive\repo_core_signal_fdr.py:190:5: E0001: Parsing failed: 'unterminated string literal (detected at line 190) (repo_core_signal_fdr, line 190)' (syntax-error)
************* Module repo_core_universe_ranking
archive\repo_core_universe_ranking.py:184:47: E0001: Parsing failed: 'unterminated string literal (detected at line 184) (repo_core_universe_ranking, line 184)' (syntax-error)
************* Module tools.validate_canary_logs
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.signal.leadlag_hy:[63:313]
==features_cross_asset:[46:296]
@dataclass
class _PricePoint:
    t: float
    logp: float


class CrossAssetHY:
    """Streaming buffers + on-demand HY estimators.

    Parameters
    ----------
    window_s : float
        Rolling horizon for retaining ticks (seconds).
    max_points : int
        Hard cap per symbol to bound memory; oldest points are evicted first.
    """

    def __init__(self, window_s: float = 60.0, max_points: int = 8000) -> None:
        self.window_s = float(window_s)
        self.max_points = int(max_points)
        self._buf: Dict[str, Deque[_PricePoint]] = {}

    # --------------------------- Ingestion ---------------------------
    def add_tick(self, symbol: str, ts: float, price: float) -> None:
        if price <= 0:
            return
        dq = self._buf.setdefault(symbol, deque())
        logp = math.log(price)
        dq.append(_PricePoint(t=float(ts), logp=logp))
        # evict by count
        while len(dq) > self.max_points:
            dq.popleft()
        # evict by time
        self._evict_old(symbol, ts)

    def add_snapshot_mid(self, symbol: str, snap: MarketSnapshot) -> None:
        # convenience: use mid as price proxy
        mid = 0.5 * (float(snap.bid_price) + float(snap.ask_price))
        self.add_tick(symbol, float(snap.timestamp), mid)

    def _evict_old(self, symbol: str, now_ts: float) -> None:
        dq = self._buf.get(symbol)
        if not dq:
            return
        cutoff = float(now_ts) - self.window_s
        while dq and dq[0].t < cutoff:
            dq.popleft()

    # ------------------------ HY helpers ------------------------
    @staticmethod
    def _returns(points: Sequence[_PricePoint]) -> List[Tuple[float, float, float]]:
        """Build log-returns and their intervals: [(t0, t1, r), ...]."""
        out: List[Tuple[float, float, float]] = []
        if len(points) < 2:
            return out
        t_prev = points[0].t
        p_prev = points[0].logp
        for i in range(1, len(points)):
            t = points[i].t
            lp = points[i].logp
            if t > t_prev and lp == lp and p_prev == p_prev:  # basic NaN guard
                out.append((t_prev, t, lp - p_prev))
            t_prev = t
            p_prev = lp
        return out

    @staticmethod
    def _hy_cov_from_returns(
        rx: Sequence[Tuple[float, float, float]],
        ry: Sequence[Tuple[float, float, float]],
    ) -> Tuple[float, float, float]:
        """Compute HY covariance and realized variances from return lists.

        rx: list of (a_i, b_i, r_i) for X; ry: (c_j, d_j, s_j) for Y.
        HY covariance = sum_{i,j} r_i * s_j * 1{ (a_i,b_i] overlaps (c_j,d_j] }.
        Returns (cov_xy, var_x, var_y).
        """
        cov = 0.0
        varx = 0.0
        vary = 0.0
        for _, _, r in rx:
            varx += r * r
        for _, _, s in ry:
            vary += s * s
        # two-pointer sweep for overlap detection (both lists are ordered by time)
        i = 0
        j = 0
        while i < len(rx) and j < len(ry):
            a0, a1, ri = rx[i]
            b0, b1, sj = ry[j]
            # overlap if min(a1, b1) > max(a0, b0)
            if min(a1, b1) > max(a0, b0):
                cov += ri * sj
            # advance the earlier-finishing interval
            if a1 <= b1:
                i += 1
            else:
                j += 1
        return cov, varx, vary

    def _prepare_returns(self, sym: str, now_ts: Optional[float]) -> List[Tuple[float, float, float]]:
        dq = self._buf.get(sym, deque())
        if not dq:
            return []
        if now_ts is None:
            now_ts = dq[-1].t
        # ensure eviction up to now_ts and slice window
        self._evict_old(sym, now_ts)
        # copy points inside window
        cutoff = float(now_ts) - self.window_s
        pts = [pt for pt in dq if pt.t >= cutoff]
        return self._returns(pts)

    def _shift_returns(self, r: Sequence[Tuple[float, float, float]], lag: float) -> List[Tuple[float, float, float]]:
        if abs(lag) < 1e-15:
            return list(r)
        return [(a + lag, b + lag, v) for (a, b, v) in r]

    # ------------------------- Public API -------------------------
    def hy_metrics(
        self,
        sym_x: str,
        sym_y: str,
        *,
        now_ts: Optional[float] = None,
        lag_s: float = 0.0,
    ) -> Dict[str, float]:
        """HY covariance/correlation and betas for (X, Y) over the rolling window.

        Positive lag means Y is shifted forward by τ: we estimate Corr(X_t, Y_{t+τ}).
        """
        rx = self._prepare_returns(sym_x, now_ts)
        ry0 = self._prepare_returns(sym_y, now_ts)
        ry = self._shift_returns(ry0, lag_s)
        cov, varx, vary = self._hy_cov_from_returns(rx, ry)
        corr = 0.0 if varx <= 0 or vary <= 0 else cov / math.sqrt(varx * vary)
        beta_x_on_y = 0.0 if vary <= 0 else cov / vary
        beta_y_on_x = 0.0 if varx <= 0 else cov / varx
        return {
            "hy_cov": cov,
            "hy_corr": corr,
            "var_x": varx,
            "var_y": vary,
            "beta_x_on_y": beta_x_on_y,
            "beta_y_on_x": beta_y_on_x,
        }

    def lead_lag_scan(
        self,
        sym_x: str,
        sym_y: str,
        *,
        lags: Sequence[float] = (-2.0, -1.0, -0.5, -0.25, 0.0, 0.25, 0.5, 1.0, 2.0),
        now_ts: Optional[float] = None,
    ) -> Dict[str, object]:
        """Scan HY correlation over a grid of lags and return the best (by |corr|).

        Note: Positive lag means Y is shifted forward by τ (Corr(X_t, Y_{t+τ})).
        If Y leads X by L>0 (i.e., Y_t ≈ X_{t+L}), the best lag tends to −L.
        """
        best_lag = 0.0
        best_corr = 0.0
        corr_by_lag: Dict[float, float] = {}
        for tau in lags:
            m = self.hy_metrics(sym_x, sym_y, now_ts=now_ts, lag_s=tau)
            corr_by_lag[float(tau)] = m["hy_corr"]
            if abs(m["hy_corr"]) > abs(best_corr):
                best_corr = m["hy_corr"]
                best_lag = float(tau)
        base = self.hy_metrics(sym_x, sym_y, now_ts=now_ts, lag_s=0.0)
        return {
            "hy_corr_0": base["hy_corr"],
            "hy_cov_0": base["hy_cov"],
            "beta_x_on_y_0": base["beta_x_on_y"],
            "beta_y_on_x_0": base["beta_y_on_x"],
            "corr_by_lag": corr_by_lag,
            "best_lag": best_lag,
            "best_corr": best_corr,
        }


# =============================
# Self-tests (synthetic irregular streams)
# =============================

def _simulate_irregular_streams(T: float = 60.0, seed: int = 42) -> Tuple[List[Tuple[float,float]], List[Tuple[float,float]]]:
    """Generate two price streams on irregular grids with a known lead–lag.

    Underlying latent log-price S(t) ~ drift + σ W_t. We generate on a dense grid,
    then sample two Poisson clocks: X observes S(t) + ε_x, Y observes S(t+L) + ε_y.
    Thus Y leads X by L>0; HY best lag should be around −L.
    """
    random.seed(seed)
    dt = 0.01
    n = int(T / dt)
    L = 0.5  # seconds lead of Y over X
    mu = 0.0
    sigma = 0.02
    eps = 0.0005
    # dense latent path
    ts_dense = [i * dt for i in range(n + 1)]
    s = 0.0
    S: List[float] = [0.0]
    for i in range(1, n + 1):
        dz = random.gauss(0.0, math.sqrt(dt))
        s = s + mu * dt + sigma * dz
        S.append(s)
    # helper to sample from dense grid
    def sample_latent(t: float) -> float:
        if t <= 0:
            return S[0]
        if t >= T:
            return S[-1]
        k = int(t / dt)
        t0 = k * dt
        t1 = (k + 1) * dt
        w = 0.0 if t1 == t0 else (t - t0) / (t1 - t0)
        return (1 - w) * S[k] + w * S[k + 1]
    # Poisson clocks
    lam_x = 12.0  # Hz
    lam_y = 11.0
    t = 0.0
    X: List[Tuple[float, float]] = []
    while t < T:
        t += random.expovariate(lam_x)
        if t > T:
            break
        px = math.exp(sample_latent(t) + random.gauss(0.0, eps))
        X.append((t, px))
    t = 0.0
    Y: List[Tuple[float, float]] = []
    while t < T:
        t += random.expovariate(lam_y)
        if t > T:
            break
        # Y observes future path shifted by L
        py = math.exp(sample_latent(min(T, t + L)) + random.gauss(0.0, eps))
        Y.append((t, py))
    return X, Y


def _test_hy_and_leadlag() -> None:
    X, Y = _simulate_irregular_streams(T=40.0, seed=1)
    hy = CrossAssetHY(window_s=30.0, max_points=10000)
    # interleave ingestion in time order
    merged = sorted([("X", t, p) for t, p in X] + [("Y", t, p) for t, p in Y], key=lambda z: z[1])
    for sym, t, p in merged:
        hy.add_tick(sym, t, p)
    scan = hy.lead_lag_scan("X", "Y", lags=[-2.0, -1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 1.0])
    # since Y leads X by L=0.5, best lag should be ~ -0.5 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[47:226]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[51:229]
    if x >= 0:
        z = math.exp(-x)
        return 1.0 / (1.0 + z)
    else:
        z = math.exp(x)
        return z / (1.0 + z)


def _clip01(p: float, eps: float = 1e-12) -> float:
    return min(1.0 - eps, max(eps, p))


def _logit(p: float, eps: float = 1e-12) -> float:
    p = _clip01(p, eps)
    return math.log(p / (1.0 - p))


# =============================
# Metrics
# =============================

def brier_score(p: Sequence[float], y: Sequence[int]) -> float:
    n = max(1, len(p))
    s = 0.0
    for pi, yi in zip(p, y):
        d = pi - float(yi)
        s += d * d
    return s / n


def log_loss(p: Sequence[float], y: Sequence[int]) -> float:
    n = max(1, len(p))
    s = 0.0
    for pi, yi in zip(p, y):
        pi = _clip01(float(pi))
        if yi:
            s += -math.log(pi)
        else:
            s += -math.log(1.0 - pi)
    return s / n


def ece_uniform(p: Sequence[float], y: Sequence[int], n_bins: int = 15) -> float:
    if n_bins <= 1:
        return abs(sum(y_i for y_i in y) / max(1, len(y)) - (sum(p) / max(1, len(p))))
    bins: List[float] = [0.0] * n_bins
    cnts: List[int] = [0] * n_bins
    hits: List[int] = [0] * n_bins
    for pi, yi in zip(p, y):
        b = min(n_bins - 1, max(0, int(float(pi) * n_bins)))
        bins[b] += float(pi)
        cnts[b] += 1
        hits[b] += int(yi)
    ece = 0.0
    n = max(1, len(p))
    for b in range(n_bins):
        if cnts[b] == 0:
            continue
        conf = bins[b] / cnts[b]
        acc = hits[b] / cnts[b]
        ece += (cnts[b] / n) * abs(acc - conf)
    return ece


@dataclass
class PrequentialMetrics:
    """Online (prequential) metrics with fixed binning for ECE."""
    n_bins: int = 15
    _n: int = 0
    _sum_brier: float = 0.0
    _sum_logloss: float = 0.0
    _bin_sum_p: List[float] = None  # type: ignore
    _bin_hits: List[int] = None  # type: ignore
    _bin_cnts: List[int] = None  # type: ignore

    def __post_init__(self) -> None:
        self._bin_sum_p = [0.0] * self.n_bins
        self._bin_hits = [0] * self.n_bins
        self._bin_cnts = [0] * self.n_bins

    def update(self, p: float, y: int) -> None:
        p = float(p)
        y = int(y)
        self._n += 1
        self._sum_brier += (p - y) ** 2
        p_c = _clip01(p)
        self._sum_logloss += - (y * math.log(p_c) + (1 - y) * math.log(1.0 - p_c))
        b = min(self.n_bins - 1, max(0, int(p * self.n_bins)))
        self._bin_sum_p[b] += p
        self._bin_hits[b] += y
        self._bin_cnts[b] += 1

    def metrics(self) -> ProbabilityMetrics:
        n = max(1, self._n)
        ece = 0.0
        for i in range(self.n_bins):
            if self._bin_cnts[i] == 0:
                continue
            conf = self._bin_sum_p[i] / self._bin_cnts[i]
            acc = self._bin_hits[i] / self._bin_cnts[i]
            ece += (self._bin_cnts[i] / n) * abs(acc - conf)
        return ProbabilityMetrics(
            ece=ece,
            brier=self._sum_brier / n,
            logloss=self._sum_logloss / n,
        )


# =============================
# Calibrators
# =============================

@dataclass
class PlattCalibrator:
    """Logistic calibration: p = σ(A·score + B)."""
    A: float = 1.0
    B: float = 0.0
    l2: float = 1e-2
    max_iter: int = 100
    tol: float = 1e-9

    def fit(self, scores: Sequence[float], y: Sequence[int]) -> "PlattCalibrator":
        A, B = self.A, self.B
        lam = float(self.l2)
        for _ in range(self.max_iter):
            gA = lam * A
            gB = lam * B
            hAA = lam
            hAB = 0.0
            hBB = lam
            for s, yi in zip(scores, y):
                z = A * float(s) + B
                q = _sigmoid(z)
                w = q * (1.0 - q)
                diff = (q - yi)
                gA += diff * s
                gB += diff
                hAA += w * (s * s)
                hAB += w * s
                hBB += w
            # Solve 2x2: H * delta = -g
            det = hAA * hBB - hAB * hAB
            if det <= 0:
                # fall back to small gradient step
                stepA = -gA / (hAA + 1e-6)
                stepB = -gB / (hBB + 1e-6)
            else:
                stepA = (-gA * hBB + gB * hAB) / det
                stepB = (-gB * hAA + gA * hAB) / det
            # backtracking line search
            t = 1.0
            def nll(a: float, b: float) -> float:
                ssum = 0.5 * lam * (a * a + b * b)
                for s, yi in zip(scores, y):
                    z = a * float(s) + b
                    # stable log(1+e^z)
                    if z >= 0:
                        ssum += (z - yi * z) + math.log1p(math.exp(-z))
                    else:
                        ssum += - yi * z + math.log1p(math.exp(z))
                return ssum
            base = nll(A, B)
            while t > 1e-6:
                A_new = A + t * stepA
                B_new = B + t * stepB
                if nll(A_new, B_new) <= base:
                    A, B = A_new, B_new
                    break
                t *= 0.5
            if abs(stepA) < self.tol and abs(stepB) < self.tol:
                break
        self.A, self.B = float(A), float(B)
        return self

    def predict_proba(self, scores: Sequence[float]) -> List[float]:
        return [_sigmoid(self.A * float(s) + self.B) for s in scores]

    def calibrate_prob(self, p_raw: float) -> float:
        """Calibrate a single probability using Platt scaling.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.schema_validator:[37:212]
==repo_core_config_schema_validator:[37:213]
class SchemaValidationError(Exception):
    """Raised when configuration fails schema validation."""

class SchemaLoadError(Exception):
    """Raised when schema file cannot be loaded or is invalid."""

# -------------------- Core --------------------

_JSON = Union[Dict[str, Any], List[Any], str, int, float, bool, None]


def _json_pointer(root: Mapping[str, Any], pointer: str) -> Any:
    """Resolve a local JSON Pointer like '#/a/b/0'."""
    if pointer.startswith("#"):
        pointer = pointer[1:]
    if pointer.startswith("/"):
        pointer = pointer[1:]
    cur: Any = root
    if not pointer:
        return cur
    for part in pointer.split("/"):
        part = part.replace("~1", "/").replace("~0", "~")
        if isinstance(cur, Mapping):
            if part not in cur:
                raise SchemaValidationError(f"$ref unresolved at '/{pointer}' (missing '{part}')")
            cur = cur[part]
        elif isinstance(cur, list):
            try:
                idx = int(part)
            except Exception:
                raise SchemaValidationError(f"$ref '/{pointer}' points into array with non-integer index '{part}'")
            try:
                cur = cur[idx]
            except Exception:
                raise SchemaValidationError(f"$ref '/{pointer}' index {idx} out of range")
        else:
            raise SchemaValidationError(f"$ref '/{pointer}' traversed non-container at '{part}'")
    return cur


def _path_join(base: str, key: str) -> str:
    return f"{base}.{key}" if base else key


@dataclass
class _Ctx:
    root_schema: Mapping[str, Any]
    allow_additional: bool


class SchemaValidator:
    def __init__(self, schema: Mapping[str, Any], *, allow_additional_properties: bool = True) -> None:
        if not isinstance(schema, Mapping):
            raise SchemaLoadError("schema must be a mapping")
        self._schema: Mapping[str, Any] = schema
        self._allow_additional = allow_additional_properties
        # cache compiled regex patterns to avoid recompilation
        self._re_cache: Dict[str, re.Pattern[str]] = {}

    # -------- Construction helpers --------

    @classmethod
    def from_path(cls, path: Union[str, Path], *, allow_additional_properties: bool = True) -> "SchemaValidator":
        p = Path(path)
        try:
            obj = json.loads(p.read_text(encoding="utf-8"))
        except Exception as e:
            raise SchemaLoadError(f"failed to load schema '{p}': {e}")
        return cls(obj, allow_additional_properties=allow_additional_properties)

    # -------- Public API --------

    def version(self) -> Optional[str]:
        return self._schema.get("$id") or self._schema.get("version")

    def raw(self) -> Mapping[str, Any]:
        return self._schema

    def hotreload_whitelist(self) -> List[str]:
        wl = self._schema.get("hotReloadWhitelist")
        return list(wl) if isinstance(wl, list) else []

    def validate(self, data: _JSON, *, apply_defaults: bool = False) -> _JSON:
        """
        Validate `data` against schema root (key 'schema' if present else whole doc).
        Returns a **new** normalized object when apply_defaults=True, else returns input (unmodified).
        Raises SchemaValidationError on first failure (fail-fast, deterministic paths).
        """
        root = self._schema.get("schema", self._schema)
        ctx = _Ctx(root_schema=root, allow_additional=self._allow_additional)
        value = deepcopy(data) if apply_defaults else data
        out = self._validate_node(value, root, path="", ctx=ctx, apply_defaults=apply_defaults)
        return out

    # -------- Internal validation --------

    def _validate_node(self, value: Any, schema: Mapping[str, Any], *, path: str, ctx: _Ctx, apply_defaults: bool) -> Any:
        # $ref resolution (local only)
        if "$ref" in schema:
            ref = schema["$ref"]
            if not isinstance(ref, str):
                raise SchemaValidationError(f"{path or '$'}: $ref must be string")
            target = _json_pointer(ctx.root_schema, ref)
            if not isinstance(target, Mapping):
                raise SchemaValidationError(f"{path or '$'}: $ref must target an object schema")
            return self._validate_node(value, target, path=path, ctx=ctx, apply_defaults=apply_defaults)

        typ = schema.get("type")
        if typ is None:
            # if schema is a composition without type, still apply compositions/const/enum
            pass
        else:
            self._enforce_type(value, typ, path)

        # compositions first (fail-fast with precise path)
        if "allOf" in schema:
            for i, sub in enumerate(schema["allOf"]):
                value = self._validate_node(value, sub, path=path, ctx=ctx, apply_defaults=apply_defaults)
        if "anyOf" in schema:
            errors: List[str] = []
            for sub in schema["anyOf"]:
                try:
                    self._validate_node(value, sub, path=path, ctx=ctx, apply_defaults=apply_defaults)
                    errors = []
                    break
                except SchemaValidationError as e:
                    errors.append(str(e))
            if errors:
                raise SchemaValidationError(f"{path or '$'}: failed anyOf; reasons: {errors[:2]}")
        if "oneOf" in schema:
            matches = 0
            last_val = value
            last_err: Optional[str] = None
            for sub in schema["oneOf"]:
                try:
                    last_val = self._validate_node(value, sub, path=path, ctx=ctx, apply_defaults=apply_defaults)
                    matches += 1
                except SchemaValidationError as e:
                    last_err = str(e)
            if matches != 1:
                raise SchemaValidationError(f"{path or '$'}: expected exactly one schema in oneOf to match (got {matches}); last_err={last_err}")
            value = last_val
        if "not" in schema:
            try:
                self._validate_node(value, schema["not"], path=path, ctx=ctx, apply_defaults=False)
            except SchemaValidationError:
                pass  # good: does not match
            else:
                raise SchemaValidationError(f"{path or '$'}: value must not match schema in 'not'")

        # enums / const
        if "enum" in schema:
            enum = schema["enum"]
            if value not in enum:
                raise SchemaValidationError(f"{path or '$'}: value {value!r} not in enum {enum}")
        if "const" in schema:
            if value != schema["const"]:
                raise SchemaValidationError(f"{path or '$'}: value {value!r} != const {schema['const']!r}")

        # Type-specific constraints
        if typ == "object" or (typ is None and isinstance(value, dict)):
            return self._validate_object(value, schema, path=path, ctx=ctx, apply_defaults=apply_defaults)
        if typ == "array" or (typ is None and isinstance(value, list)):
            return self._validate_array(value, schema, path=path, ctx=ctx, apply_defaults=apply_defaults)
        if typ == "string" or (typ is None and isinstance(value, str)):
            self._validate_string(value, schema, path)
            return value
        if typ == "number" or (typ is None and isinstance(value, (int, float))):
            self._validate_number(value, schema, path, integer=False)
            return value
        if typ == "integer" or (typ is None and isinstance(value, int) and not isinstance(value, bool)):
            self._validate_number(value, schema, path, integer=True)
            return value
        if typ == "boolean" or (typ is None and isinstance(value, bool)):
            return value
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==evt_pot:[33:198]
==repo_core_risk_evt_pot_py_pot_gpd_over_threshold_u_tail_va_r_es_bootstrap_ci_self_tests:[28:193]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# =============================
# Utilities
# =============================

def _quantile(values: Sequence[float], q: float) -> float:
    if not values:
        return 0.0
    q = min(1.0, max(0.0, float(q)))
    xs = sorted(float(v) for v in values)
    k = int(math.ceil(q * len(xs)) - 1)
    k = max(0, min(len(xs) - 1, k))
    return xs[k]


def _mean_var(xs: Sequence[float]) -> Tuple[float, float]:
    n = len(xs)
    if n == 0:
        return 0.0, 0.0
    mu = sum(xs) / n
    var = sum((x - mu) ** 2 for x in xs) / max(1, n - 1)
    return mu, var


# =============================
# GPD fit and tail metrics
# =============================

@dataclass
class GPDEstimate:
    xi: float
    beta: float
    u: float
    zeta: float  # tail fraction k/n
    n_exc: int
    n_total: int


def select_threshold(losses: Sequence[float], q: float = 0.95) -> float:
    return _quantile([max(0.0, float(x)) for x in losses], q)


def fit_gpd_mom(excesses: Sequence[float], *, clip_xi: Tuple[float, float] = (-0.25, 0.9)) -> Tuple[float, float]:
    """Method-of-moments (MoM) for GPD parameters from **excesses** (x≥0).

    For GPD(ξ, β), mean μ = β/(1−ξ) (ξ<1) and var σ² = β²/((1−ξ)²(1−2ξ)) (ξ<1/2).
    Solving gives ξ̂ = (1 − μ²/σ²)/2, β̂ = μ(1 − ξ̂) with guards & clipping.
    If σ² ≤ μ² → ξ≈0 (exponential), β≈μ.
    """
    x = [float(v) for v in excesses if float(v) > 0.0]
    if not x:
        return 0.0, 1e-9
    mu, var = _mean_var(x)
    if var <= 0.0 or var <= mu * mu:
        xi = 0.0
        beta = max(1e-9, mu)
        return xi, beta
    xi = 0.5 * (1.0 - (mu * mu) / var)
    xi = min(clip_xi[1], max(clip_xi[0], xi))
    beta = max(1e-9, mu * (1.0 - xi))
    return xi, beta


def pot_fit(losses: Sequence[float], *, q_u: float = 0.95) -> GPDEstimate:
    """Fit GPD on excesses above threshold u = Q_{q_u}(losses)."""
    L = [max(0.0, float(z)) for z in losses]
    if not L:
        return GPDEstimate(0.0, 1e-9, 0.0, 0.0, 0, 0)
    u = select_threshold(L, q=q_u)
    exc = [x - u for x in L if x > u]
    n_total = len(L)
    n_exc = len(exc)
    zeta = 0.0 if n_total == 0 else n_exc / n_total
    xi, beta = fit_gpd_mom(exc)
    return GPDEstimate(xi=xi, beta=beta, u=u, zeta=zeta, n_exc=n_exc, n_total=n_total)


def pot_var_es(est: GPDEstimate, p: float) -> Tuple[float, float]:
    """Compute VaR_p and ES_p for original loss variable X using POT.

    Given u, zeta=k/n, and GPD(ξ,β) fit on Y=X−u | X>u.
    For target tail prob 1−p (e.g., p=0.99):
      y_p = β/ξ * ((ζ/(1−p))^ξ − 1),  ξ≠0
      y_p = β * ln(ζ/(1−p)),           ξ=0
      VaR_p = u + y_p
      ES_p = u + (y_p + β)/(1−ξ)      (ξ<1)
    """
    xi = float(est.xi)
    beta = max(1e-12, float(est.beta))
    u = float(est.u)
    zeta = max(1e-12, float(est.zeta))
    p = min(1.0 - 1e-12, max(0.0, float(p)))
    tail = max(1e-12, 1.0 - p)
    if abs(xi) < 1e-12:
        y = beta * math.log(zeta / tail)
    else:
        y = (beta / xi) * ( (zeta / tail) ** xi - 1.0 )
    var_p = u + max(0.0, y)
    # ES only defined for ξ<1
    if xi < 1.0:
        es_p = u + (y + beta) / (1.0 - xi)
    else:
        es_p = float('inf')
    return var_p, es_p


def pot_var_bootstrap(losses: Sequence[float], p: float, *, q_u: float = 0.95, n_boot: int = 300, seed: int = 7, ci: Tuple[float, float] = (0.05, 0.95)) -> Dict[str, float]:
    """Bootstrap percentile CI for VaR_p via resampling exceedances.

    Keeps the same threshold u and tail fraction ζ̂; resamples exceedances with
    replacement, re-fits (MoM), recomputes VaR_p each time.
    """
    rnd = random.Random(seed)
    L = [max(0.0, float(z)) for z in losses]
    if not L:
        return {"var": 0.0, "lo": 0.0, "hi": 0.0, "u": 0.0, "zeta": 0.0}
    u = select_threshold(L, q=q_u)
    exc = [x - u for x in L if x > u]
    n_total = len(L)
    n_exc = len(exc)
    zeta = 0.0 if n_total == 0 else n_exc / n_total
    # point
    est = GPDEstimate(*fit_gpd_mom(exc), u, zeta, n_exc, n_total)
    var_p, _ = pot_var_es(est, p)
    if n_exc < 5:
        return {"var": var_p, "lo": var_p, "hi": var_p, "u": u, "zeta": zeta}
    vals: List[float] = []
    for _ in range(int(n_boot)):
        bs = [exc[rnd.randrange(0, n_exc)] for __ in range(n_exc)]
        xi, beta = fit_gpd_mom(bs)
        est_b = GPDEstimate(xi, beta, u, zeta, n_exc, n_total)
        v, _ = pot_var_es(est_b, p)
        vals.append(v)
    lo_q, hi_q = ci
    lo = _quantile(vals, lo_q)
    hi = _quantile(vals, hi_q)
    return {"var": var_p, "lo": lo, "hi": hi, "u": u, "zeta": zeta}


# =============================
# Rolling POT window
# =============================

class RollingPOT:
    """Maintain a window of recent losses and provide POT tail metrics on demand."""
    def __init__(self, window_n: int = 5000, q_u: float = 0.95) -> None:
        self.N = int(window_n)
        self.q_u = float(q_u)
        self.q: Deque[float] = deque()

    def add(self, loss: float) -> None:
        x = max(0.0, float(loss))
        self.q.append(x)
        while len(self.q) > self.N:
            self.q.popleft()

    def report(self, p: float = 0.99, with_bootstrap: bool = False, n_boot: int = 200) -> Dict[str, float]:
        L = list(self.q)
        est = pot_fit(L, q_u=self.q_u)
        var_p, es_p = pot_var_es(est, p)
        out = { (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==icp:[23:176]
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[23:176]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# =============================
# Utilities
# =============================

def _quantile_leq(sorted_vals: Sequence[float], q: float) -> float:
    """Left-closed quantile: returns smallest t such that P(X ≤ t) ≥ q.
    Input must be sorted ascending.
    """
    n = len(sorted_vals)
    if n == 0:
        return 0.0
    a = min(1.0, max(0.0, float(q)))
    k = int(math.ceil(a * n)) - 1
    k = max(0, min(n - 1, k))
    return float(sorted_vals[k])


def _nonconformity_binary(p: float, y: int) -> float:
    """s = 1 − p_true = y·(1−p) + (1−y)·p"""
    p = min(1.0, max(0.0, float(p)))
    y = 1 if int(y) == 1 else 0
    return y * (1.0 - p) + (1 - y) * p


# =============================
# Split conformal (binary)
# =============================

@dataclass
class SplitConformalBinary:
    alpha: float = 0.1  # miscoverage level (target coverage 1−α)

    def fit(self, p_hat: Sequence[float], y: Sequence[int]) -> None:
        assert len(p_hat) == len(y)
        s = [_nonconformity_binary(pi, yi) for pi, yi in zip(p_hat, y)]
        self.n = len(s)
        self.scores = sorted(s)

    def p_values(self, p_new: float) -> Tuple[float, float]:
        """Return p-values (p_y=1, p_y=0) for a new probability p_new=P(y=1)."""
        n = getattr(self, "n", 0)
        S = getattr(self, "scores", [])
        if n == 0:
            return 1.0, 1.0
        s1 = _nonconformity_binary(p_new, 1)
        s0 = _nonconformity_binary(p_new, 0)
        # count >= s (right tail); S is sorted asc
        def count_ge(t: float) -> int:
            # binary search first index > t then n - idx; inclusive ≥ via small epsilon
            lo, hi = 0, n
            while lo < hi:
                mid = (lo + hi) // 2
                if S[mid] > t:
                    hi = mid
                else:
                    lo = mid + 1
            # lo is first index with S[lo] > t → #≥ = n - (lo - k_eq)
            # to ensure ≥, we used (S[mid] > t) branch; so indices equal to t are included
            ge = n - (lo - 0)
            return ge
        ge1 = count_ge(s1)
        ge0 = count_ge(s0)
        # conformal p-values with +1 smoothing
        pval1 = (ge1 + 1) / (n + 1)
        pval0 = (ge0 + 1) / (n + 1)
        return float(pval1), float(pval0)

    def predict_set(self, p_new: float) -> List[int]:
        p1, p0 = self.p_values(p_new)
        S = []
        if p1 > self.alpha:
            S.append(1)
        if p0 > self.alpha:
            S.append(0)
        return S


# =============================
# Mondrian split conformal (per-group)
# =============================

@dataclass
class MondrianConformalBinary:
    alpha: float = 0.1

    def fit(self, p_hat: Sequence[float], y: Sequence[int], groups: Sequence[str]) -> None:
        assert len(p_hat) == len(y) == len(groups)
        self.bucket: Dict[str, List[float]] = {}
        for pi, yi, g in zip(p_hat, y, groups):
            s = _nonconformity_binary(pi, yi)
            L = self.bucket.setdefault(str(g), [])
            L.append(s)
        for k in list(self.bucket.keys()):
            self.bucket[k].sort()
        # global fallback
        self.global_scores = sorted([_nonconformity_binary(pi, yi) for pi, yi in zip(p_hat, y)])

    def _p_values_from_scores(self, scores: Sequence[float], p_new: float) -> Tuple[float, float]:
        if not scores:
            return 1.0, 1.0
        n = len(scores)
        s1 = _nonconformity_binary(p_new, 1)
        s0 = _nonconformity_binary(p_new, 0)
        # count ≥ via binary search
        import bisect
        k1 = n - bisect.bisect_left(scores, s1)  # elements >= s1
        k0 = n - bisect.bisect_left(scores, s0)
        return (k1 + 1) / (n + 1), (k0 + 1) / (n + 1)

    def p_values(self, p_new: float, group: Optional[str]) -> Tuple[float, float]:
        if group is not None and group in getattr(self, "bucket", {}):
            return self._p_values_from_scores(self.bucket[group], p_new)
        return self._p_values_from_scores(getattr(self, "global_scores", []), p_new)

    def predict_set(self, p_new: float, group: Optional[str]) -> List[int]:
        p1, p0 = self.p_values(p_new, group)
        S = []
        if p1 > self.alpha:
            S.append(1)
        if p0 > self.alpha:
            S.append(0)
        return S


# =============================
# Isotonic regression (PAV) for Venn–Abers
# =============================

@dataclass
class _Iso:
    xs: List[float]
    ys: List[float]

    def fit(self) -> None:
        # Pool-Adjacent-Violators for isotonic (non-decreasing) fit
        xs = [float(x) for x in self.xs]
        ys = [float(y) for y in self.ys]
        order = sorted(range(len(xs)), key=lambda i: xs[i])
        x = [xs[i] for i in order]
        y = [ys[i] for i in order]
        n = len(x)
        if n == 0:
            self.blocks = []
            return
        # initialize blocks
        v = [float(val) for val in y]
        w = [1.0] * n
        i = 0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==cvar:[224:371]
==repo_core_risk_cvar:[168:315]
    for i, ui in enumerate(u):
        css += ui
        t = (css - z) / (i + 1)
        if i == len(u) - 1 or u[i + 1] <= t:
            rho = i
            tau = t
            break
    return [max(0.0, xi - tau) for xi in x]


def _proj_l1_ball(v: Sequence[float], c: float) -> List[float]:
    """Project onto L1 ball {∑|w| ≤ c}. Duchi et al. (2008)."""
    c = max(1e-12, float(c))
    u = [abs(float(t)) for t in v]
    if sum(u) <= c:
        return [float(t) for t in v]
    # find threshold
    u_sorted = sorted(u, reverse=True)
    css = 0.0
    rho = 0
    tau = 0.0
    for i, ui in enumerate(u_sorted):
        css += ui
        t = (css - c) / (i + 1)
        if i == len(u_sorted) - 1 or u_sorted[i + 1] <= t:
            rho = i
            tau = t
            break
    return [math.copysign(max(0.0, abs(vi) - tau), vi) for vi in v]


def cvar_minimize(
    returns: Sequence[Sequence[float]],
    *,
    alpha: float = 0.99,
    steps: int = 400,
    lr: float = 0.5,
    long_only: bool = True,
    sum_to_one: bool = True,
    leverage_cap: float = 1.0,
    seed: int = 7,
) -> List[float]:
    """Minimize CVaRα of portfolio **returns** via projected subgradient.

    - If `long_only` and `sum_to_one`: project to simplex (∑w=1, w≥0).
    - Else: project to L1 ball with radius `leverage_cap` (∑|w|≤cap).

    Subgradient of CVaR wrt w ≈ −E[r | loss≥VaR]. We estimate VaR at each step,
    collect the tail set, and update w ← w − lr · (−mean_tail_return).
    """
    rnd = random.Random(seed)
    m = len(returns[0]) if returns else 0
    if m == 0:
        return []
    # init weights uniform on simplex
    w = [1.0 / m] * m if long_only and sum_to_one else [rnd.uniform(-0.1, 0.1) for _ in range(m)]

    def tail_grad(weights: Sequence[float]) -> Tuple[float, List[float]]:
        # compute portfolio pnl for each scenario
        pnl: List[float] = []
        for scen in returns:
            pnl.append(sum(float(a) * float(b) for a, b in zip(scen, weights)))
        # losses
        losses = [max(0.0, -x) for x in pnl]
        # VaR and tail mask
        var, _ = var_cvar_from_losses(losses, alpha)
        tail_idx = [i for i, L in enumerate(losses) if L >= var]
        if not tail_idx:
            return var, [0.0] * m
        # gradient = -mean of tail returns (vector)
        g = [0.0] * m
        for i in tail_idx:
            ri = returns[i]
            for j in range(m):
                g[j] += -float(ri[j])
        g = [gj / len(tail_idx) for gj in g]
        # also return current CVaR for logging (optional)
        return var, g

    for t in range(steps):
        _, g = tail_grad(w)
        # step
        w = [wi - lr * gi for wi, gi in zip(w, g)]
        # project
        if long_only and sum_to_one:
            w = _proj_simplex(w, z=1.0)
        else:
            w = _proj_l1_ball(w, c=leverage_cap)
        # anneal lr slightly
        lr *= 0.99
    return w


# =============================
# Self-tests
# =============================

def _make_scenarios(n: int = 3000, m: int = 3, seed: int = 3) -> List[List[float]]:
    rnd = random.Random(seed)
    R: List[List[float]] = []
    for i in range(n):
        # heavy-ish tails: mix of Gaussians
        row: List[float] = []
        for j in range(m):
            s = 0.01 + 0.02 * (j + 1)
            # with small prob, draw from wider tail
            if rnd.random() < 0.05:
                val = rnd.gauss(0.0, 3.0 * s)
            else:
                val = rnd.gauss(0.0, s)
            row.append(val)
        R.append(row)
    return R


def _test_empirical() -> None:
    losses = [0, 1, 2, 3, 4, 5]
    var, cvar = var_cvar_from_losses(losses, alpha=5/6)  # 83.33% → VaR≈5th element (index 4)
    assert abs(var - 4.0) < 1e-12
    assert abs(cvar - (4.0 + 5.0) / 2.0) < 1e-12


def _test_rolling() -> None:
    rc = RollingCVaR(window_n=100, alpha=0.95)
    for i in range(200):
        rc.update(loss=float(i % 20))
    v, c = rc.metrics()
    assert v >= 0 and c >= v


def _test_portfolio_and_opt() -> None:
    R = _make_scenarios(n=1500, m=3, seed=9)
    # equal weights baseline
    m = len(R[0])
    w0 = [1.0 / m] * m
    v0, c0 = portfolio_cvar(w0, R, alpha=0.95)
    w = cvar_minimize(R, alpha=0.95, steps=150, lr=0.8, long_only=True, sum_to_one=True)
    v1, c1 = portfolio_cvar(w, R, alpha=0.95)
    # optimized CVaR should be <= baseline (allow small tolerance)
    assert c1 <= c0 + 1e-6


if __name__ == "__main__":
    _test_empirical()
    _test_rolling()
    _test_portfolio_and_opt()
    print("OK - repo/core/risk/cvar.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.signal.score:[68:205]
==repo_core_signal_score:[54:191]
        return ez / (1.0 + ez)


def _clip(x: float, lo: float, hi: float) -> float:
    return lo if x < lo else hi if x > hi else x


@dataclass
class ScoreOutput:
    score: float
    p_raw: float
    p: float
    components: Dict[str, float]

    def as_dict(self) -> Dict[str, float]:
        d: Dict[str, float] = {
            "score": self.score,
            "p_raw": self.p_raw,
            "p": self.p,
        }
        for k, v in self.components.items():
            d[f"comp_{k}"] = v
        return d


class ScoreModel:
    def __init__(
        self,
        *,
        weights: Mapping[str, float],
        intercept: float = 0.0,
        gamma: Optional[float] = None,
        use_cross_asset: Optional[bool] = None,
    ) -> None:
        self._w = {str(k): float(v) for k, v in weights.items()}
        self._b = float(intercept)

        # Defaults from config if not supplied
        g = gamma
        uca = use_cross_asset
        if g is None or uca is None:
            try:
                cfg = get_config()
                if g is None:
                    g = float(cfg.get("signal.score.gamma", 0.0))
                if uca is None:
                    uca = bool(cfg.get("signal.score.use_cross_asset", True))
            except (ConfigError, Exception):
                # fall back on safe defaults
                if g is None:
                    g = 0.0
                if uca is None:
                    uca = True
        self._gamma = float(g)
        self._use_cross = bool(uca)

    # --------- public API ---------

    def score_event(
        self,
        *,
        features: Mapping[str, Any],
        cross_beta: Optional[float] = None,
        cross_return: Optional[float] = None,
        calibrator: Optional[Union[CalibratorProto, ModelProto, TransformerProto]] = None,
    ) -> ScoreOutput:
        """
        Compute linear score and probability, with optional cross-asset coupling and calibration.

        Parameters
        ----------
        features : mapping from feature name to numeric value (missing treated as 0.0)
        cross_beta : beta_{i|SOL} estimated by lead-lag model (optional)
        cross_return : lagged return of SOL at tau* (optional)
        calibrator : object with one of methods {calibrate_prob(p), predict_proba(p)}; optional
        """
        lin = 0.0
        for k, w in self._w.items():
            xk = features.get(k, 0.0)
            try:
                x = float(xk)
            except Exception:
                x = 0.0
            lin += w * x

        cross = 0.0
        if self._use_cross and self._gamma != 0.0 and cross_beta is not None and cross_return is not None:
            try:
                cross = self._gamma * float(cross_beta) * float(cross_return)
            except Exception:
                cross = 0.0

        s = lin + self._b + cross
        p_raw = _sigmoid(_clip(s, -40.0, 40.0))

        p = p_raw
        if calibrator is not None:
            # flexible adapter
            if hasattr(calibrator, "calibrate_prob"):
                p = float(calibrator.calibrate_prob(p_raw))
            elif hasattr(calibrator, "predict_proba"):
                p = float(calibrator.predict_proba(p_raw))
            elif hasattr(calibrator, "transform"):
                p = float(calibrator.transform(p_raw))
            else:
                # unknown calibrator interface — leave raw prob
                p = p_raw
            # clamp to [0,1]
            p = 0.0 if p < 0.0 else 1.0 if p > 1.0 else p

        comps = {
            "lin": lin,
            "intercept": self._b,
            "cross": cross,
            "gamma": self._gamma,
        }
        return ScoreOutput(score=s, p_raw=p_raw, p=p, components=comps)

    def score_only(self, features: Mapping[str, Any], *, cross_beta: Optional[float] = None, cross_return: Optional[float] = None) -> float:
        return self.score_event(features=features, cross_beta=cross_beta, cross_return=cross_return).score

    def predict_proba(self, features: Mapping[str, Any], *, cross_beta: Optional[float] = None, cross_return: Optional[float] = None, calibrator: Optional[Union[CalibratorProto, ModelProto, TransformerProto]] = None) -> float:
        return self.score_event(features=features, cross_beta=cross_beta, cross_return=cross_return, calibrator=calibrator).p

    # --------- utilities ---------

    def weights(self) -> Mapping[str, float]:
        return dict(self._w)

    def intercept(self) -> float:
        return self._b

    def gamma(self) -> float:
        return self._gamma

    def use_cross_asset(self) -> bool:
        return self._use_cross (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.hotreload:[24:154]
==repo_core_config_hotreload:[24:154]
logger = logging.getLogger("aurora.config.hotreload")
logger.setLevel(logging.INFO)

# -------------------- Exceptions --------------------

class HotReloadViolation(Exception):
    """Raised when a reload attempts to change non-whitelisted keys."""

# -------------------- Utilities --------------------

def _flatten(d: Mapping[str, Any], prefix: str = "") -> Dict[str, Any]:
    out: Dict[str, Any] = {}
    for k, v in d.items():
        key = f"{prefix}.{k}" if prefix else k
        if isinstance(v, Mapping):
            out.update(_flatten(v, key))
        else:
            out[key] = v
    return out


def diff_dicts(old: Mapping[str, Any], new: Mapping[str, Any]) -> Set[str]:
    """Return set of fully-qualified keys that changed between two nested mappings."""
    a = _flatten(old)
    b = _flatten(new)
    changed: Set[str] = set()
    keys = set(a.keys()).union(b.keys())
    for k in keys:
        if a.get(k) != b.get(k):
            changed.add(k)
    return changed

# -------------------- Policy --------------------

@dataclass
class HotReloadPolicy:
    whitelist: Set[str]

    @classmethod
    def from_iterable(cls, items: Iterable[str]) -> "HotReloadPolicy":
        return cls(whitelist={str(x).strip() for x in items if str(x).strip()})

    def is_allowed_key(self, key: str) -> bool:
        """
        Allowed if any whitelist entry equals key or is a prefix of key (with '.')
        """
        for w in self.whitelist:
            if key == w or key.startswith(w + "."):
                return True
        return False

    def violations(self, changed_keys: Iterable[str]) -> Set[str]:
        v: Set[str] = set()
        for k in changed_keys:
            if not self.is_allowed_key(k):
                v.add(k)
        return v

    def require(self, changed_keys: Iterable[str]) -> None:
        v = self.violations(changed_keys)
        if v:
            logger.error("Hot-reload denied; violations: %s", sorted(v))
            raise HotReloadViolation(f"Non-whitelisted changes: {sorted(v)[:5]}")

# -------------------- File watcher --------------------

class FileWatcher:
    """
    Simple mtime-based file watcher.

    on_change callback signature:  (path: Path, mtime: float) -> None
    """

    def __init__(
        self,
        path: Union[str, Path],
        on_change: Callable[[Path, float], None],
        *,
        poll_interval_sec: float = 1.5,
    ) -> None:
        self._path = Path(path).absolute()
        self._on_change = on_change
        self._poll = float(poll_interval_sec)
        self._mtime: Optional[float] = None
        self._thread: Optional[threading.Thread] = None
        self._stop_evt = threading.Event()

    @property
    def path(self) -> Path:
        return self._path

    def start(self) -> None:
        if self._thread is not None:
            return
        self._stop_evt.clear()
        self._thread = threading.Thread(target=self._loop, name=f"FileWatcher[{self._path.name}]", daemon=True)
        self._thread.start()

    def stop(self) -> None:
        if self._thread is None:
            return
        self._stop_evt.set()
        self._thread.join(timeout=3.0)
        self._thread = None

    # ----- internals -----

    def _loop(self) -> None:  # pragma: no cover (threading path)
        while not self._stop_evt.wait(self._poll):
            try:
                st = self._path.stat()
            except FileNotFoundError:
                continue
            mtime = st.st_mtime
            if self._mtime is None:
                self._mtime = mtime
                continue
            if mtime != self._mtime:
                self._mtime = mtime
                try:
                    self._on_change(self._path, mtime)
                except Exception:
                    logger.exception("FileWatcher callback failed for %s", self._path)

__all__ = [
    "HotReloadViolation",
    "HotReloadPolicy",
    "diff_dicts",
    "FileWatcher",
] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==drift:[59:185]
==repo_core_calibration_drift:[34:160]
@dataclass
class CUSUMDetector:
    """Two-sided Page's CUSUM for shifts in mean of a stream x_t.

    Parameters
    ----------
    k : float
        Reference value (half the shift you want to be most sensitive to).
    h : float
        Decision threshold. Typical range: 4..8 times the std of in-control noise.
    clip : float
        Optional absolute clip for x to increase robustness.
    """
    k: float = 0.25
    h: float = 5.0
    clip: Optional[float] = None

    s_pos: float = 0.0
    s_neg: float = 0.0
    last_ts: Optional[float] = None

    def reset(self) -> None:
        self.s_pos = 0.0
        self.s_neg = 0.0
        self.last_ts = None

    def update(self, x: float, ts: Optional[float] = None) -> Dict[str, float]:
        if self.clip is not None:
            c = float(self.clip)
            x = max(-c, min(c, float(x)))
        else:
            x = float(x)
        # Page's recursion
        self.s_pos = max(0.0, self.s_pos + x - self.k)
        self.s_neg = min(0.0, self.s_neg + x + self.k)
        self.last_ts = None if ts is None else float(ts)
        alarm = 1.0 if (self.s_pos >= self.h or -self.s_neg >= self.h) else 0.0
        return {
            "cusum_pos": self.s_pos,
            "cusum_neg": self.s_neg,
            "cusum_alarm": alarm,
        }


# =============================
# GLR over rolling window (Gaussian, change in mean)
# =============================

@dataclass
class GLRDetector:
    """Generalized Likelihood Ratio for a change-in-mean within a rolling window.

    Assumes in-control: x_t ~ N(μ, σ²) with constant σ. Tests H0: no change vs H1:
    one change-point τ in the last W points. Uses the maximum standardized
    difference in sample means across splits (τ) as GLR stat.

    Threshold `thr` is on the squared standardized mean gap times an empirical
    factor. This is a pragmatic detector; for rigorous control consult Lorden/Page.
    """
    window: int = 200
    thr: float = 25.0  # typical 16..36 for unit-variance noise
    clip: Optional[float] = 6.0

    def __post_init__(self) -> None:
        self.buf: Deque[float] = deque()
        self.sum: float = 0.0
        self.sum2: float = 0.0

    def reset(self) -> None:
        self.buf.clear()
        self.sum = 0.0
        self.sum2 = 0.0

    def _push(self, x: float) -> None:
        if self.clip is not None:
            c = float(self.clip)
            x = max(-c, min(c, float(x)))
        else:
            x = float(x)
        self.buf.append(x)
        self.sum += x
        self.sum2 += x * x
        if len(self.buf) > self.window:
            y = self.buf.popleft()
            self.sum -= y
            self.sum2 -= y * y

    def _glr_stat(self) -> float:
        n = len(self.buf)
        if n < 2:
            return 0.0
        # estimate pooled variance
        mu = self.sum / n
        var = max(1e-9, self.sum2 / n - mu * mu)
        sd = math.sqrt(var)
        # try all split points τ (leave at least 1 point on each side)
        best = 0.0
        # precompute prefix sums
        s = 0.0
        k = 0
        for x in self.buf:
            k += 1
            if k == n:
                break
            s += x
            m1 = s / k
            m2 = (self.sum - s) / (n - k)
            gap = abs(m1 - m2) / max(1e-12, sd)
            stat = gap * gap * min(k, n - k)  # scaled by segment size
            if stat > best:
                best = stat
        return best

    def update(self, x: float, ts: Optional[float] = None) -> Dict[str, float]:
        self._push(x)
        g = self._glr_stat()
        alarm = 1.0 if g >= self.thr else 0.0
        return {"glr_stat": g, "glr_alarm": alarm}


# =============================
# Drift monitor (combo)
# =============================

@dataclass
class DriftMonitor: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.schema_validator:[235:312]
==repo_core_config_schema_validator:[231:308]
        else:
            raise SchemaValidationError(f"{path or '$'}: unsupported schema type '{typ}'")
        if not ok:
            raise SchemaValidationError(f"{path or '$'}: expected {typ}, got {type(value).__name__}")

    def _validate_number(self, value: Union[int, float], schema: Mapping[str, Any], path: str, *, integer: bool) -> None:
        if integer and not (isinstance(value, int) and not isinstance(value, bool)):
            raise SchemaValidationError(f"{path or '$'}: expected integer")
        if "multipleOf" in schema:
            m = schema["multipleOf"]
            try:
                if (value / m) % 1 != 0:
                    raise SchemaValidationError(f"{path or '$'}: {value} not multipleOf {m}")
            except Exception:
                raise SchemaValidationError(f"{path or '$'}: invalid multipleOf {m}")
        for key, op in (("minimum", lambda a, b: a < b), ("maximum", lambda a, b: a > b)):
            if key in schema:
                bound = schema[key]
                if op(value, bound):
                    raise SchemaValidationError(f"{path or '$'}: value {value} violates {key} {bound}")
        for key, strict, op in (
            ("exclusiveMinimum", True, lambda a, b: a <= b),
            ("exclusiveMaximum", True, lambda a, b: a >= b),
        ):
            if key in schema:
                bound = schema[key]
                if op(value, bound):
                    raise SchemaValidationError(f"{path or '$'}: value {value} violates {key} {bound}")

    def _validate_string(self, value: str, schema: Mapping[str, Any], path: str) -> None:
        if "minLength" in schema and len(value) < int(schema["minLength"]):
            raise SchemaValidationError(f"{path or '$'}: string shorter than minLength {schema['minLength']}")
        if "maxLength" in schema and len(value) > int(schema["maxLength"]):
            raise SchemaValidationError(f"{path or '$'}: string longer than maxLength {schema['maxLength']}")
        if "pattern" in schema:
            pat = schema["pattern"]
            if pat not in self._re_cache:
                try:
                    self._re_cache[pat] = re.compile(pat)
                except re.error as e:
                    raise SchemaValidationError(f"{path or '$'}: invalid regex pattern '{pat}': {e}")
            if not self._re_cache[pat].search(value):
                raise SchemaValidationError(f"{path or '$'}: value does not match pattern '{pat}'")

    def _validate_array(self, value: List[Any], schema: Mapping[str, Any], *, path: str, ctx: _Ctx, apply_defaults: bool) -> List[Any]:
        if "minItems" in schema and len(value) < int(schema["minItems"]):
            raise SchemaValidationError(f"{path or '$'}: array length {len(value)} < minItems {schema['minItems']}")
        if "maxItems" in schema and len(value) > int(schema["maxItems"]):
            raise SchemaValidationError(f"{path or '$'}: array length {len(value)} > maxItems {schema['maxItems']}")
        if schema.get("uniqueItems"):
            seen = set()
            for i, item in enumerate(value):
                key = json.dumps(item, sort_keys=True) if isinstance(item, (dict, list)) else item
                if key in seen:
                    raise SchemaValidationError(f"{_path_join(path, str(i))}: duplicate item with uniqueItems=true")
                seen.add(key)
        items = schema.get("items")
        out = value if not apply_defaults else list(value)
        if isinstance(items, list):
            for i, (item, sub) in enumerate(zip(value, items)):
                norm = self._validate_node(item, sub, path=_path_join(path, str(i)), ctx=ctx, apply_defaults=apply_defaults)
                if apply_defaults:
                    out[i] = norm
            # Additional items not allowed unless additionalItems (deprecated) or no constraint
            if len(value) > len(items):
                raise SchemaValidationError(f"{path or '$'}: additional array items beyond defined tuple schema")
        elif isinstance(items, Mapping):
            for i, item in enumerate(value):
                norm = self._validate_node(item, items, path=_path_join(path, str(i)), ctx=ctx, apply_defaults=apply_defaults)
                if apply_defaults:
                    out[i] = norm
        return out

    def _validate_object(self, value: Mapping[str, Any], schema: Mapping[str, Any], *, path: str, ctx: _Ctx, apply_defaults: bool) -> Dict[str, Any]:
        props: Mapping[str, Any] = schema.get("properties", {}) if isinstance(schema.get("properties"), Mapping) else {}
        required: List[str] = list(schema.get("required", []))
        addl = schema.get("additionalProperties", self._allow_additional) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==lambdas:[38:155]
==repo_core_sizing_lambdas:[45:162]
@dataclass
class LambdaCalConfig:
    ece_target: float = 0.02   # acceptable ECE
    ll_target: float = 0.70    # acceptable LogLoss (binary)
    eta: float = 12.0          # penalty weight for ECE above target
    zeta: float = 6.0          # penalty weight for LogLoss above target


def lambda_cal(metrics: ProbabilityMetrics, cfg: LambdaCalConfig = LambdaCalConfig()) -> float:
    """Calibration multiplier from ECE & LogLoss (higher → worse → smaller λ).

    λ_cal = exp(-η·max(0, ECE−ECE₀)) · exp(-ζ·max(0, LL−LL₀))

    If metrics are missing, returns 1.0 (no penalty).
    """
    ece = float(metrics.ece) if metrics and metrics.ece is not None else None
    ll = float(metrics.logloss) if metrics and metrics.logloss is not None else None
    if ece is None and ll is None:
        return 1.0
    pen_e = max(0.0, (ece - cfg.ece_target)) if ece is not None else 0.0
    pen_l = max(0.0, (ll - cfg.ll_target)) if ll is not None else 0.0
    lam = math.exp(-cfg.eta * pen_e) * math.exp(-cfg.zeta * pen_l)
    return max(0.0, min(1.0, lam))


@dataclass
class LambdaRegConfig:
    allowed: tuple = ("trend", "grind")  # tradeable regimes
    off_regime_penalty: float = 0.2       # λ when regime not allowed but tradeable


def lambda_reg(*, tradeable: bool, regime: Optional[str], cfg: LambdaRegConfig = LambdaRegConfig()) -> float:
    """Regime multiplier.

    - If not tradeable: 0.
    - If regime ∈ allowed: 1.
    - Else: off_regime_penalty (e.g., 0.2).
    """
    if not tradeable:
        return 0.0
    if regime is None:
        return 1.0
    return 1.0 if regime in cfg.allowed else max(0.0, min(1.0, cfg.off_regime_penalty))


@dataclass
class LambdaLiqConfig:
    spread_ref_bps: float = 2.0   # above this, penalize
    ttd_ref_s: float = 0.5        # below this, penalize (queue likely to deplete fast)
    a_spread: float = 0.25        # penalty slope for spread excess
    b_ttd: float = 1.50           # penalty slope for TTD shortfall (relative to ref)


def lambda_liq(*, spread_bps: Optional[float], ttd_s: Optional[float], cfg: LambdaLiqConfig = LambdaLiqConfig()) -> float:
    """Liquidity multiplier from spread & time-to-depletion (best-quote).

    λ_liq = exp(-a·max(0, spread−s₀)) · exp(-b·max(0, (t₀−TTD)/t₀))
    """
    s = None if spread_bps is None else max(0.0, float(spread_bps))
    ttd = None if ttd_s is None else max(0.0, float(ttd_s))
    pen_s = 0.0 if s is None else max(0.0, s - cfg.spread_ref_bps)
    pen_t = 0.0 if ttd is None else max(0.0, (cfg.ttd_ref_s - ttd) / max(1e-9, cfg.ttd_ref_s))
    lam = math.exp(-cfg.a_spread * pen_s) * math.exp(-cfg.b_ttd * pen_t)
    return max(0.0, min(1.0, lam))


@dataclass
class LambdaDDConfig:
    gamma: float = 1.5  # curvature for drawdown penalty


def lambda_dd(*, dd_ratio: float, cfg: LambdaDDConfig = LambdaDDConfig()) -> float:
    """Drawdown multiplier, dd_ratio = current_DD / DD_limit.

    λ_dd = max(0, 1 − dd_ratio)^γ.
    """
    x = max(0.0, float(dd_ratio))
    base = max(0.0, 1.0 - x)
    return max(0.0, min(1.0, base ** cfg.gamma))


@dataclass
class LambdaLatConfig:
    alpha: float = 3.0  # slope for SLA overrun in relative terms


def lambda_lat(*, latency_ms: float, sla_ms: float, cfg: LambdaLatConfig = LambdaLatConfig()) -> float:
    """Latency multiplier from SLA (lower is better).

    over = max(0, (ℓ−SLA)/SLA). λ_lat = exp(−α·over).
    """
    SLA = max(1e-9, float(sla_ms))
    over = max(0.0, float(latency_ms) - SLA) / SLA
    return max(0.0, min(1.0, math.exp(-cfg.alpha * over)))


# =============================
# Combiners & Policy wrapper
# =============================

def combine_lambdas(d: Mapping[str, float]) -> float:
    prod = 1.0
    for k, v in d.items():
        try:
            x = float(v)
        except Exception:
            x = 1.0
        if x < 0.0:
            x = 0.0
        if x > 1.0:
            x = 1.0
        prod *= x
    return max(0.0, min(1.0, prod))


@dataclass
class LambdaPolicy: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==icp:[247:336]
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[246:335]
        iso = _Iso(xs=xs, ys=ys)
        iso.fit()
        return max(0.0, min(1.0, iso.predict(float(s_new))))

    def predict_interval(self, score_new: float) -> Tuple[float, float]:
        # map prob to logit if necessary (mirror of fit)
        s_new = float(score_new)
        if 0.0 <= s_new <= 1.0:
            p = min(1 - 1e-9, max(1e-9, s_new))
            s_new = math.log(p / (1 - p))
        p0 = self._calibrate_with_added(s_new, 0)
        p1 = self._calibrate_with_added(s_new, 1)
        lo = min(p0, p1)
        hi = max(p0, p1)
        return lo, hi


# =============================
# Self-tests (synthetic)
# =============================

def _make_scores(n: int = 1200, seed: int = 11) -> Tuple[List[float], List[int]]:
    rnd = random.Random(seed)
    scores: List[float] = []
    y: List[int] = []
    for i in range(n):
        # latent probability depends on score via sigmoid with mild miscalibration
        s = rnd.uniform(-2.5, 2.5)
        p = 1 / (1 + math.exp(-(s + 0.3)))  # shift +0.3 to miscalibrate
        scores.append(s)
        y.append(1 if rnd.random() < p else 0)
    return scores, y


def _test_split_conformal_coverage() -> None:
    s, y = _make_scores()
    # pretend we only have calibrated probabilities from a model: map via sigmoid (miscalibrated)
    p_hat = [1 / (1 + math.exp(-si)) for si in s]
    # use 50% for calibration
    n = len(y)
    idx = n // 2
    cal = SplitConformalBinary(alpha=0.1)
    cal.fit(p_hat[:idx], y[:idx])
    # evaluate coverage on holdout: probability that true label ∈ prediction set
    cover = 0
    m = 0
    for pi, yi in zip(p_hat[idx:], y[idx:]):
        S = cal.predict_set(pi)
        cover += 1 if yi in S else 0
        m += 1
    cov_rate = cover / max(1, m)
    # target coverage ≥ 1−α ≈ 0.9, allow small randomness tolerance
    assert cov_rate >= 0.85


def _test_mondrian_grouping() -> None:
    s, y = _make_scores(seed=21)
    p_hat = [1 / (1 + math.exp(-si)) for si in s]
    # define groups by coarse score bins
    groups = ["low" if si < -0.5 else "mid" if si < 0.5 else "high" for si in s]
    n = len(y)
    idx = n // 2
    mon = MondrianConformalBinary(alpha=0.1)
    mon.fit(p_hat[:idx], y[:idx], groups[:idx])
    # check p-values bounded in [0,1] and sets non-empty most of the time
    cnt_nonempty = 0
    for pi, gi in zip(p_hat[idx:], groups[idx:]):
        p1, p0 = mon.p_values(pi, gi)
        assert 0.0 <= p1 <= 1.0 and 0.0 <= p0 <= 1.0
        if mon.predict_set(pi, gi):
            cnt_nonempty += 1
    assert cnt_nonempty >= len(p_hat[idx:]) * 0.7


def _test_venn_abers_interval() -> None:
    s, y = _make_scores(seed=33)
    va = VennAbersBinary()
    va.fit(s[:600], y[:600])
    lo, hi = va.predict_interval(s[601])
    assert 0.0 <= lo <= hi <= 1.0
    # interval shouldn't be absurdly wide on typical scores
    assert (hi - lo) <= 0.5


if __name__ == "__main__":
    _test_split_conformal_coverage()
    _test_mondrian_grouping()
    _test_venn_abers_interval()
    print("OK - repo/core/calibration/icp.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==cvar:[128:223]
==repo_core_risk_cvar:[73:168]
@dataclass
class _Item:
    loss: float
    uid: int


class RollingCVaR:
    """Rolling VaR/CVaR over the last N samples using a sorted multiset.

    Operations: update(loss) in O(log N); metrics(): (VaRα, CVaRα).
    """

    def __init__(self, window_n: int = 2000, alpha: float = 0.99) -> None:
        self.N = int(window_n)
        self.alpha = float(alpha)
        self.q: Deque[_Item] = deque()
        self.sorted: List[Tuple[float, int]] = []  # (loss, uid)
        self._uid = 0
        self._sum = 0.0

    def update(self, loss: float) -> None:
        l = max(0.0, float(loss))
        it = _Item(l, self._uid)
        self._uid += 1
        # append to deque
        self.q.append(it)
        self._sum += l
        # insert into sorted
        bisect.insort(self.sorted, (it.loss, it.uid))
        # evict
        while len(self.q) > self.N:
            old = self.q.popleft()
            self._sum -= old.loss
            k = bisect.bisect_left(self.sorted, (old.loss, old.uid))
            if 0 <= k < len(self.sorted) and self.sorted[k] == (old.loss, old.uid):
                self.sorted.pop(k)
            else:  # fallback linear search (should rarely happen)
                for j in range(max(0, k - 3), min(len(self.sorted), k + 4)):
                    if self.sorted[j] == (old.loss, old.uid):
                        self.sorted.pop(j)
                        break

    def metrics(self) -> Tuple[float, float]:
        if not self.sorted:
            return 0.0, 0.0
        var = _quantile([v for (v, _) in self.sorted], self.alpha)
        # tail sum via index
        n = len(self.sorted)
        k = int(math.ceil(self.alpha * n) - 1)
        k = max(0, min(n - 1, k))
        tail_vals = [self.sorted[i][0] for i in range(k, n)]
        cvar = sum(tail_vals) / max(1, len(tail_vals))
        return var, cvar


# =============================
# Portfolio CVaR
# =============================

def portfolio_cvar(weights: Sequence[float], returns: Sequence[Sequence[float]], alpha: float = 0.99) -> Tuple[float, float]:
    """Portfolio VaR/CVaR from scenario **returns** (PnL), using losses L = −R·w.

    - `weights`: portfolio weights (can be long/short). No normalization enforced here.
    - `returns`: list of scenarios, each a vector of asset returns.
    - Returns (VaRα, CVaRα) in **loss** units (same as returns magnitudes).
    """
    w = [float(x) for x in weights]
    if np is not None:
        R = np.array(returns, dtype=float)
        wv = np.array(w, dtype=float)
        pnl = R.dot(wv)  # shape: (#scen,)
        losses = [max(0.0, -float(x)) for x in pnl.tolist()]
    else:
        losses = []
        for scen in returns:
            pnl = sum(float(a) * float(b) for a, b in zip(scen, w))
            losses.append(max(0.0, -pnl))
    return var_cvar_from_losses(losses, alpha)


# =============================
# CVaR-Min optimization (projected subgradient)
# =============================

def _proj_simplex(v: Sequence[float], z: float = 1.0) -> List[float]:
    """Project vector onto the probability simplex {w ≥ 0, ∑w = z}."""
    x = [max(0.0, float(t)) for t in v]
    s = sum(x)
    if s == 0:
        n = len(x)
        return [z / n] * n
    # shift by tau so that sum=max(0, x - tau) = z
    u = sorted(x, reverse=True)
    css = 0.0
    rho = -1 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==evt_pot:[201:274]
==repo_core_risk_evt_pot_py_pot_gpd_over_threshold_u_tail_va_r_es_bootstrap_ci_self_tests:[193:266]
            "u": est.u,
            "xi": est.xi,
            "beta": est.beta,
            "zeta": est.zeta,
            "n_exc": float(est.n_exc),
            "n_total": float(est.n_total),
            "var_p": var_p,
            "es_p": es_p,
        }
        if with_bootstrap:
            ci = pot_var_bootstrap(L, p, q_u=self.q_u, n_boot=n_boot)
            out.update({"var_lo": ci["lo"], "var_hi": ci["hi"]})
        return out


# =============================
# Self-tests
# =============================

def _gpd_sample(n: int, xi: float, beta: float, seed: int = 3) -> List[float]:
    rnd = random.Random(seed)
    out = []
    for _ in range(n):
        u = rnd.random()
        if abs(xi) < 1e-12:
            y = -beta * math.log(1 - u)
        else:
            y = beta / xi * ((1 - u) ** (-xi) - 1)
        out.append(y)
    return out


def _make_losses(n: int = 5000, seed: int = 5) -> List[float]:
    rnd = random.Random(seed)
    base = [max(0.0, rnd.expovariate(1.5)) for _ in range(n)]  # light-tail base
    # inject heavy-tail components
    heavy = _gpd_sample(n // 5, xi=0.3, beta=1.0, seed=seed + 17)
    # place heavy-tail samples randomly in the series
    for y in heavy:
        idx = rnd.randrange(0, n)
        base[idx] += 1.0 + y
    return base


def _test_fit_and_quantiles() -> None:
    L = _make_losses(n=4000, seed=11)
    est = pot_fit(L, q_u=0.9)
    v99, e99 = pot_var_es(est, 0.99)
    # sanity: VaR well above threshold; ES >= VaR
    assert v99 >= est.u
    assert e99 >= v99


def _test_bootstrap_ci() -> None:
    L = _make_losses(n=3000, seed=21)
    out = pot_var_bootstrap(L, p=0.995, q_u=0.9, n_boot=80)
    assert out["hi"] >= out["var"] >= out["lo"]


def _test_rolling() -> None:
    L = _make_losses(n=2500, seed=33)
    rp = RollingPOT(window_n=1000, q_u=0.9)
    for x in L:
        rp.add(x)
    rep = rp.report(p=0.995, with_bootstrap=True, n_boot=50)
    assert rep["var_p"] >= rep["u"] and rep["es_p"] >= rep["var_p"]


if __name__ == "__main__":
    _test_fit_and_quantiles()
    _test_bootstrap_ci()
    _test_rolling()
    print("OK - repo/core/risk/evt_pot.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.universe.hysteresis:[29:91]
==repo_core_universe_hysteresis:[29:91]
@dataclass
class HState:
    active: bool
    changed: bool
    score: float
    ticks_since_change: int


class Hysteresis:
    def __init__(self, *, add_thresh: float, drop_thresh: float, min_dwell: int = 0, start_active: bool = False) -> None:
        if drop_thresh > add_thresh:
            raise ValueError("require drop_thresh <= add_thresh")
        self.add = float(add_thresh)
        self.drop = float(drop_thresh)
        self.min_dwell = int(max(0, min_dwell))
        self.active = bool(start_active)
        self._ticks = 0
        self._score = 0.0

    def reset(self, *, start_active: Optional[bool] = None) -> None:
        if start_active is not None:
            self.active = bool(start_active)
        self._ticks = 0
        self._score = 0.0

    def update(self, score: float) -> HState:
        self._score = float(score)
        self._ticks += 1
        changed = False
        if self.active:
            if self._ticks >= self.min_dwell and self._score <= self.drop:
                self.active = False
                self._ticks = 0
                changed = True
        else:
            if self._ticks >= self.min_dwell and self._score >= self.add:
                self.active = True
                self._ticks = 0
                changed = True
        return HState(active=self.active, changed=changed, score=self._score, ticks_since_change=self._ticks)


class EmaSmoother:
    def __init__(self, *, alpha: float = 0.2, init: Optional[float] = None) -> None:
        if not (0.0 < alpha <= 1.0):
            raise ValueError("alpha in (0,1]")
        self.alpha = float(alpha)
        self._y: Optional[float] = float(init) if init is not None else None

    def update(self, x: float) -> float:
        x = float(x)
        if self._y is None:
            self._y = x
        else:
            self._y = self.alpha * x + (1.0 - self.alpha) * self._y
        return self._y

    def value(self) -> Optional[float]:
        return self._y


__all__ = ["Hysteresis", "HState", "EmaSmoother"] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==microprice:[42:139]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[57:154]
    return 0.5 * (float(bid_price) + float(ask_price))


def microprice_l1(bid_price: float, ask_price: float, bid_q1: float, ask_q1: float) -> float:
    """Classic L1 microprice.
    mp = (ask_q1 * bid_price + bid_q1 * ask_price) / (bid_q1 + ask_q1)
    If denominator is zero, falls back to mid.
    """
    b, a = float(bid_price), float(ask_price)
    qb, qa = max(0.0, float(bid_q1)), max(0.0, float(ask_q1))
    den = qb + qa
    if den <= 0.0:
        return _safe_mid(b, a)
    return (qa * b + qb * a) / den


def _sum_first_k(x: Sequence[float], k: int) -> float:
    return sum(float(v) for v in x[:max(1, k)])


def microprice_lk(
    bid_price: float,
    ask_price: float,
    bid_volumes_l: Sequence[float],
    ask_volumes_l: Sequence[float],
    levels: int = 5,
) -> float:
    """Lk microprice using aggregated volumes as weights.
    mp_k = (Σ_{ask} q_a · bid_price + Σ_{bid} q_b · ask_price) / (Σ_{ask} q_a + Σ_{bid} q_b)
    """
    b, a = float(bid_price), float(ask_price)
    qb = _sum_first_k(bid_volumes_l, levels)
    qa = _sum_first_k(ask_volumes_l, levels)
    den = qb + qa
    if den <= 0.0:
        return _safe_mid(b, a)
    return (qa * b + qb * a) / den


def micro_premium_bps(mid: float, microprice: float) -> float:
    """Premium of microprice over mid, in basis points."""
    m, mp = float(mid), float(microprice)
    if m <= 0.0:
        return 0.0
    return 1e4 * (mp - m) / m


# =============================
# Streaming wrapper
# =============================

class MicropriceStream:
    """Convenience streaming extractor.

    Parameters
    ----------
    levels : int
        Depth levels to aggregate for Lk microprice (k≥1).
    """

    def __init__(self, levels: int = 5) -> None:
        self.levels = max(1, int(levels))

    def update(self, snap: MarketSnapshot) -> Dict[str, float]:
        k = self.levels
        qb1 = float(snap.bid_volumes_l[0]) if snap.bid_volumes_l else 0.0
        qa1 = float(snap.ask_volumes_l[0]) if snap.ask_volumes_l else 0.0
        mp1 = microprice_l1(snap.bid_price, snap.ask_price, qb1, qa1)
        mpk = microprice_lk(snap.bid_price, snap.ask_price, snap.bid_volumes_l, snap.ask_volumes_l, k)
        feats = {
            "mid": snap.mid,
            "spread": snap.spread,
            "spread_bps": snap.spread_bps(),
            "microprice_l1": mp1,
            "microprice_lk": mpk,
            "micro_premium_l1_bps": micro_premium_bps(snap.mid, mp1),
            "micro_premium_lk_bps": micro_premium_bps(snap.mid, mpk),
        }
        return feats


# =============================
# Self-tests
# =============================

def _mock_snapseq() -> List[MarketSnapshot]:
    t0 = time.time()
    snaps: List[MarketSnapshot] = []
    bid, ask = 100.00, 100.02
    qb1, qa1 = 500.0, 520.0
    for i in range(30):
        ts = t0 + 0.1 * i
        # vary best sizes to move microprice around mid
        qb1 = max(50.0, qb1 + (35.0 if i % 3 == 0 else -18.0))
        qa1 = max(50.0, qa1 + (-28.0 if i % 4 == 0 else 12.0))
        # sometimes tighten/widen ask
        if i % 5 == 0: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[229:291]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[229:290]
@dataclass
class TemperatureScaler:
    """Temperature scaling for probabilities via logits: q = σ(logit(p)/T)."""
    T: float = 1.0
    max_iter: int = 100
    tol: float = 1e-9

    def fit(self, p: Sequence[float], y: Sequence[int]) -> "TemperatureScaler":
        # Use Newton steps with backtracking; ensure T>0.
        T = max(1e-3, float(self.T))
        x = [_logit(pi) for pi in p]
        def nll(temp: float) -> float:
            ssum = 0.0
            for xi, yi in zip(x, y):
                z = xi / temp
                qi = _sigmoid(z)
                ssum += - (yi * math.log(_clip01(qi)) + (1 - yi) * math.log(_clip01(1.0 - qi)))
            return ssum
        for _ in range(self.max_iter):
            g = 0.0
            h = 0.0
            for xi, yi in zip(x, y):
                z = xi / T
                q = _sigmoid(z)
                g += - (q - yi) * (xi / (T * T))
                h += q * (1.0 - q) * (xi * xi) / (T ** 4) + 2.0 * (q - yi) * (xi) / (T ** 3)
            step = - g / max(1e-12, h)
            # backtracking line search
            base = nll(T)
            t = 1.0
            while t > 1e-6:
                T_new = max(1e-3, T + t * step)
                if nll(T_new) <= base:
                    T = T_new
                    break
                t *= 0.5
            if abs(step) < self.tol:
                break
        self.T = float(T)
        return self

    def predict_proba(self, p: Sequence[float]) -> List[float]:
        return [_sigmoid(_logit(pi) / self.T) for pi in p]


@dataclass
class IsotonicCalibrator:
    """Isotonic regression (PAV) mapping x→p, where x∈R is score or uncali. prob.

    Implementation: pool-adjacent-violators with unit weights. For prediction,
    we perform step-function lookup with linear interpolation between knots for
    smoother behavior.
    """
    xs_: List[float] = None  # type: ignore
    ys_: List[float] = None  # type: ignore

    def fit(self, x: Sequence[float], y: Sequence[int]) -> "IsotonicCalibrator":
        pairs = sorted((float(xi), int(yi)) for xi, yi in zip(x, y))
        xs = [xi for xi, _ in pairs]
        ys = [float(yi) for _, yi in pairs]
        # Pool Adjacent Violators (unit weights) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.obi:[43:125]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[59:141]
    k = max(1, int(levels))
    b = sum(float(x) for x in bid_volumes_l[:k])
    a = sum(float(x) for x in ask_volumes_l[:k])
    return b, a


def depth_ratio(bid_volumes_l: Sequence[float], ask_volumes_l: Sequence[float], levels: int = 5) -> float:
    b, a = depth_sums(bid_volumes_l, ask_volumes_l, levels)
    den = b + a
    return 0.0 if den == 0.0 else b / den


def obi_l1(bid_volumes_l: Sequence[float], ask_volumes_l: Sequence[float]) -> float:
    """L1-OBI = (q_b1 − q_a1) / (q_b1 + q_a1)."""
    qb = float(bid_volumes_l[0]) if bid_volumes_l else 0.0
    qa = float(ask_volumes_l[0]) if ask_volumes_l else 0.0
    den = qb + qa
    return 0.0 if den == 0.0 else (qb - qa) / den


def obi_lk(bid_volumes_l: Sequence[float], ask_volumes_l: Sequence[float], levels: int = 5) -> float:
    """Lk-OBI over first k levels: (Σ q_b − Σ q_a) / (Σ q_b + Σ q_a)."""
    b, a = depth_sums(bid_volumes_l, ask_volumes_l, levels)
    den = b + a
    return 0.0 if den == 0.0 else (b - a) / den


def spread_bps(bid_price: float, ask_price: float) -> float:
    mid = 0.5 * (float(bid_price) + float(ask_price))
    spr = float(ask_price) - float(bid_price)
    return 0.0 if mid <= 0 else 1e4 * spr / mid


# =============================
# Streaming wrapper (optional)
# =============================

class OBIStream:
    """Convenience streaming extractor returning a compact feature dict per snapshot.

    Example
    -------
    >>> obi = OBIStream(levels=5)
    >>> feats = obi.update(snap)
    >>> feats["obi_l1"], feats["obi_lk"], feats["depth_ratio"]
    """

    def __init__(self, levels: int = 5) -> None:
        self.levels = max(1, int(levels))

    def update(self, snap: MarketSnapshot) -> Dict[str, float]:
        k = self.levels
        b, a = depth_sums(snap.bid_volumes_l, snap.ask_volumes_l, k)
        feats = {
            "mid": snap.mid,
            "spread": snap.spread,
            "spread_bps": snap.spread_bps(),
            "depth_bid_lk": b,
            "depth_ask_lk": a,
            "depth_ratio": depth_ratio(snap.bid_volumes_l, snap.ask_volumes_l, k),
            "obi_l1": obi_l1(snap.bid_volumes_l, snap.ask_volumes_l),
            "obi_lk": obi_lk(snap.bid_volumes_l, snap.ask_volumes_l, k),
        }
        return feats


# =============================
# Self-tests
# =============================

def _mock_snapseq() -> List[MarketSnapshot]:
    t0 = time.time()
    snaps: List[MarketSnapshot] = []
    bid, ask = 100.00, 100.02
    qb1, qa1 = 500.0, 520.0
    for i in range(20):
        ts = t0 + 0.1 * i
        # oscillate best sizes to cause OBI swings
        qb1 = max(50.0, qb1 + (30.0 if i % 3 == 0 else -15.0))
        qa1 = max(50.0, qa1 + (-25.0 if i % 4 == 0 else 10.0))
        # tweak ask to vary spread a bit
        if i % 5 == 0: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==lambdas:[172:237]
==repo_core_sizing_lambdas:[179:244]
        l_reg = lambda_reg(tradeable=tradeable, regime=regime, cfg=self.reg)
        l_liq = lambda_liq(spread_bps=spread_bps, ttd_s=ttd_s, cfg=self.liq)
        l_dd = lambda_dd(dd_ratio=dd_ratio, cfg=self.dd)
        l_lat = lambda_lat(latency_ms=latency_ms, sla_ms=sla_ms, cfg=self.lat)
        d = {"cal": l_cal, "reg": l_reg, "liq": l_liq, "dd": l_dd, "lat": l_lat}
        d["lambda_product"] = combine_lambdas(d)
        return d


# =============================
# Self-tests
# =============================

def _test_lambda_cal_monotone() -> None:
    m_good = ProbabilityMetrics(ece=0.01, logloss=0.5)
    m_bad = ProbabilityMetrics(ece=0.08, logloss=1.0)
    lc = lambda_cal(m_good)
    lb = lambda_cal(m_bad)
    assert 0.0 <= lb <= lc <= 1.0


def _test_lambda_reg() -> None:
    assert lambda_reg(tradeable=False, regime="trend") == 0.0
    assert lambda_reg(tradeable=True, regime="trend") == 1.0
    assert 0.0 <= lambda_reg(tradeable=True, regime="chaos") <= 1.0


def _test_lambda_liq_monotone() -> None:
    # worse spread and smaller TTD → smaller λ
    l1 = lambda_liq(spread_bps=1.5, ttd_s=1.0)
    l2 = lambda_liq(spread_bps=5.0, ttd_s=0.2)
    assert 0.0 <= l2 <= l1 <= 1.0


def _test_lambda_dd_monotone() -> None:
    l0 = lambda_dd(dd_ratio=0.0)
    l5 = lambda_dd(dd_ratio=0.5)
    l9 = lambda_dd(dd_ratio=0.9)
    assert 1.0 >= l0 >= l5 >= l9 >= 0.0


def _test_lambda_lat_monotone() -> None:
    l_ok = lambda_lat(latency_ms=8.0, sla_ms=10.0)
    l_bad = lambda_lat(latency_ms=25.0, sla_ms=10.0)
    assert 0.0 <= l_bad <= l_ok <= 1.0


def _test_policy_product() -> None:
    pol = LambdaPolicy()
    d = pol.compute(metrics=ProbabilityMetrics(ece=0.03, logloss=0.8),
                    tradeable=True, regime="trend",
                    spread_bps=2.5, ttd_s=0.4,
                    dd_ratio=0.3,
                    latency_ms=12.0, sla_ms=10.0)
    assert 0.0 <= d["lambda_product"] <= 1.0 and all(0.0 <= v <= 1.0 for k, v in d.items() if k != "lambda_product")


if __name__ == "__main__":
    _test_lambda_cal_monotone()
    _test_lambda_reg()
    _test_lambda_liq_monotone()
    _test_lambda_dd_monotone()
    _test_lambda_lat_monotone()
    _test_policy_product()
    print("OK - repo/core/sizing/lambdas.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[196:248]
==repo_core_features_tfi:[195:247]
    random.seed(seed)
    t0 = time.time()
    out: List[Trade] = []
    ts = t0
    for i in range(n):
        # 70% buys, 30% sells, sizes around 10±3
        is_buy = (random.random() < 0.7)
        size = max(0.1, 10.0 + random.gauss(0.0, 3.0))
        ts += max(0.0, random.expovariate(20.0))
        out.append(Trade(timestamp=ts, price=100.0, size=size, side=Side.BUY if is_buy else Side.SELL))
    return out


def _make_trades_balanced(n: int = 200, seed: int = 2) -> List[Trade]:
    import random
    random.seed(seed)
    t0 = time.time()
    out: List[Trade] = []
    ts = t0
    for i in range(n):
        is_buy = (i % 2 == 0)
        size = max(0.1, 10.0 + random.gauss(0.0, 3.0))
        ts += max(0.0, random.expovariate(20.0))
        out.append(Trade(timestamp=ts, price=100.0, size=size, side=Side.BUY if is_buy else Side.SELL))
    return out


def _test_event_time_tfi_vpin() -> None:
    tr = _make_trades_imbalanced()
    tfi = TFIStream(window_s=2.0, bucket_volume=100.0)
    for x in tr:
        tfi.ingest_trade(x)
    feats = tfi.features(now_ts=tr[-1].timestamp)
    assert feats["buy_vol"] > feats["sell_vol"]
    assert feats["tfi"] > 0
    assert 0.0 <= feats["vpin_like"] <= 1.0
    assert 0.0 <= feats["vpin_bucketed"] <= 1.0


def _test_vpin_contrast() -> None:
    tr_imbal = _make_trades_imbalanced(seed=11)
    tr_bal = _make_trades_balanced(seed=22)
    vpin_imbal = vpin_volume_buckets(tr_imbal, bucket_volume=100.0, max_buckets=50)
    vpin_bal = vpin_volume_buckets(tr_bal, bucket_volume=100.0, max_buckets=50)
    # Imbalanced stream should have higher VPIN than balanced
    assert vpin_imbal >= vpin_bal - 1e-6


if __name__ == "__main__":
    _test_event_time_tfi_vpin()
    _test_vpin_contrast()
    print("OK - repo/core/features/tfi.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[85:150]
==repo_core_features_tfi:[124:195]
                imbalances.append(min(1.0, abs(B - S) / V))
                B, S = 0.0, 0.0
                remain = V
    # if last partial bucket exists but not full, ignore it (standard practice)
    if not imbalances:
        return 0.0
    n = min(int(max_buckets), len(imbalances))
    return sum(imbalances[-n:]) / n


# =============================
# Streaming class
# =============================

class TFIStream:
    """Streaming TFI/VPIN extractor on event-time window.

    Parameters
    ----------
    window_s : float
        Event-time horizon for rolling sums (default 5.0 seconds).
    bucket_volume : float
        Volume per VPIN bucket (same units as trade size). If <=0, VPIN buckets
        are disabled and only VPIN-like ratio is returned.
    max_trades : int
        Hard cap on trades retained for VPIN-bucket computation.
    """
    def __init__(self, window_s: float = 5.0, bucket_volume: float = 100.0, max_trades: int = 5000) -> None:
        self.win = _Rolling(window_s)
        self.bucket_volume = float(bucket_volume)
        self.max_trades = int(max_trades)
        self._trades: Deque[Trade] = deque()

    def ingest_trade(self, tr: Trade) -> None:
        ts = float(tr.timestamp)
        buy = float(tr.size) if str(tr.side) == "Side.BUY" or str(tr.side) == "BUY" else 0.0
        sell = float(tr.size) if str(tr.side) == "Side.SELL" or str(tr.side) == "SELL" else 0.0
        self.win.add(ts, buy=buy, sell=sell)
        # store for VPIN-bucket (cap by count and evict by time horizon generously)
        self._trades.append(tr)
        while len(self._trades) > self.max_trades:
            self._trades.popleft()
        # also time-based cleanup to keep fresh
        cutoff = ts - 10.0 * self.win.h  # keep at most 10×window for bucket VPIN context
        while self._trades and float(self._trades[0].timestamp) < cutoff:
            self._trades.popleft()

    def features(self, now_ts: Optional[float] = None) -> Dict[str, float]:
        if now_ts is None:
            now_ts = time.time()
        b, s = self.win.sums(now_ts)
        tfi = b - s
        feats = {
            "buy_vol": b,
            "sell_vol": s,
            "tfi": tfi,
            "vpin_like": vpin_like(b, s),
        }
        if self.bucket_volume > 0.0 and self._trades:
            feats["vpin_bucketed"] = vpin_volume_buckets(list(self._trades), self.bucket_volume, max_buckets=50)
        else:
            feats["vpin_bucketed"] = 0.0
        return feats

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[93:120]
==test_shadow_broker_old:[411:440]
        mock_response.json.return_value = {
            "symbols": [{
                "symbol": "BTCUSDT",
                "status": "TRADING",
                "filters": [
                    {
                        "filterType": "LOT_SIZE",
                        "minQty": "0.00001000",
                        "maxQty": "9000.00000000",
                        "stepSize": "0.00001000"
                    },
                    {
                        "filterType": "PRICE_FILTER",
                        "minPrice": "0.01000000",
                        "maxPrice": "1000000.00000000",
                        "tickSize": "0.01000000"
                    },
                    {
                        "filterType": "MIN_NOTIONAL",
                        "minNotional": "10.00000000"
                    }
                ]
            }]
        }
        mock_get.return_value = mock_response

        broker = ShadowBroker(symbols=["BTCUSDT"])

        # Check that filters were loaded (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.signal.leadlag_hy:[28:63]
==features_microstructure:[27:62]
try:  # Optional; used only for pretty array ops if present
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from core/types; fallback if unavailable ---------
try:  # pragma: no cover - exercised in integration, not in unit self-tests
    from aurora.core.types import (
        Trade, MarketSnapshot, Side,
    )
except Exception:  # Minimal fallbacks to run this file standalone
    class Side(str, Enum):
        BUY = "BUY"
        SELL = "SELL"

    @dataclass
    class Trade:
        timestamp: float
        price: float
        size: float
        side: Side

    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float]
        trades: Sequence[Trade]

        @property
        def mid(self) -> float:
            return 0.5 * (self.bid_price + self.ask_price)
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[327:360]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[309:345]
        return self

    def predict_proba(self, x: Sequence[float]) -> List[float]:
        if not self.xs_:
            return [0.5] * len(list(x))
        res: List[float] = []
        for xi in x:
            xi = float(xi)
            # find insertion point
            lo, hi = 0, len(self.xs_) - 1
            if xi <= self.xs_[0]:
                res.append(self.ys_[0])
                continue
            if xi >= self.xs_[-1]:
                res.append(self.ys_[-1])
                continue
            # binary search
            while lo <= hi:
                mid = (lo + hi) // 2
                if self.xs_[mid] <= xi:
                    lo = mid + 1
                else:
                    hi = mid - 1
            j = max(1, lo - 1)
            x0, x1 = self.xs_[j - 1], self.xs_[j]
            y0, y1 = self.ys_[j - 1], self.ys_[j]
            # linear interpolation between knots
            t = 0.0 if x1 == x0 else (xi - x0) / (x1 - x0)
            res.append((1 - t) * y0 + t * y1)
        return [min(1.0, max(0.0, r)) for r in res]

    def calibrate_prob(self, p_raw: float) -> float:
        """Calibrate a single probability using isotonic regression.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==icp:[197:246]
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[196:245]
                x_lo = x[idx]
                x_hi = x[min(len(x) - 1, idx + int(wt) - 1)]
                blocks.append((x_lo, x_hi, val))
                idx += int(wt)
        self.blocks = blocks

    def predict(self, xq: float) -> float:
        if not hasattr(self, "blocks") or not self.blocks:
            return 0.5
        xq = float(xq)
        # find block
        for lo, hi, val in self.blocks:
            if xq <= hi:
                if xq < lo:
                    # interpolate with previous block if exists
                    return val
                return val
        return self.blocks[-1][2]


# =============================
# Venn–Abers (binary, simple refit per query)
# =============================

@dataclass
class VennAbersBinary:
    """Venn–Abers via isotonic refits with the new point labelled 0 and 1.

    API:
      - fit(scores, y) where `scores` are *monotone* scores (e.g., logits or raw model scores
        increasing with P(y=1)). If you only have probabilities p, you may pass
        scores = logit(p) (guarded inside).
      - predict_interval(score_new) → (p_low, p_high)
    """
    def fit(self, scores: Sequence[float], y: Sequence[int]) -> None:
        assert len(scores) == len(y)
        self.s = [float(x) for x in scores]
        self.y = [1 if int(t) == 1 else 0 for t in y]
        # if scores are probabilities, map to logits to improve monotonicity spacing
        # guard at 0/1 boundaries
        def to_logit(p: float) -> float:
            p = min(1 - 1e-9, max(1e-9, float(p)))
            return math.log(p / (1 - p))
        # decide if looks like probs
        if all(0.0 <= x <= 1.0 for x in self.s):
            self.s = [to_logit(x) for x in self.s]

    def _calibrate_with_added(self, s_new: float, y_new: int) -> float:
        xs = self.s + [float(s_new)] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[367:413]
==repo_core_calibration_calibrator:[261:304]
class ProbabilityCalibrator:
    """Facade over Platt/Isotonic with a unified API.

    method: 'platt' or 'isotonic'
    input: expects probabilities p_raw \in (0,1)
    """

    def __init__(self, method: str = "isotonic") -> None:
        m = method.lower()
        if m not in ("platt", "isotonic"):
            raise ValueError("unknown calibration method: " + method)
        self._method = m
        self._platt: Optional[PlattCalibrator] = None
        self._iso: Optional[IsotonicCalibrator] = None

    def fit(self, p_raw: Sequence[float], y: Sequence[int]) -> None:
        if self._method == "platt":
            self._platt = PlattCalibrator()
            self._platt.fit(p_raw, y)
        else:
            self._iso = IsotonicCalibrator()
            self._iso.fit(p_raw, y)

    def calibrate_prob(self, p_raw: float) -> float:
        if self._method == "platt":
            if self._platt is None:
                raise ValueError("Platt calibrator not fitted")
            return self._platt.calibrate_prob(p_raw)
        else:
            if self._iso is None:
                raise ValueError("Isotonic calibrator not fitted")
            return self._iso.calibrate_prob(p_raw)

    # Aliases for compatibility with score.py
    def transform(self, p_raw: float) -> float:
        return self.calibrate_prob(p_raw)

    def predict_proba(self, p_raw: float) -> float:
        return self.calibrate_prob(p_raw)


# -------------------- Prequential metrics --------------------
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==microprice:[154:190]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[168:204]
    return snaps


def _test_microprice_bounds() -> None:
    # microprice must lie in [bid, ask]
    b, a = 100.00, 100.02
    mp1 = microprice_l1(b, a, 500, 520)
    assert b <= mp1 <= a
    mpk = microprice_lk(b, a, [500, 400, 300], [520, 380, 280], 3)
    assert b <= mpk <= a


def _test_stream_last_values() -> None:
    seq = _mock_snapseq()
    ms = MicropriceStream(levels=5)
    last: Dict[str, float] = {}
    for s in seq:
        last = ms.update(s)
    assert "microprice_l1" in last and "microprice_lk" in last
    # premiums should be finite and typically small in bps
    assert abs(last["micro_premium_l1_bps"]) < 100.0
    assert abs(last["micro_premium_lk_bps"]) < 100.0


def _test_invariance_zero_den() -> None:
    # if both sides zero, fallback to mid
    b, a = 100.00, 100.02
    mp = microprice_l1(b, a, 0.0, 0.0)
    assert abs(mp - _safe_mid(b, a)) < 1e-12


if __name__ == "__main__":
    _test_microprice_bounds()
    _test_stream_last_values()
    _test_invariance_zero_den()
    print("OK - repo/core/features/microprice.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_basic_endpoints:[9:47]
==test_basic_endpoints_new:[10:48]
    os.environ['AURORA_API_TOKEN'] = 'test_token_12345678901234567890'
    os.environ['AURORA_IP_ALLOWLIST'] = '127.0.0.1'
    os.chdir(tmp_path)

    import api.service as svc
    importlib.reload(svc)
    return TestClient(svc.app)


def setup_app_state(client):
    """Setup minimal app state for testing"""
    app = client.app

    # Initialize basic state attributes
    if not hasattr(app.state, 'cfg'):
        app.state.cfg = {'test': 'config'}
    if not hasattr(app.state, 'trading_system'):
        app.state.trading_system = None
    if not hasattr(app.state, 'governance'):
        from aurora.governance import Governance
        app.state.governance = Governance()
    if not hasattr(app.state, 'events_emitter'):
        app.state.events_emitter = MagicMock()
    if not hasattr(app.state, 'last_event_ts'):
        app.state.last_event_ts = None
    if not hasattr(app.state, 'session_dir'):
        from pathlib import Path
        app.state.session_dir = Path('logs')

    return app


class TestBasicEndpoints:
    """Test basic API endpoints that don't require complex setup"""

    def test_root_endpoint_redirects_to_docs(self, tmp_path):
        """Test root endpoint redirects to /docs"""
        client = make_client(tmp_path) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[24:57]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[24:59]
try:  # optional pretty array ops only; core logic does not require NumPy
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from aurora core types; fallback if not present -------
try:  # pragma: no cover - used during integration
    from aurora.core.types import MarketSnapshot
except Exception:
    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float]
        # minimal helpers to mirror the real class
        @property
        def mid(self) -> float:
            return 0.5 * (float(self.bid_price) + float(self.ask_price))
        @property
        def spread(self) -> float:
            return float(self.ask_price) - float(self.bid_price)
        def spread_bps(self) -> float:
            m = self.mid
            return 0.0 if m <= 0 else 1e4 * self.spread / m

# ------------------------------------------------------------------------------

# =============================
# Pure feature functions
# =============================

def depth_sums(bid_volumes_l: Sequence[float], ask_volumes_l: Sequence[float], levels: int = 5) -> Tuple[float, float]:
    """Sum of depths on bid/ask over first k levels. Levels>len(list) → clamp.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_signal:[64:99]
==tools.mutation_test_standalone:[58:96]
    calculator = CrossAssetHY()
    assert calculator is not None


def test_add_tick():
    """Test adding tick data"""
    calculator = CrossAssetHY()
    calculator.add_tick("SOL", 1000.0, 50.0)
    # Should not raise exception
    assert True


# Simple comparison tests for mutation testing
def test_simple_comparisons():
    """Simple tests that mutation testing can work with"""
    x = 5
    y = 10

    # These comparisons will be mutated by our simple mutator
    assert x < y
    assert x != y
    assert y > x
    assert x <= 5
    assert y >= 10


def test_boolean_logic():
    """Boolean logic tests for mutation testing"""
    a = True
    b = False

    # These will be mutated (and/or operations)
    assert a and not b
    assert a or b
    assert not (a and b)
    assert (a or b) and True

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.microstructure:[38:59]
==features_microstructure:[42:63]
    @dataclass
    class Trade:
        timestamp: float
        price: float
        size: float
        side: Side

    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float]
        trades: Sequence[Trade]

        @property
        def mid(self) -> float:
            return 0.5 * (self.bid_price + self.ask_price)

        @property (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[413:443]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[378:408]
    return ProbabilityMetrics(
        ece=ece_uniform(p, y, n_bins=n_bins),
        brier=brier_score(p, y),
        logloss=log_loss(p, y),
    )


# =============================
# Self-tests (synthetic)
# =============================

def _make_synthetic(n: int = 5000, seed: int = 7) -> Tuple[List[float], List[int], List[float]]:
    random.seed(seed)
    xs: List[float] = []  # raw scores
    y: List[int] = []
    for _ in range(n):
        s = random.gauss(0.0, 1.0)
        xs.append(s)
    # True mapping: p* = σ(2.0*s - 0.5)
    ps_true = [_sigmoid(2.0 * s - 0.5) for s in xs]
    y = [1 if random.random() < p else 0 for p in ps_true]
    # Model's uncalibrated probabilities are mis-scaled: p_unc = σ(1.2*s - 0.2)
    p_unc = [_sigmoid(1.2 * s - 0.2) for s in xs]
    return xs, y, p_unc


def _test_metrics() -> None:
    p = [0.1, 0.9, 0.6, 0.4]
    y = [0, 1, 1, 0]
    m = evaluate_calibration(p, y, n_bins=4) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.microstructure:[38:58]
==core.signal.leadlag_hy:[41:63]
    @dataclass
    class Trade:
        timestamp: float
        price: float
        size: float
        side: Side

    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float]
        trades: Sequence[Trade]

        @property
        def mid(self) -> float:
            return 0.5 * (self.bid_price + self.ask_price)

# ---------------------------------------------------------------------------------
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.obi:[148:170]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[164:186]
    l1 = obi_l1(b, a)
    lk = obi_lk(b, a, 3)
    assert -1.0 <= l1 <= 1.0 and -1.0 <= lk <= 1.0


def _test_stream() -> None:
    seq = _mock_snapseq()
    obi = OBIStream(levels=5)
    last = {}
    for s in seq:
        last = obi.update(s)
    assert "obi_l1" in last and "obi_lk" in last
    assert -1.0 <= last["obi_l1"] <= 1.0
    assert -1.0 <= last["obi_lk"] <= 1.0
    assert last["depth_bid_lk"] > 0 and last["depth_ask_lk"] > 0
    assert last["spread_bps"] >= 0


if __name__ == "__main__":
    _test_pure_funcs()
    _test_stream()
    print("OK - repo/core/features/obi.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==drift:[224:255]
==repo_core_calibration_drift:[178:209]
        out: Dict[str, float] = {**out_c, **out_g}
        out["drift_alarm"] = drift_alarm
        return out


# =============================
# Self-tests (synthetic)
# =============================

def _make_logit_series(n: int = 600, mu0: float = 0.0, sigma: float = 1.0, shift_at: int = 300, dmu: float = 0.8, seed: int = 7) -> List[float]:
    rnd = random.Random(seed)
    xs: List[float] = []
    for t in range(n):
        m = mu0 + (dmu if t >= shift_at else 0.0)
        xs.append(rnd.gauss(m, sigma))
    return xs


def _test_cusum_and_glr_detect_shift() -> None:
    xs = _make_logit_series()
    dm = DriftMonitor(cusum=CUSUMDetector(k=0.25, h=6.0, clip=6.0), glr=GLRDetector(window=120, thr=20.0, clip=6.0))
    alarms = []
    for i, x in enumerate(xs):
        out = dm.update(x, ts=float(i))
        if out["drift_alarm"] > 0.5:
            alarms.append(i)
    # Must detect after the shift point within a reasonable delay
    assert any(i >= 300 and i <= 500 for i in alarms)


def _test_no_false_alarm_on_stationary() -> None: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[16:35]
==test_shadow_broker_old:[18:40]
        filters = BinanceFilters(
            lot_size_min_qty=Decimal("0.00001"),
            lot_size_max_qty=Decimal("100000"),
            lot_size_step_size=Decimal("0.00001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("1000000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        assert filters.lot_size_min_qty == Decimal("0.00001")
        assert filters.lot_size_max_qty == Decimal("100000")
        assert filters.lot_size_step_size == Decimal("0.00001")
        assert filters.price_filter_min_price == Decimal("0.01")
        assert filters.price_filter_max_price == Decimal("1000000")
        assert filters.price_filter_tick_size == Decimal("0.01")
        assert filters.min_notional == Decimal("10.0")

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[66:84]
==repo_core_features_tfi:[105:123]
    V = max(1e-9, float(bucket_volume))
    B = 0.0
    S = 0.0
    imbalances: List[float] = []
    for tr in trades:
        # how much of trade fits into current bucket
        remain = V - (B + S)
        vol = float(tr.size)
        side = tr.side
        while vol > 0.0:
            take = min(remain, vol)
            if str(side) == "Side.BUY" or str(side) == "BUY":
                B += take
            else:
                S += take
            vol -= take
            remain -= take
            # bucket complete (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[153:173]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[140:160]
        if i % 5 == 0:
            ask = round(ask - 0.01, 2)
        elif i % 7 == 0:
            ask = round(ask + 0.02, 2)
        snaps.append(MarketSnapshot(
            timestamp=ts,
            bid_price=bid,
            ask_price=ask,
            bid_volumes_l=[qb1, 400, 300, 200, 100],
            ask_volumes_l=[qa1, 380, 280, 180, 80],
        ))
        # drift bid
        if i % 8 == 0:
            bid = round(bid + 0.01, 2)
        elif i % 9 == 0:
            bid = round(bid - 0.01, 2)
    return snaps


def _test_pure_funcs() -> None: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.loader:[87:114]
==repo_core_config_hotreload:[35:58]
    out: Dict[str, Any] = {}
    for k, v in d.items():
        key = f"{prefix}.{k}" if prefix else k
        if isinstance(v, Mapping):
            out.update(_flatten(v, key))
        else:
            out[key] = v
    return out


def diff_dicts(old: Mapping[str, Any], new: Mapping[str, Any]) -> Set[str]:
    """Return set of fully-qualified keys that changed between two nested mappings."""
    a = _flatten(old)
    b = _flatten(new)
    changed: Set[str] = set()
    keys = set(a.keys()).union(b.keys())
    for k in keys:
        if a.get(k) != b.get(k):
            changed.add(k)
    return changed

# -------------------- Policy --------------------
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.obi:[124:144]
==microprice:[138:159]
        if i % 5 == 0:
            ask = max(bid + 0.01, round(ask - 0.01, 2))  # ensure ask > bid
        elif i % 7 == 0:
            ask = round(ask + 0.02, 2)
        snaps.append(MarketSnapshot(
            timestamp=ts,
            bid_price=bid,
            ask_price=ask,
            bid_volumes_l=[qb1, 400, 300, 200, 100],
            ask_volumes_l=[qa1, 380, 280, 180, 80],
        ))
        # Modify bid after creating snapshot to maintain valid spread
        if i % 8 == 0:
            bid = min(ask - 0.01, round(bid + 0.01, 2))  # ensure bid < ask
        elif i % 9 == 0:
            bid = max(0.01, round(bid - 0.01, 2))  # ensure bid > 0
    return snaps


def _test_microprice_bounds() -> None:
    # microprice must lie in [bid, ask] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_api_survived_mutants:[245:272]
==test_core_aurora_survived_mutants:[302:329]
        test_dicts = [
            {},  # Empty dict
            {"key": "value"},  # Has key
            {"other_key": "value"},  # Missing key
        ]

        for d in test_dicts:
            # Test pattern: d.get('key') or default
            result = d.get('key') or "default"
            assert result is not None

            # Test pattern: d.get('key') or {}
            result_dict = d.get('key') or {}
            assert isinstance(result_dict, (str, dict))

    def test_compound_get_or_patterns_mutant(self):
        """Kill mutant: boolean logic in compound get() or patterns"""

        test_dicts = [
            {},  # Missing both
            {"guards": {}},  # Has guards
            {"gates": {}},   # Has gates
            {"guards": {}, "gates": {}},  # Has both
        ]

        for d in test_dicts:
            # Test pattern: ((d.get('guards') or d.get('gates') or {})) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_basic_endpoints:[57:81]
==test_basic_endpoints_new:[56:77]
        response = client.get('/version')
        assert response.status_code == 200
        data = response.json()
        assert 'version' in data
        assert isinstance(data['version'], str)

    def test_health_endpoint_with_models_loaded(self, tmp_path):
        """Test health endpoint when models are loaded"""
        client = make_client(tmp_path)
        setup_app_state(client)

        # Mock trading system as loaded
        with patch.object(client.app.state, 'trading_system') as mock_ts:
            mock_ts.student = MagicMock()
            mock_ts.router = MagicMock()

            response = client.get('/health')
            assert response.status_code == 200
            data = response.json()
            assert data['status'] == 'healthy'
            assert data['models_loaded'] is True (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_scripts_run_shadow:[116:131]
==run_replay:[140:155]
    p = calibrator.calibrate_prob(p_raw)
    action = "enter" if p >= threshold else "deny"
    rec = {
        "decision_id": f"{evt['symbol']}-{evt['ts_ns']}",
        "timestamp_ns": evt["ts_ns"],
        "symbol": evt["symbol"],
        "action": action,
        "score": float(S),
        "p_raw": float(p_raw),
        "p": float(p),
        "threshold": float(threshold),
        "features": dict(feats),
        "components": model.score_event(features=feats).components,
        "config_hash": "",  # filled by logger
        "config_schema_version": None,  # filled by logger (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.schema_validator:[213:233]
==repo_core_config_schema_validator:[211:231]
            return value

        return value

    # ----- primitive validators -----

    def _enforce_type(self, value: Any, typ: str, path: str) -> None:
        ok = False
        if typ == "object":
            ok = isinstance(value, Mapping)
        elif typ == "array":
            ok = isinstance(value, list)
        elif typ == "string":
            ok = isinstance(value, str)
        elif typ == "number":
            ok = isinstance(value, (int, float)) and not isinstance(value, bool)
        elif typ == "integer":
            ok = isinstance(value, int) and not isinstance(value, bool)
        elif typ == "boolean":
            ok = isinstance(value, bool) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[150:172]
==repo_core_features_tfi:[49:71]
@dataclass
class _WinTrade:
    ts: float
    buy: float
    sell: float


class _Rolling:
    """Rolling event-time window for buy/sell volumes with O(1) evictions."""
    def __init__(self, horizon_s: float) -> None:
        self.h = float(horizon_s)
        self.q: Deque[_WinTrade] = deque()
        self.bsum = 0.0
        self.ssum = 0.0

    def add(self, ts: float, buy: float, sell: float) -> None:
        self.q.append(_WinTrade(ts=ts, buy=buy, sell=sell))
        self.bsum += buy
        self.ssum += sell
        self._evict(ts)

    def _evict(self, now_ts: float) -> None: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.schema_validator:[331:346]
==repo_core_config_schema_validator:[325:340]
        for k, v in value.items():
            if k in props:
                continue
            if addl is False:
                raise SchemaValidationError(f"{_path_join(path, k)}: additionalProperties not allowed")
            elif addl is True or addl is None:
                if apply_defaults:
                    out[k] = deepcopy(v)
            elif isinstance(addl, Mapping):
                norm = self._validate_node(v, addl, path=_path_join(path, k), ctx=ctx, apply_defaults=apply_defaults)
                if apply_defaults:
                    out[k] = norm
            else:
                raise SchemaValidationError(f"{_path_join(path, k)}: invalid additionalProperties spec")
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_absorption:[118:140]
==tests.test_features:[1186:1214]
        ]
        result = _sum_trades(trades, Side.BUY, 1000.0)
        assert result == 15.0


class TestAbsorptionStream:
    """Test the AbsorptionStream class."""

    def test_initialization(self):
        """Test AbsorptionStream initialization."""
        stream = AbsorptionStream(window_s=5.0, ema_half_life_s=2.0)
        assert stream.window_s == 5.0
        assert stream.hl == 2.0
        assert stream.st.last_ts is None
        assert stream.st.bid_p is None
        assert stream.st.ask_p is None
        assert stream.st.bid_q1 == 0.0
        assert stream.st.ask_q1 == 0.0

    def test_initialization_defaults(self):
        """Test AbsorptionStream with default parameters."""
        stream = AbsorptionStream() (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_signal:[16:39]
==tools.mutation_test_standalone:[12:40]
    model = ScoreModel(weights={"test": 1.0}, intercept=0.0)
    assert model is not None


def test_score_calculation():
    """Test score calculation with features"""
    model = ScoreModel(weights={"feature1": 0.5, "feature2": 0.3}, intercept=-0.1)
    features = {"feature1": 1.0, "feature2": 2.0}
    score = model.score_only(features)
    assert isinstance(score, float)


def test_sigmoid_function():
    """Test sigmoid function"""
    assert _sigmoid(0) == 0.5
    assert _sigmoid(10) > 0.5
    assert _sigmoid(-10) < 0.5


def test_clip_function():
    """Test clip function"""
    assert _clip(5, 0, 10) == 5
    assert _clip(-5, 0, 10) == 0
    assert _clip(15, 0, 10) == 10


def test_bh_qvalues_basic():
    """Test BH q-values calculation""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.tca.edge_budget:[27:49]
==core.types:[291:319]
    if not (0.0 <= p <= 1.0):
        raise ValueError("p must be in [0,1]")
    if G < 0 or L < 0:
        raise ValueError("G and L must be non-negative")
    return p * G - (1.0 - p) * L - c


def p_star_threshold(r: float, c_prime: float, delta: float = 0.0) -> float:
    """Minimal calibrated probability to enter, given payoff ratio r=G/L and c' = c/L.
    p* = (1 + c') / (1 + r). A practical buffer δ≥0 can be added: p > p* + δ.
    """
    if r <= 0:
        raise ValueError("r must be > 0")
    if c_prime < 0:
        raise ValueError("c' must be ≥ 0")
    base = (1.0 + c_prime) / (1.0 + r)
    return min(1.0, max(0.0, base + max(0.0, delta)))


def latency_degradation(edge0_bps: float, kappa_bps_per_ms: float, latency_ms: float) -> float:
    """E[Π(ℓ)] ≈ E[Π(0)] − κ·ℓ  ⇒ returns edge after latency penalty in bps."""
    return edge0_bps - kappa_bps_per_ms * max(0.0, latency_ms)


def raw_kelly_fraction(p: float, b: float, f_max: float = 1.0) -> float:
    """f_raw = clip( (b p − (1−p))/b, 0, f_max ), with b = G/L (odds).
    Guarded for numeric issues; returns 0 if b≤0 or p outside [0,1].
    """ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==features_cross_asset:[27:46]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[24:40]
try:  # optional, used only for pretty-printing/arrays
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from aurora core types; fallback if not present -------
try:  # pragma: no cover - used during integration
    from aurora.core.types import MarketSnapshot
except Exception:
    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[83:97]
==test_execution_router_v1:[419:433]
        context = sample_context
        children = router.execute_sizing_decision(context, sample_market_data)

        child = children[0]
        router.handle_order_ack(child.order_id, time.time_ns(), 5.0)

        # First fill
        fill = FillEvent(
            ts_ns=time.time_ns(),
            qty=0.1,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_api_survived_mutants:[21:39]
==test_core_aurora_survived_mutants:[26:44]
        test_configs = [
            {},  # Missing both guards and gates
            {"guards": {}},  # Has guards, missing gates
            {"gates": {}},   # Has gates, missing guards
            {"guards": {}, "gates": {}},  # Has both
        ]

        for cfg in test_configs:
            # The 'or' logic should handle all these cases
            guards_cfg = (cfg.get('guards') or cfg.get('gates') or {})
            assert isinstance(guards_cfg, dict)

    def test_boolean_or_in_trap_cfg_access_mutant(self):
        """Kill mutant: 'and' -> 'or' in pipeline.py trap_cfg access"""
        # Tests: trap_cfg = (cfg_all.get('trap') or {})

        test_configs = [
            {},  # Missing trap section (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[29:48]
==tests.integration.test_full_b2b7_pipeline:[46:66]
        cox = CoxPH()
        cox._beta = {'obi': 0.1, 'spread_bps': -0.05}
        cox._feat = ['obi', 'spread_bps']

        # SLA
        sla = SLAGate(max_latency_ms=250, kappa_bps_per_ms=0.01, min_edge_after_bps=1.0)

        # Router
        router = Router(
            hazard_model=cox,
            slagate=sla,
            min_p_fill=0.25,
            exchange_name='test'
        )

        return router

    def test_decision_structure(self, router_components):
        """Test that decisions have all required fields.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==run_live:[221:233]
==run_replay:[287:300]
            fh.write(f"APPLIED PROFILE: {args.profile}\n")
            fh.write("CHANGED KEYS:\n")
            for p in changed:
                old = before
                for part in p.split('.'):
                    old = old.get(part, None) if isinstance(old, dict) else None
                new = cfg
                for part in p.split('.'):
                    new = new.get(part, None) if isinstance(new, dict) else None
                fh.write(f"- {p}: {old!r} -> {new!r}\n")
        print(f"PROFILE: applied {args.profile} -> {out_path}")

    # Source (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==run_live:[177:189]
==run_replay:[222:241]
            parts = key.split("_")
            for i in range(1, len(parts)):
                prefix = parts[:i]
                last = "_".join(parts[i:])
                nested = _get_nested(base, prefix)
                if nested is None:
                    continue
                _set_nested(base, prefix + [last], value)
                return ".".join(prefix + [last])
            base[key] = value
            return key
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[46:57]
==core.execution.router_backup:[66:77]
@dataclass
class RouteDecision:
    route: str  # 'maker' | 'taker' | 'deny'
    e_maker_bps: float
    e_taker_bps: float
    p_fill: float
    reason: str
    maker_fee_bps: float = 0.0
    taker_fee_bps: float = 0.0
    net_e_maker_bps: float = 0.0  # Expected edge after fees
    net_e_taker_bps: float = 0.0  # Expected edge after fees (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==icp:[183:196]
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[183:196]
                wt = w[j] + w[j + 1]
                val = (w[j] * v[j] + w[j + 1] * v[j + 1]) / wt
                v[j] = val
                w[j] = wt
                # delete j+1
                del v[j + 1]
                del w[j + 1]
                j -= 1
            i = max(j, 0)
        # build blocks with x spans
        blocks = []
        idx = 0
        for val, wt in zip(v, w): (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_tca_api_backward_compat:[40:51]
==test_tca_identity:[119:130]
        symbol="BTCUSDT",
        side="BUY",
        target_qty=1.0,
        fills=fills,
        arrival_ts_ns=1000000000,
        decision_ts_ns=1000000000,
        arrival_price=100.0,
        arrival_spread_bps=2.0,
        latency_ms=10.0
    )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_tca_api_backward_compat:[13:24]
==test_tca_identity:[14:25]
        symbol="BTCUSDT",
        side="BUY",
        target_qty=1.0,
        fills=fills,
        arrival_ts_ns=1000000000,
        decision_ts_ns=1000000000,
        arrival_price=100.0,
        arrival_spread_bps=2.0,
        latency_ms=10.0
    )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_sizing_kelly:[496:508]
==tests.test_sizing_live_integration:[161:171]
        xai_details = {
            "method": optimizer.method,
            "gross_cap": optimizer.gross_cap,
            "max_weight": optimizer.max_weight,
            "cvar_alpha": optimizer.cvar_alpha,
            "cvar_limit": optimizer.cvar_limit,
            "feasible": True,
            "w_raw": w,
            "w_final": w
        }

        # Check all fields are present and valid types (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_signal:[44:59]
==tools.mutation_test_standalone:[40:58]
        p_values = [0.01, 0.02, 0.03, 0.04, 0.05]
        q_values = bh_qvalues(p_values)
        assert len(q_values) == len(p_values)
        assert all(isinstance(q, float) for q in q_values)

    def test_reject_basic(self):
        """Test reject function"""
        p_values = [0.01, 0.02, 0.03, 0.04, 0.05]
        rejected_mask, num_rejected = reject(p_values, alpha=0.05)
        assert isinstance(rejected_mask, list)
        assert isinstance(num_rejected, int)
        assert len(rejected_mask) == len(p_values)
        assert all(isinstance(r, bool) for r in rejected_mask)

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_absorption:[21:41]
==tests.test_features:[1077:1105]
class TestEMA:
    """Test _EMA exponential moving average functionality."""

    def test_ema_initialization(self):
        """Test EMA initialization."""
        from core.features.absorption import _EMA

        ema = _EMA(half_life_s=2.0)
        assert ema.half_life_s == 2.0
        assert ema.value == 0.0
        assert ema._last_ts is None

    def test_ema_first_update(self):
        """Test first EMA update sets value directly."""
        from core.features.absorption import _EMA

        ema = _EMA(half_life_s=2.0)
        result = ema.update(10.0, 1000.0)

        assert result == 10.0
        assert ema.value == 10.0
        assert ema._last_ts == 1000.0

    def test_ema_subsequent_updates(self):
        """Test EMA smoothing over multiple updates."""
        from core.features.absorption import _EMA
        import math
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_api_integration:[33:46]
==tests.test_api_validation:[65:78]
        port = find_free_port()

    # Start server in subprocess
    cmd = [
        sys.executable, "-m", "uvicorn",
        "api.service:app",
        "--host", "127.0.0.1",
        "--port", str(port),
        "--log-level", "error"
    ]

    process = None
    try: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_portfolio_correlation_effect:[33:49]
==test_sizing_orchestrator_effect:[30:46]
            }
        },
        'execution':{'sla':{'p95_ms':200}}
    }


def mk_market():
    return MarketSpec(
        tick_size=Decimal('0.01'), lot_size=Decimal('0.1'), min_notional=Decimal('10'),
        maker_fee_bps=1, taker_fee_bps=5, best_bid=Decimal('100'), best_ask=Decimal('100.1'),
        spread_bps=1.0, mid=Decimal('100.05')
    )


def mk_intent(stop_bps=100, qty_hint=None, equity=Decimal('10000')):
    return OrderIntent( (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.xai.__init__:[21:31]
==test_xai_init:[34:44]
    "SCHEMA_ID",
    "validate_decision",
    "canonical_json",
    "DecisionLogger",
    "AlertResult",
    "NoTradesAlert",
    "DenySpikeAlert",
    "CalibrationDriftAlert",
    "CvarBreachAlert",
] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.regime.manager:[40:52]
==core.xai.alerts:[65:79]
    if not xs:
        return 0.0
    q = 0.0 if q < 0.0 else 1.0 if q > 1.0 else q
    xs2 = sorted(xs)
    pos = q * (len(xs2) - 1)
    lo = int(pos)
    hi = min(lo + 1, len(xs2) - 1)
    frac = pos - lo
    return xs2[lo] * (1 - frac) + xs2[hi] * frac


@dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.order_lifecycle:[14:24]
==core.order_logger:[365:375]
    priority = [
        "FILLED",
        "CANCELLED",
        "EXPIRED",
        "PARTIAL",
        "ACK",
        "SUBMITTED",
        "CREATED",
    ]
    seen = set() (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.exchange.__init__:[27:37]
==core.execution.exchange.common:[302:312]
    "ExchangeError",
    "ValidationError",
    "RateLimitError",
    "Side",
    "OrderType",
    "TimeInForce",
    "SymbolInfo",
    "OrderRequest",
    "Fill",
    "OrderResult", (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==cvar:[26:61]
==repo_core_risk_cvar:[26:51]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# =============================
# Empirical VaR / CVaR
# =============================

def _quantile(sorted_vals: Sequence[float], alpha: float) -> float:
    n = len(sorted_vals)
    if n == 0:
        return 0.0
    a = min(1.0, max(0.0, float(alpha)))
    k = int(math.ceil(a * n) - 1)
    k = max(0, min(n - 1, k))
    return float(sorted_vals[k])


def var_cvar_from_losses(losses: Sequence[float], alpha: float = 0.99) -> Tuple[float, float]:
    """Compute empirical VaRα and CVaRα given **losses** (higher = worse).

    VaRα is the α-quantile of the loss distribution. CVaRα is the mean of the
    tail beyond VaRα (including equals).
    """ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.obi:[126:137]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[142:153]
        elif i % 7 == 0:
            ask = round(ask + 0.02, 2)
        snaps.append(MarketSnapshot(
            timestamp=ts,
            bid_price=bid,
            ask_price=ask,
            bid_volumes_l=[qb1, 400, 300, 200, 100],
            ask_volumes_l=[qa1, 380, 280, 180, 80],
        ))
        # drift bid
        if i % 8 == 0: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==microprice:[140:151]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[155:165]
        elif i % 7 == 0:
            ask = round(ask + 0.02, 2)
        snaps.append(MarketSnapshot(
            timestamp=ts,
            bid_price=bid,
            ask_price=ask,
            bid_volumes_l=[qb1, 400, 300, 200, 100],
            ask_volumes_l=[qa1, 380, 280, 180, 80],
        ))
        if i % 8 == 0: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_signal_leadlag_hy_py_hy_lead_lag_glr_te_embargo_single_file_self_tests:[86:99]
==features_cross_asset:[110:133]
        return out

    @staticmethod
    def _hy_cov(rx: Sequence[Tuple[float, float, float]], ry: Sequence[Tuple[float, float, float]]) -> Tuple[float, float, float, int]:
        cov = 0.0
        varx = 0.0
        vary = 0.0
        for _, _, r in rx:
            varx += r * r
        for _, _, s in ry:
            vary += s * s
        i = 0
        j = 0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[31:42]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[31:43]
except Exception:
    @dataclass
    class ProbabilityMetrics:  # fallback
        ece: Optional[float] = None
        brier: Optional[float] = None
        logloss: Optional[float] = None

        def lambda_cal(self, *, eta: float = 10.0, zeta: float = 5.0) -> float:
            ece = 0.0 if self.ece is None else float(self.ece)
            logloss = 0.0 if self.logloss is None else float(self.logloss)
            return math.exp(-eta * ece) * math.exp(-zeta * logloss)
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[23:38]
==repo_core_sizing_lambdas:[25:45]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from core/types; minimal fallbacks if unavailable -----
try:  # pragma: no cover - exercised in integration
    from aurora.core.types import ProbabilityMetrics, ConformalInterval
except Exception:
    @dataclass
    class ProbabilityMetrics:  # fallback
        ece: Optional[float] = None
        brier: Optional[float] = None
        logloss: Optional[float] = None
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[86:97]
==test_execution_router_v1:[563:574]
            child = children[0]
            router.handle_order_ack(child.order_id, time.time_ns(), 5.0)

            fill = FillEvent(
                ts_ns=time.time_ns(),
                qty=0.1,
                price=child.price,
                fee=0.001,
                liquidity_flag='M'
            )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_absorption:[63:82]
==tests.test_features:[1131:1155]
        ema = _EMA(half_life_s=2.0)
        ema.update(10.0, 1000.0)

        # Time going backwards should be treated as zero dt
        result = ema.update(20.0, 999.0)
        assert result == 10.0  # Should keep previous value due to zero dt


class TestSumTrades:
    """Test _sum_trades helper function."""

    def test_sum_trades_empty(self):
        """Test summing empty trades list."""
        from core.features.absorption import _sum_trades

        trades = []
        result = _sum_trades(trades, Side.BUY, 1000.0)
        assert result == 0.0

    def test_sum_trades_single_side(self):
        """Test summing trades of single side."""
        from core.features.absorption import _sum_trades

        trades = [ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_config_loader:[18:27]
==tests.test_config_schema_validator:[7:16]
        "schema": {
            "type": "object",
            "properties": {
                "risk": {
                    "type": "object",
                    "properties": {
                        "cvar": {
                            "type": "object",
                            "properties": { (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_basic_endpoints_new:[236:245]
==test_metrics_export:[48:57]
        'aurora_deny_rate_15m',
        'aurora_latency_p99_ms',
        'aurora_ece',
        'aurora_cvar95_min',
        'aurora_sse_clients',
        'aurora_sse_disconnects_total',
        'aurora_parent_gate_allow_total',
        'aurora_parent_gate_deny_total',
        'aurora_expected_net_reward_blocked_total', (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.regime.manager:[40:51]
==core.universe.ranking:[63:74]
    if not xs:
        return 0.0
    q = 0.0 if q < 0.0 else 1.0 if q > 1.0 else q
    xs2 = sorted(xs)
    pos = q * (len(xs2) - 1)
    lo = int(pos)
    hi = min(lo + 1, len(xs2) - 1)
    frac = pos - lo
    return xs2[lo] * (1 - frac) + xs2[hi] * frac

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[259:270]
==core.execution.router_new:[86:97]
                route="deny",
                why_code="WHY_UNATTRACTIVE",
                scores={
                    "edge_after_latency_bps": edge_after_lat,
                    "edge_floor_bps": self.edge_floor_bps,
                    "spread_bps": spread_bps,
                    "spread_deny_bps": self.spread_deny_bps
                }
            )

        # 4) Edge floor після latency (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.exchange.binance:[130:140]
==core.execution.exchange.gate:[103:113]
        info = self.get_symbol_info(req.symbol)
        clean = self.validate_order(req, info)
        coid = clean.client_order_id or make_idempotency_key("oid", {
            "s": clean.symbol,
            "sd": clean.side.value,
            "t": clean.type.value,
            "q": clean.quantity,
            "p": clean.price if clean.price is not None else "",
        })
        # Gate expects symbol as BASE_QUOTE with underscore (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==cvar:[72:103]
==repo_core_risk_cvar:[51:73]
    x = sorted(float(z) for z in losses)
    if not x:
        return 0.0, 0.0
    var = _quantile(x, alpha)
    tail = [z for z in x if z >= var]
    cvar = sum(tail) / max(1, len(tail))
    return var, cvar


def var_cvar_from_pnl(pnl: Sequence[float], alpha: float = 0.99) -> Tuple[float, float]:
    """Convenience wrapper when inputs are **PnL/returns** (negative = loss).

    Converts to losses via L = max(0, −PnL) and applies `var_cvar_from_losses`.
    """
    losses = [max(0.0, -float(r)) for r in pnl]
    return var_cvar_from_losses(losses, alpha)


def var_with_ci(losses: Sequence[float], alpha: float = 0.99, *, method: str = "empirical", **kwargs) -> Tuple[float, float, float]:
    """Compute VaRα with confidence interval given **losses** (higher = worse).

    Args:
        losses: Sequence of loss values
        alpha: Confidence level (0.99 for 99% VaR)
        method: "empirical" (default) or "POT" for Peaks-Over-Threshold
        **kwargs: Additional arguments for specific methods
            For method="POT": q_u (threshold quantile, default 0.95), n_boot (bootstrap samples, default 1000)

    Returns:
        Tuple of (VaR, CI_lower, CI_upper) values
    """ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==drift:[257:269]
==repo_core_calibration_drift:[211:223]
    tripped = False
    for i, x in enumerate(xs):
        out = dm.update(x, ts=float(i))
        tripped = tripped or (out["drift_alarm"] > 0.5)
    # With stricter thresholds, should not trip
    assert not tripped


if __name__ == "__main__":
    _test_cusum_and_glr_detect_shift()
    _test_no_false_alarm_on_stationary()
    print("OK - repo/core/calibration/drift.py self-tests passed") (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.types:[214:227]
==edge_budget_py:[36:55]
class EdgeBreakdown:
    raw_edge_bps: float = 0.0
    fees_bps: float = 0.0
    slippage_bps: float = 0.0
    adverse_bps: float = 0.0
    latency_bps: float = 0.0
    rebates_bps: float = 0.0

    def net_edge_bps(self) -> float:
        return (self.raw_edge_bps - self.fees_bps - self.slippage_bps -
                self.adverse_bps - self.latency_bps + self.rebates_bps)

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_signal_leadlag_hy_py_hy_lead_lag_glr_te_embargo_single_file_self_tests:[62:78]
==features_cross_asset:[87:101]
        dq = self._buf.get(symbol)
        if not dq:
            return
        cutoff = float(now_ts) - self.window_s
        while dq and dq[0].t < cutoff:
            dq.popleft()

    # ------------------------ HY helpers ------------------------
    @staticmethod
    def _returns(points: Sequence[_PricePoint]) -> List[Tuple[float, float, float]]:
        """Build log-returns and their intervals: [(t0, t1, r), ...]."""
        out: List[Tuple[float, float, float]] = []
        if len(points) < 2:
            return out (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_tca_api_backward_compat:[15:24]
==test_tca_identity:[68:77]
        target_qty=1.0,
        fills=fills,
        arrival_ts_ns=1000000000,
        decision_ts_ns=1000000000,
        arrival_price=100.0,
        arrival_spread_bps=2.0,
        latency_ms=10.0
    )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[640:650]
==test_shadow_broker_old:[556:564]
                lot_size_min_qty=Decimal("0.001"),
                lot_size_max_qty=Decimal("1000"),
                lot_size_step_size=Decimal("0.001"),
                price_filter_min_price=Decimal("0.01"),
                price_filter_max_price=Decimal("100000"),
                price_filter_tick_size=Decimal("0.01"),
                min_notional=Decimal("10.0")
            ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[604:613]
==test_shadow_broker_old:[523:531]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[567:576]
==test_shadow_broker_old:[358:366]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[532:542]
==test_shadow_broker_old:[326:334]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        # Test insufficient notional (0.001 * 1.0 = 0.001 < 10.0) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[496:506]
==test_shadow_broker_old:[263:271]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        # Test quantity too small (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[326:336]
==test_shadow_broker_old:[231:239]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        # Submit order with invalid quantity (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[290:299]
==test_shadow_broker_old:[189:197]
                    lot_size_min_qty=Decimal("0.001"),
                    lot_size_max_qty=Decimal("1000"),
                    lot_size_step_size=Decimal("0.001"),
                    price_filter_min_price=Decimal("0.01"),
                    price_filter_max_price=Decimal("100000"),
                    price_filter_tick_size=Decimal("0.01"),
                    min_notional=Decimal("10.0")
                ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[253:262]
==test_shadow_broker_old:[125:133]
                    lot_size_min_qty=Decimal("0.001"),
                    lot_size_max_qty=Decimal("1000"),
                    lot_size_step_size=Decimal("0.001"),
                    price_filter_min_price=Decimal("0.01"),
                    price_filter_max_price=Decimal("100000"),
                    price_filter_tick_size=Decimal("0.01"),
                    min_notional=Decimal("10.0")
                ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[218:228]
==test_shadow_broker_old:[93:101]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        # Test insufficient notional (0.001 * 1.0 = 0.001 < 10.0) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[182:192]
==test_shadow_broker_old:[49:57]
            lot_size_min_qty=Decimal("0.001"),
            lot_size_max_qty=Decimal("1000"),
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        # Test quantity too small (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[221:233]
==test_execution_router_v1:[426:434]
        fill = FillEvent(
            ts_ns=time.time_ns(),
            qty=0.1,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        )
        router.handle_order_fill(child.order_id, fill) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_sizing_live_integration:[142:155]
==tests.test_sizing_portfolio:[23:36]
        )

        # Simple 3-asset case
        cov = [
            [0.04, 0.01, 0.005],
            [0.01, 0.09, 0.02],
            [0.005, 0.02, 0.16]
        ]
        mu = [0.02, 0.03, 0.04]

        w = optimizer.optimize(cov, mu)

        # Check constraints (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_router_backup:[438:446]
==tests.test_execution_router:[291:306]
        assert hasattr(decision, 'e_maker_bps')
        assert hasattr(decision, 'e_taker_bps')
        assert hasattr(decision, 'p_fill')
        assert hasattr(decision, 'reason')
        assert hasattr(decision, 'maker_fee_bps')
        assert hasattr(decision, 'taker_fee_bps')
        assert hasattr(decision, 'net_e_maker_bps')
        assert hasattr(decision, 'net_e_taker_bps') (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_exchange_adapters:[24:42]
==tests.test_unified_exchange_adapter:[54:73]
class MockHttpClient:
    """Mock HTTP client for testing."""

    def __init__(self, responses=None):
        self.responses = responses or {}
        self.requests = []

    def request(self, method, url, *, params=None, headers=None, json=None):
        self.requests.append((method, url, params, headers, json))
        # Return mock response based on URL pattern
        for pattern, response in self.responses.items():
            if pattern in url:
                return response
        return {}


# Test Factory Pattern
def test_exchange_adapter_factory():
    """Test exchange adapter factory creation.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_api_integration:[15:32]
==tests.test_api_validation:[12:29]
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)


def find_free_port():
    """Find a free port for testing."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port


@contextmanager
def test_server(port=None):
    """Context manager to start and stop test server.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[84:95]
==tests.integration.test_run_live_paper_loop:[253:264]
        quote = QuoteSnapshot(bid_px=49999.0, ask_px=50001.0)

        # High latency should trigger SLA deny
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=5.0,
            latency_ms=300.0,  # Breach SLA
            fill_features={'obi': 0.5, 'spread_bps': 2.0}
        )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_basic_endpoints:[81:91]
==test_basic_endpoints_new:[81:91]
        client = make_client(tmp_path)
        setup_app_state(client)

        # Mock trading system as not loaded
        with patch.object(client.app.state, 'trading_system', None):
            response = client.get('/health')
            assert response.status_code == 200
            data = response.json()
            assert data['status'] == 'starting'
            assert data['models_loaded'] is False (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[132:145]
==core.execution.router_new:[50:69]
        self.edge_floor_bps: float = float(ex.get("edge_floor_bps", 0.0))
        self.p_min_fill: float = float(r.get("p_min_fill", 0.25))
        self.horizon_ms: int = int(r.get("horizon_ms", 1500))
        self.kappa_bps_per_ms: float = float(sla.get("kappa_bps_per_ms", 0.0))
        self.max_latency_ms: float = float(sla.get("max_latency_ms", float("inf")))

        # додаткові пороги
        self.spread_deny_bps: float = float(r.get("spread_deny_bps", 8.0))
        self.maker_spread_ok_bps: float = float(r.get("maker_spread_ok_bps", 2.0))
        self.switch_margin_bps: float = float(r.get("switch_margin_bps", 0.0))

    def decide(self,
               side: str,
               quote,                       # QuoteSnapshot із bid/ask
               edge_bps_estimate: float,
               latency_ms: float,
               fill_features: Dict[str, Any]) -> Decision:

        # 1) SLA gate — надмірна латентність (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tools.run_canary:[38:48]
==tools.run_live_testnet:[62:72]
    runner_cmd = [
        sys.executable,
        "-m",
        "skalp_bot.runner.run_live_aurora"
    ]

    if args.runner_config:
        runner_cmd.extend(["--config", args.runner_config])

    # Set base URL to match API port (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_sim_local_basic:[8:18]
==test_sim_local_sink_coverage:[13:22]
    return {
        'best_bid': bid,
        'best_ask': ask,
        'liquidity': {'bid': bid_qty, 'ask': ask_qty},
        'depth': {'at_price': {}, 'levels_sum': {}},
        'traded_since_last': {},
    }

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[120:131]
==test_execution_router_v1:[428:437]
            qty=0.1,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        )
        router.handle_order_fill(child.order_id, fill)
        assert len(child.fills) == 1

        # Duplicate fill - should be ignored (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[90:97]
==test_execution_router_v1:[453:460]
        fill = FillEvent(
            ts_ns=time.time_ns(),
            qty=0.1,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[86:95]
==tests.integration.test_run_live_paper_loop:[156:165]
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=5.0,
            latency_ms=300.0,  # Breach SLA (250ms limit)
            fill_features={'obi': 0.5, 'spread_bps': 2.0}
        )

        # SLA breach should either deny or fallback to maker if maker is still attractive (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[52:61]
==tests.integration.test_full_b2b7_pipeline:[96:105]
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=5.0,
            latency_ms=10.0,
            fill_features={'obi': 0.8, 'spread_bps': 1.0}
        )

        # Verify decision has all required fields (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_execution_router_v1:[170:177]
==tests.integration.test_e2e_integration:[263:270]
            fill = FillEvent(
                ts_ns=time.time_ns(),
                qty=child.target_qty * 0.5,
                price=child.price,
                fee=0.001,
                liquidity_flag='M'
            ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_execution_router_v1:[255:262]
==tests.integration.test_e2e_integration:[100:107]
        fill = FillEvent(
            ts_ns=time.time_ns(),
            qty=fill_qty,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.universe.__init__:[12:19]
==test_universe_init:[26:33]
    "EmaSmoother",
    "Hysteresis",
    "HState",
    "Ranked",
    "SymbolMetrics",
    "UniverseRanker",
] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.regime.__init__:[15:22]
==test_regime_init:[26:33]
            "PageHinkley",
            "PHResult",
            "GLRMeanShift",
            "GLRResult",
            "RegimeManager",
            "RegimeState",
        ] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.order_lifecycle:[27:34]
==core.order_logger:[376:383]
        s = str(ev.get("status") or ev.get("state") or ev.get("lifecycle") or "").upper()
        if s:
            seen.add(s)
    for s in priority:
        if s in seen:
            return s
    return "UNKNOWN" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==alpha_ledger:[227:236]
==test_alpha_ledger:[197:204]
            "total_alloc": 0.0,
            "total_spent": 0.0,
            "active_tests": 0,
            "closed_tests": 0,
            "by_test_id": {},
            "by_outcome": {}
        } (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[351:358]
==core.execution.router_new:[116:123]
                route="maker",
                why_code="OK_ROUTE_MAKER",
                scores={
                    "p_fill": p_fill,
                    "p_min_fill": self.p_min_fill,
                    "spread_bps": spread_bps,
                    "maker_spread_ok_bps": self.maker_spread_ok_bps, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[299:308]
==core.execution.router_new:[99:108]
                route="deny",
                why_code="WHY_UNATTRACTIVE",
                scores={
                    "edge_after_latency_bps": edge_after_lat,
                    "edge_floor_bps": self.edge_floor_bps
                }
            )

        # 5) Вибір maker/taker за очікуваною вигодою з P(fill) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[219:228]
==core.execution.router_new:[71:80]
                route="deny",
                why_code="WHY_SLA_LATENCY",
                scores={
                    "latency_ms": float(latency_ms),
                    "max_latency_ms": self.max_latency_ms
                }
            )

        # 2) Штраф за латентність -> edge_after_latency (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.enhanced_router:[26:33]
==core.execution.router_backup:[46:56]
@dataclass
class QuoteSnapshot:
    bid_px: float
    ask_px: float
    bid_sz: float = 0.0
    ask_sz: float = 0.0
    ts_ns: int = 0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.config.api_integration:[36:44]
==tools.config_cli:[55:63]
        'testnet': Environment.TESTNET,
        'live': Environment.PRODUCTION,
        'prod': Environment.PRODUCTION,
        'production': Environment.PRODUCTION,
        'dev': Environment.DEVELOPMENT,
        'development': Environment.DEVELOPMENT
    }
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_scripts_run_shadow:[57:64]
==run_replay:[70:81]
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                continue
            if isinstance(obj, dict):
                yield obj (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[179:196]
==repo_core_features_tfi:[71:88]
        cutoff = float(now_ts) - self.h
        while self.q and self.q[0].ts < cutoff:
            t = self.q.popleft()
            self.bsum -= t.buy
            self.ssum -= t.sell

    def sums(self, now_ts: float) -> Tuple[float, float]:
        self._evict(now_ts)
        return self.bsum, self.ssum


# =============================
# Pure helpers
# =============================

def tfi_increment(tr: Trade) -> float:
    """+size for BUY taker, −size for SELL taker.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.signal.leadlag_hy:[48:55]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[33:41]
    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float]
        # minimal helpers to mirror the real class (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.microstructure:[45:52]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[33:40]
    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==icp:[97:110]
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[144:157]
        S = []
        if p1 > self.alpha:
            S.append(1)
        if p0 > self.alpha:
            S.append(0)
        return S


# =============================
# Mondrian split conformal (per-group)
# =============================

@dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==icp:[144:157]
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[97:110]
        S = []
        if p1 > self.alpha:
            S.append(1)
        if p0 > self.alpha:
            S.append(0)
        return S


# =============================
# Mondrian split conformal (per-group)
# =============================

@dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==features_cross_asset:[36:46]
==features_microstructure:[49:56]
    @dataclass
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float] (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_signal_leadlag_hy_py_hy_lead_lag_glr_te_embargo_single_file_self_tests:[152:159]
==features_cross_asset:[184:191]
        return {
            "hy_cov": cov,
            "hy_corr": corr,
            "var_x": varx,
            "var_y": vary,
            "beta_x_on_y": beta_x_on_y,
            "beta_y_on_x": beta_y_on_x, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[472:486]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[432:447]
    assert m_temp.ece <= m_unc.ece + 0.05


def _test_isotonic_monotone() -> None:
    # create a zigzag mapping so isotonic must pool
    x = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
    y = [0, 1, 0, 1, 0, 1]
    iso = IsotonicCalibrator().fit(x, y)
    # predictions must be non-decreasing
    preds = iso.predict_proba(x)
    for i in range(len(preds) - 1):
        assert preds[i] <= preds[i + 1] + 1e-12

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[458:468]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[420:430]
    assert m_pl.logloss <= m_unc.logloss + 0.05


def _test_temperature_improves() -> None:
    xs, y, p_unc = _make_synthetic()
    ts = TemperatureScaler()
    ts.fit(p_unc, y)
    p_temp = ts.predict_proba(p_unc)
    m_unc = evaluate_calibration(p_unc, y)
    m_temp = evaluate_calibration(p_temp, y) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tools.simple_mutator:[21:30]
==tools.ultra_simple_mutator:[21:30]
        self.source_dir = Path(source_dir)
        self.test_command = test_command
        self.timeout = timeout
        self.mutants_created = 0
        self.mutants_killed = 0
        self.mutants_survived = 0

    def get_python_files(self) -> List[Path]:
        """Get all Python files in the source directory""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tools.run_canary:[63:73]
==tools.run_live_testnet:[90:100]
        )

        # Monitor the process with timeout
        start_time = time.time()
        end_time = start_time + (args.minutes * 60)

        while time.time() < end_time:
            if process.poll() is not None:
                # Process has terminated
                break (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_tca_api_backward_compat:[17:24]
==test_tca_identity:[266:273]
        arrival_ts_ns=1000000000,
        decision_ts_ns=1000000000,
        arrival_price=100.0,
        arrival_spread_bps=2.0,
        latency_ms=10.0
    )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[184:192]
==test_shadow_broker_old:[160:166]
            lot_size_step_size=Decimal("0.001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("100000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )

        # Test quantity too small (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[64:72]
==test_shadow_broker_old:[454:462]
        mock_cfg = Mock()
        mock_cfg.base_url = "https://api.binance.com"
        mock_cfg.api_key = "test_key"
        mock_cfg.api_secret = "test_secret"
        mock_load_cfg.return_value = mock_cfg

        # Mock exchange info response
        mock_response = Mock() (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[42:50]
==test_shadow_broker_old:[402:410]
        mock_cfg = Mock()
        mock_cfg.base_url = "https://api.binance.com"
        mock_cfg.api_key = "test_key"
        mock_cfg.api_secret = "test_secret"
        mock_load_cfg.return_value = mock_cfg

        # Mock exchange info response
        mock_response = Mock() (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[148:155]
==test_shadow_broker_old:[21:28]
            lot_size_step_size=Decimal("0.00001"),
            price_filter_min_price=Decimal("0.01"),
            price_filter_max_price=Decimal("1000000"),
            price_filter_tick_size=Decimal("0.01"),
            min_notional=Decimal("10.0")
        )
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_router_decide_expected_edge:[11:17]
==test_router_new:[118:124]
            "execution": {
                "edge_floor_bps": 1.0,
                "router": {
                    "horizon_ms": 1500,
                    "p_min_fill": 0.25,
                    "spread_deny_bps": 8.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[265:271]
==test_execution_router_v1:[256:262]
            ts_ns=time.time_ns(),
            qty=fill_qty,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[120:128]
==test_execution_router_v1:[568:577]
            qty=0.1,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        )

        # First fill
        router.handle_order_fill(child.order_id, fill) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[83:92]
==test_execution_router_v1:[163:172]
        context = sample_context
        children = router.execute_sizing_decision(context, sample_market_data)

        child = children[0]
        router.handle_order_ack(child.order_id, time.time_ns(), 5.0)

        # Create fill with trade_id
        fill = FillEvent(
            ts_ns=time.time_ns(), (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_sizing_kelly:[487:496]
==tests.test_sizing_portfolio:[128:137]
        cov = [
            [0.04, 0.01],
            [0.01, 0.09]
        ]
        mu = [0.02, 0.03]

        w = optimizer.optimize(cov, mu)

        # Should respect tight constraints (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_tfi_complete:[327:340]
==tests.test_features:[342:355]
class TestRollingWindow:
    """Test _Rolling internal class."""

    def test_rolling_initialization(self):
        """Test _Rolling initialization."""
        rolling = _Rolling(horizon_s=5.0)
        assert rolling.h == 5.0
        assert rolling.bsum == 0.0
        assert rolling.ssum == 0.0
        assert len(rolling.q) == 0

    def test_rolling_add_and_sums(self):
        """Test adding trades and getting sums.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_absorption:[261:275]
==tests.test_features:[1401:1418]
        )

        features = stream.update(snap2)

        # Should detect depletion at bid (no replenishment)
        assert features["rate_replenish_bid"] == 0.0  # No replenishment when price decreases

    def test_update_with_price_change_ask_up(self):
        """Test update when ask price increases (depletion case)."""
        stream = AbsorptionStream()

        # First snapshot
        snap1 = MarketSnapshot(
            timestamp=1000.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_absorption:[231:245]
==tests.test_features:[1305:1321]
        )

        features = stream.update(snap2)

        # Should have replenishment at bid
        assert features["rate_replenish_bid"] > 0

    def test_absorption_stream_price_step_up_bid(self):
        """Test absorption stream with bid price stepping up."""
        from core.features.absorption import AbsorptionStream

        stream = AbsorptionStream()

        # Initialize
        snap1 = MarketSnapshot(
            timestamp=1000.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_api_integration:[69:77]
==tests.test_api_validation:[138:148]
            process.terminate()
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                process.kill()
                process.wait()


def main():
    """Run all API tests.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_api_integration:[46:54]
==tests.test_api_validation:[79:87]
        process = subprocess.Popen(
            cmd,
            cwd=PROJECT_ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )

        # Wait for server to start (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[58:66]
==tests.integration.test_full_b2b7_pipeline:[244:252]
        )

        # Verify decision has all required fields
        assert hasattr(decision, 'route')
        assert hasattr(decision, 'e_maker_bps')
        assert hasattr(decision, 'e_taker_bps')
        assert hasattr(decision, 'p_fill')
        assert hasattr(decision, 'reason') (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.integration.test_full_b2b7_pipeline:[139:147]
==tests.integration.test_run_live_paper_loop:[132:139]
        )

        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=2.0,  # Small positive edge
            latency_ms=10.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.integration.test_full_b2b7_pipeline:[93:101]
==tests.integration.test_run_live_paper_loop:[102:109]
            )

            # Make routing decision
            decision = router.decide(
                side='buy',
                quote=quote,
                edge_bps_estimate=5.0,
                latency_ms=10.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[152:158]
==tests.integration.test_e2e_integration:[101:107]
                    ts_ns=time.time_ns(),
                    qty=fill_qty,
                    price=child.price,
                    fee=0.001,
                    liquidity_flag='M'
                ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_metrics_exposition:[9:18]
==test_portfolio_correlation_effect:[40:49]
    return MarketSpec(
        tick_size=Decimal('0.01'), lot_size=Decimal('0.1'), min_notional=Decimal('10'),
        maker_fee_bps=1, taker_fee_bps=5, best_bid=Decimal('100'), best_ask=Decimal('100.1'),
        spread_bps=1.0, mid=Decimal('100.05')
    )


def mk_intent(equity: Decimal, stop_bps=200):
    return OrderIntent( (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==skalp_bot.runner.run_live_aurora:[348:354]
==tests.test_sizing_live_integration:[46:52]
            "p_cal": p_cal,
            "rr": rr,
            "f_raw": f_raw,
            "f_clipped": f_raw,  # No clipping in this case
            "notional_target": notional_target,
            "qty": qty, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==skalp_bot.integrations.aurora_gate:[20:26]
==skalp_bot.runner.run_live_aurora:[50:56]
            "order": order,
            "market": market,
            "risk_tags": list(risk_tags),
            "fees_bps": float(fees_bps),
        }
        try: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==run_live:[169:176]
==run_replay:[211:222]
        cur = d
        for p in parts[:-1]:
            if p not in cur or not isinstance(cur[p], dict):
                cur[p] = {}
            cur = cur[p]
        cur[parts[-1]] = value

    def _find_best_split_and_set(base: dict, key: str, value):
        """Try to split underscore-style key into nested path that exists in base.
        For key like 'execution_sla_max_latency_ms' try ['execution','sla','max_latency_ms'], etc.
        Return dot-path string that was set.""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==run_live:[161:168]
==run_replay:[203:211]
            cur = d
            for p in parts:
                if not isinstance(cur, dict) or p not in cur:
                    return None
                cur = cur[p]
            return cur
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.microstructure:[26:34]
==core.types:[41:49]
class Side(str, Enum):
    BUY = "BUY"
    SELL = "SELL"

class OrderType(str, Enum):
    LIMIT = "LIMIT"
    MARKET = "MARKET"
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.sla:[105:111]
==core.tca.latency:[74:80]
        if max_latency_ms is None:
            try:
                cfg = get_config()
                max_latency_ms = float(cfg.get("execution.sla.max_latency_ms", 25))
            except (ConfigError, Exception):
                max_latency_ms = 25.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[394:405]
==core.execution.router_new:[128:137]
                route="taker",
                why_code="OK_ROUTE_TAKER",
                scores={
                    "p_fill": p_fill,
                    "p_min_fill": self.p_min_fill,
                    "spread_bps": spread_bps,
                    "edge_after_latency_bps": edge_after_lat
                }
            ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.exchange.binance:[134:140]
==core.execution.exchange.common:[283:289]
                    "s": clean.symbol,
                    "sd": clean.side.value,
                    "t": clean.type.value,
                    "q": clean.quantity,
                    "p": clean.price if clean.price is not None else "",
                }) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==cvar:[26:39]
==icp:[23:39]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# =============================
# Utilities
# =============================

def _quantile_leq(sorted_vals: Sequence[float], q: float) -> float:
    """Left-closed quantile: returns smallest t such that P(X ≤ t) ≥ q.
    Input must be sorted ascending.
    """
    n = len(sorted_vals)
    if n == 0:
        return 0.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_scripts_run_shadow:[93:99]
==run_replay:[111:117]
        if typ == "quote":
            bid = float(evt["bid_px"])  # ensured by Normalizer
            ask = float(evt["ask_px"])
            bid_sz = 0.0 if evt.get("bid_sz") is None else float(evt["bid_sz"])
            ask_sz = 0.0 if evt.get("ask_sz") is None else float(evt["ask_sz"])
            mp = (ask * bid_sz + bid * ask_sz) / max(1e-12, (bid_sz + ask_sz)) if (bid_sz + ask_sz) > 0 else (bid + ask) / 2 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[31:38]
==repo_core_sizing_lambdas:[33:45]
except Exception:
    @dataclass
    class ProbabilityMetrics:  # minimal fallback
        ece: Optional[float] = None
        brier: Optional[float] = None
        logloss: Optional[float] = None


# =============================
# Core λ functions (each returns a value in [0,1])
# =============================
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[24:34]
==repo_core_sizing_lambdas:[25:35]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from project calibrator; fallback if not present ------
try:  # pragma: no cover - used during integration
    from aurora.core.calibration import ProbabilityMetrics  # type: ignore
except Exception:
    @dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.obi:[105:118]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[134:147]
        }
        return feats


# =============================
# Self-tests
# =============================

def _mock_snapseq() -> List[MarketSnapshot]:
    t0 = time.time()
    snaps: List[MarketSnapshot] = []
    bid, ask = 100.00, 100.02
    qb1, qa1 = 500.0, 520.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[24:34]
==repo_core_features_tfi:[27:38]
try:  # optional, used only if available
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from aurora core types; fallback if not present -------
try:  # pragma: no cover - used during integration
    from aurora.core.types import Trade, Side
except Exception:
    from enum import Enum
    @dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_core_calibration_icp_py_split_mondrian_conformal_venn_abers_self_tests:[23:39]
==repo_core_risk_cvar:[26:39]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# =============================
# Empirical VaR / CVaR
# =============================

def _quantile(sorted_vals: Sequence[float], alpha: float) -> float:
    n = len(sorted_vals)
    if n == 0:
        return 0.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==repo_core_calibration_calibrator:[51:60]
==repo_core_signal_score:[49:58]
    if z >= 0:
        ez = pow(2.718281828459045, -z)
        return 1.0 / (1.0 + ez)
    else:
        ez = pow(2.718281828459045, z)
        return ez / (1.0 + ez)


def _logit(p: float, eps: float = 1e-12) -> float: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==microprice:[119:132]
==repo_core_features_obi_py_order_book_imbalance_l_1_lk_depth_metrics_single_file_self_tests:[121:134]
        }
        return feats


# =============================
# Self-tests
# =============================

def _mock_snapseq() -> List[MarketSnapshot]:
    t0 = time.time()
    snaps: List[MarketSnapshot] = []
    bid, ask = 100.00, 100.02
    qb1, qa1 = 500.0, 520.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.absorption:[42:50]
==features_microstructure:[90:98]
@dataclass
class _EMA:
    half_life_s: float
    value: float = 0.0
    _last_ts: Optional[float] = None

    def update(self, x: float, ts: float) -> float:
        if self._last_ts is None: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.types:[117:129]
==features_cross_asset:[37:46]
    class MarketSnapshot:
        timestamp: float
        bid_price: float
        ask_price: float
        bid_volumes_l: Sequence[float]
        ask_volumes_l: Sequence[float]

# ---------------------------------------------------------------------------------
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_signal_leadlag_hy_py_hy_lead_lag_glr_te_embargo_single_file_self_tests:[184:190]
==features_cross_asset:[216:222]
        return {
            "hy_corr_0": base["hy_corr"],
            "hy_cov_0": base["hy_cov"],
            "beta_x_on_y_0": base["beta_x_on_y"],
            "beta_y_on_x_0": base["beta_y_on_x"],
            "corr_by_lag": corr_by_lag, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_signal_leadlag_hy_py_hy_lead_lag_glr_te_embargo_single_file_self_tests:[113:120]
==features_cross_asset:[147:155]
        dq = self._buf.get(sym, deque())
        if not dq:
            return []
        if now_ts is None:
            now_ts = dq[-1].t
        # ensure eviction up to now_ts and slice window
        self._evict_old(sym, now_ts)
        # copy points inside window (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[449:457]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[412:420]
    xs, y, p_unc = _make_synthetic()
    # Use raw scores for Platt (assume model returns a score; here we reuse xs)
    pl = PlattCalibrator(l2=1e-2, max_iter=100)
    pl.fit(xs, y)
    p_platt = pl.predict_proba(xs)
    m_unc = evaluate_calibration(p_unc, y)
    m_pl = evaluate_calibration(p_platt, y)
    # should improve logloss vs uncalibrated probabilities (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[23:33]
==features_cross_asset:[27:37]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# ---- Optional imports from core/types (fallbacks provided for standalone run) ----
try:  # pragma: no cover - used in integration tests
    from aurora.core.types import MarketSnapshot
except Exception:
    @dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tools.run_canary:[108:113]
==tools.run_live_testnet:[128:133]
        if 'process' in locals() and process.poll() is None:
            process.terminate()
            process.wait()
        return 130
    except Exception as e: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker_complete:[136:141]
==test_shadow_broker_old:[411:416]
        mock_response.json.return_value = {
            "symbols": [{
                "symbol": "BTCUSDT",
                "status": "TRADING",
                "filters": [ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[64:71]
==test_shadow_broker_old:[512:518]
        mock_cfg = Mock()
        mock_cfg.base_url = "https://api.binance.com"
        mock_cfg.api_key = "test_key"
        mock_cfg.api_secret = "test_secret"
        mock_load_cfg.return_value = mock_cfg

        # Mock exchange info response (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[42:49]
==test_shadow_broker_old:[480:486]
        mock_cfg = Mock()
        mock_cfg.base_url = "https://api.binance.com"
        mock_cfg.api_key = "test_key"
        mock_cfg.api_secret = "test_secret"
        mock_load_cfg.return_value = mock_cfg

        # Mock exchange info response (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[269:281]
==test_shadow_broker_complete:[337:347]
                assert "orderId" in result
                assert "fills" in result

    @patch('core.execution.shadow_broker.load_binance_cfg')
    def test_simulate_fill_limit_order(self, mock_load_cfg):
        """Тест симуляції виконання limit ордера"""
        mock_cfg = Mock()
        mock_cfg.base_url = "https://api.binance.com"
        mock_load_cfg.return_value = mock_cfg
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[407:412]
==test_shadow_broker_complete:[107:112]
        mock_response.json.return_value = {
            "symbols": [{
                "symbol": "BTCUSDT",
                "status": "TRADING",
                "filters": [ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_shadow_broker:[93:98]
==test_shadow_broker_complete:[21:26]
        mock_response.json.return_value = {
            "symbols": [{
                "symbol": "BTCUSDT",
                "status": "TRADING",
                "filters": [ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[185:195]
==test_execution_router_v1:[255:264]
        fill = FillEvent(
            ts_ns=time.time_ns(),
            qty=fill_qty,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        )
        router.handle_order_fill(child.order_id, fill)
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[139:150]
==test_execution_router_v1:[438:449]
        assert len(child.fills) == 1  # Still 1

    def test_late_fill_after_cancel_accepted(self, router, sample_context, sample_market_data):
        """Test late fill after cancel is accepted and logged"""
        context = sample_context
        children = router.execute_sizing_decision(context, sample_market_data)

        child = children[0]
        router.handle_order_ack(child.order_id, time.time_ns(), 5.0)

        # Cancel order (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[211:221]
==test_execution_router_v1:[442:450]
        context = sample_context
        children = router.execute_sizing_decision(context, sample_market_data)

        child = children[0]
        router.handle_order_ack(child.order_id, time.time_ns(), 5.0)

        # Cancel order (cleanup)
        router.handle_order_cancel(child.order_id, time.time_ns())

        # Late fill arrives (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.test_signal:[82:91]
==tools.ultra_simple_mutator:[63:68]
    assert x < y
    assert x != y
    assert y > x
    assert x <= 5
    assert y >= 10


def test_boolean_logic():
    """Boolean logic tests for mutation testing""" (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_tfi_complete:[305:313]
==tests.test_features:[335:342]
        assert features["buy_vol"] == 0.0
        assert features["sell_vol"] == 0.0
        assert features["tfi"] == 0.0
        assert features["vpin_like"] == 0.0
        assert features["vpin_bucketed"] == 0.0

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_obi_complete:[186:191]
==tests.test_features:[625:630]
        snapshot = MarketSnapshot(
            timestamp=1000.0,
            bid_price=99.98,
            ask_price=100.02,
            bid_volumes_l=[10.0, 8.0], (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_absorption:[205:215]
==tests.test_features:[1277:1289]
        assert 0.0 <= features["absorption_frac_bid"] <= 1.0
        assert 0.0 <= features["absorption_frac_ask"] <= 1.0

    def test_absorption_stream_price_step_up(self):
        """Test absorption stream with bid price stepping up."""
        from core.features.absorption import AbsorptionStream

        stream = AbsorptionStream()

        # Initial snapshot
        snap1 = MarketSnapshot(
            timestamp=1000.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[130:135]
==tests.integration.test_run_live_paper_loop:[237:242]
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=3.0,
            latency_ms=10.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[52:57]
==tests.integration.test_run_live_paper_loop:[104:109]
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=5.0,  # Positive edge
            latency_ms=10.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[50:56]
==tests.integration.test_run_live_paper_loop:[253:260]
        quote = QuoteSnapshot(bid_px=49999.0, ask_px=50001.0)

        # High latency should trigger SLA deny
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=5.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_xai_decision_trail:[107:112]
==tests.integration.test_full_b2b7_pipeline:[96:101]
        decision = router.decide(
            side='buy',
            quote=quote,
            edge_bps_estimate=5.0,
            latency_ms=10.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_enhanced_idempotency_partials:[185:192]
==tests.integration.test_e2e_integration:[100:107]
        fill = FillEvent(
            ts_ns=time.time_ns(),
            qty=fill_qty,
            price=child.price,
            fee=0.001,
            liquidity_flag='M'
        ) (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.integration.oms.test_order_lifecycle:[74:79]
==tests.integration.test_xai_audit_trail:[318:323]
            order = OrderRequest(
                symbol="BTCUSDT",
                side=Side.BUY,
                type=OrderType.MARKET,
                quantity=1.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.integration.oms.test_concurrent_orders:[269:274]
==tests.integration.test_xai_audit_trail:[492:497]
            order = OrderRequest(
                symbol="SOLUSDT",
                side=Side.BUY,
                type=OrderType.MARKET,
                quantity=1.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==tests.integration.oms.test_concurrent_orders:[75:80]
==tests.integration.test_xai_audit_trail:[558:563]
            order = OrderRequest(
                symbol="BTCUSDT",
                side=Side.BUY if i % 2 == 0 else Side.SELL,
                type=OrderType.MARKET,
                quantity=1.0, (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==test_basic_endpoints_new:[236:241]
==test_metrics_export:[23:28]
            'aurora_deny_rate_15m',
            'aurora_latency_p99_ms',
            'aurora_ece',
            'aurora_cvar95_min',
            'aurora_sse_clients', (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==run_live:[203:209]
==run_replay:[257:263]
                    else:
                        mapped = _find_best_split_and_set(base, k, v)
                        if mapped:
                            changes.append(mapped)
            return changes
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.xai.__init__:[25:30]
==core.xai.alerts:[244:249]
    "AlertResult",
    "NoTradesAlert",
    "DenySpikeAlert",
    "CalibrationDriftAlert",
    "CvarBreachAlert", (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[66:81]
==core.execution.router_new:[14:29]
    return 0.0 if x < 0.0 else (1.0 if x > 1.0 else x)


def _estimate_p_fill(fill_features: Dict[str, Any]) -> float:
    """
    Простий, детермінований естіматор P(fill) із ознак:
    - OBI in [-1,1] збільшує P
    - spread_bps зменшує P (5 bps ~ -0.25 до P)
    """
    obi = float(fill_features.get("obi", 0.0))
    spread_bps = float(fill_features.get("spread_bps", 0.0))
    p = 0.5 + 0.5 * obi - 0.05 * spread_bps
    return _clip01(p)

 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.router:[8:14]
==core.execution.router_new:[6:14]
@dataclass
class Decision:
    route: str                 # "maker" | "taker" | "deny"
    why_code: str              # e.g. "OK_ROUTE_MAKER", "OK_ROUTE_TAKER", "WHY_UNATTRACTIVE", "WHY_SLA_LATENCY"
    scores: Dict[str, float]


def _clip01(x: float) -> float: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.enhanced_router:[105:110]
==core.execution.router_backup:[94:99]
        try:
            cfg = get_config()
            max_latency_ms = float(cfg.get("execution.sla.max_latency_ms", 25))
        except (ConfigError, Exception):
            max_latency_ms = 25.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.enhanced_router:[43:50]
==core.execution.router_backup:[60:67]
        m = self.mid
        if m <= 0:
            return 0.0
        return (self.ask_px - self.bid_px) / m * 1e4 * 0.5


@dataclass (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.execution.enhanced_router:[70:75]
==core.execution.router:[52:57]
    reason: str
    maker_fee_bps: float = 0.0
    taker_fee_bps: float = 0.0
    net_e_maker_bps: float = 0.0
    net_e_taker_bps: float = 0.0 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==evt_pot:[105:110]
==repo_core_risk_evt_pot_py_pot_gpd_over_threshold_u_tail_va_r_es_bootstrap_ci_self_tests:[148:154]
    u = select_threshold(L, q=q_u)
    exc = [x - u for x in L if x > u]
    n_total = len(L)
    n_exc = len(exc)
    zeta = 0.0 if n_total == 0 else n_exc / n_total
    # point (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==evt_pot:[153:159]
==repo_core_risk_evt_pot_py_pot_gpd_over_threshold_u_tail_va_r_es_bootstrap_ci_self_tests:[100:105]
    u = select_threshold(L, q=q_u)
    exc = [x - u for x in L if x > u]
    n_total = len(L)
    n_exc = len(exc)
    zeta = 0.0 if n_total == 0 else n_exc / n_total (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[196:202]
==repo_core_features_tfi:[210:215]
    random.seed(seed)
    t0 = time.time()
    out: List[Trade] = []
    ts = t0
    for i in range(n):
        # 70% buys, 30% sells, sizes around 10±3 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[211:216]
==repo_core_features_tfi:[195:201]
    random.seed(seed)
    t0 = time.time()
    out: List[Trade] = []
    ts = t0
    for i in range(n): (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.tfi:[46:63]
==repo_core_features_tfi:[88:105]
    return float(tr.size) if str(tr.side) == "Side.BUY" or str(tr.side) == "BUY" else -float(tr.size)


def vpin_like(buy_vol: float, sell_vol: float) -> float:
    den = float(buy_vol) + float(sell_vol)
    if den <= 0.0:
        return 0.0
    return abs(float(buy_vol) - float(sell_vol)) / den


def vpin_volume_buckets(trades: Sequence[Trade], bucket_volume: float, max_buckets: int = 50) -> float:
    """Simplified VPIN: partition stream into successive buckets of fixed volume V.

    For each bucket i, accumulate BUY and SELL volumes until reaching V. The
    bucket imbalance is |B_i − S_i| / V (capped at 1). VPIN is the average over
    the last N=min(max_buckets, #buckets) buckets.
    """ (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.signal.leadlag_hy:[28:37]
==repo_core_features_microprice_py_microprice_l_1_lk_micro_premium_streaming_wrapper_self_tests:[24:33]
try:  # optional, used only for pretty-printing/arrays
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from aurora core types; fallback if not present -------
try:  # pragma: no cover - used during integration
    from aurora.core.types import MarketSnapshot
except Exception: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==drift:[189:205]
==repo_core_calibration_drift:[164:178]
        self.cusum.reset()
        self.glr.reset()

    def update(self, logit: float, ts: Optional[float] = None) -> Dict[str, float]:
        """Feed next logit/score and get combined drift diagnostics.

        Returns
        -------
        dict with keys: cusum_pos, cusum_neg, cusum_alarm, glr_stat, glr_alarm,
        and composite `drift_alarm` = 1 if either sub-alarm is 1.
        """
        out_c = self.cusum.update(logit, ts)
        out_g = self.glr.update(logit, ts)
        drift_alarm = 1.0 if (out_c["cusum_alarm"] > 0.5 or out_g["glr_alarm"] > 0.5) else 0.0

        # Emit ALERT_CALIBRATION_DRIFT event if drift detected (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==drift:[86:92]
==repo_core_calibration_drift:[108:113]
        if self.clip is not None:
            c = float(self.clip)
            x = max(-c, min(c, float(x)))
        else:
            x = float(x)
        # Page's recursion (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==drift:[133:138]
==repo_core_calibration_drift:[61:67]
        if self.clip is not None:
            c = float(self.clip)
            x = max(-c, min(c, float(x)))
        else:
            x = float(x)
        # Page's recursion (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.types:[105:112]
==features_microstructure:[43:49]
class Trade:
    timestamp: float
    price: float
    size: float
    side: Side  # aggression side (taker)

    def __post_init__(self) -> None: (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core.features.microstructure:[21:30]
==features_microstructure:[33:42]
try:  # pragma: no cover
    from core.types import MarketSnapshot, Trade, Side, OrderType, ExecMode
except Exception:  # pragma: no cover - fallback for standalone testing
    from enum import Enum

    class Side(str, Enum):
        BUY = "BUY"
        SELL = "SELL"
 (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==features_microstructure:[42:47]
==repo_core_features_tfi:[37:42]
    @dataclass
    class Trade:
        timestamp: float
        price: float
        size: float (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==features_cross_asset:[27:36]
==features_microstructure:[27:38]
try:  # Optional; used only for pretty array ops if present
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from core/types; fallback if unavailable ---------
try:  # pragma: no cover - exercised in integration, not in unit self-tests
    from aurora.core.types import (
        Trade, MarketSnapshot, Side,
    )
except Exception:  # Minimal fallbacks to run this file standalone (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_signal_leadlag_hy_py_hy_lead_lag_glr_te_embargo_single_file_self_tests:[100:105]
==features_cross_asset:[133:140]
        while i < len(rx) and j < len(ry):
            a0, a1, ri = rx[i]
            b0, b1, sj = ry[j]
            if min(a1, b1) > max(a0, b0):
                cov += ri * sj (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==calibrator:[486:491]
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[454:459]
if __name__ == "__main__":
    _test_metrics()
    _test_platt_improves_logloss()
    _test_temperature_improves()
    _test_isotonic_monotone() (duplicate-code)
tools\validate_canary_logs.py:1:0: R0801: Similar lines in 2 files
==core_calibration_py_probabilistic_calibration_platt_temperature_isotonic_ece_brier_log_loss_prequential_conformal_ci_self_tests:[23:32]
==edge_budget_py:[23:32]
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# -------- Optional import from core/types; minimal fallbacks if unavailable -----
try:  # pragma: no cover - exercised in integration
    from aurora.core.types import ProbabilityMetrics, ConformalInterval
except Exception: (duplicate-code)

-----------------------------------
Your code has been rated at 0.00/10
